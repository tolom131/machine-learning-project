{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "assignment_08_solution_latest.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "rgD-FF4MA9Kg",
        "SnBT9fWXA9Kj"
      ]
    },
    "interpreter": {
      "hash": "84bbda367bac7e7bffd9b7890a44d65326aaedad40e5a9021c2651157391b1ef"
    },
    "kernelspec": {
      "display_name": "Python 3.8.5 64-bit ('base': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c7dd830e296b45cd9e092bf59dbbc653": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_844e7e5dfb7e41d593b7e958cb75eb2f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2099b337ee7544f6b9a9bb5c5a83e0eb",
              "IPY_MODEL_9d1ede75fdb745b6a169b3822bdddce7",
              "IPY_MODEL_20e6be85f5ac4a428e24000f9a350914"
            ]
          }
        },
        "844e7e5dfb7e41d593b7e958cb75eb2f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2099b337ee7544f6b9a9bb5c5a83e0eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_70709925d8e9445bba3fa1280aaa8fb7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 26%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6d5bac7f8ffe43daaf32c017f3212d6a"
          }
        },
        "9d1ede75fdb745b6a169b3822bdddce7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c4b1d3fa540e400eb4d9be245104f612",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 200,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 52,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4a0fd79e295d4fa0a43fe6198f3a18bc"
          }
        },
        "20e6be85f5ac4a428e24000f9a350914": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_090ec7ad58d9411f987f73fc4196fbef",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 52/200 [04:44&lt;13:27,  5.46s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_151ce427841a4157af4cc2343793f755"
          }
        },
        "70709925d8e9445bba3fa1280aaa8fb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6d5bac7f8ffe43daaf32c017f3212d6a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c4b1d3fa540e400eb4d9be245104f612": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4a0fd79e295d4fa0a43fe6198f3a18bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "090ec7ad58d9411f987f73fc4196fbef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "151ce427841a4157af4cc2343793f755": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQGQXmOpA9J-"
      },
      "source": [
        "# Unsupervised image denoising"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGBcGR2LA9KI"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNpI2DZ6A9KJ"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset\n",
        "from os import listdir\n",
        "from os.path import join\n",
        "from torchvision.transforms import Compose, ToTensor, ToPILImage, Resize, Lambda, Normalize, Grayscale\n",
        "from torch.utils.data import DataLoader\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from math import log10\n",
        "from tqdm.notebook import tqdm\n",
        "import os"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OU32jTFuA9KM"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gf-s-8nrA9KN",
        "outputId": "90ba2d63-ae76-47f8-bd5c-bedd8ab76656"
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "directory_data  = './drive/MyDrive/Machine_Learning/'\n",
        "filename_data   = 'assignment_08_data.npz'\n",
        "data            = np.load(os.path.join(directory_data, filename_data))\n",
        "\n",
        "\n",
        "original_train = data['original_train'] \n",
        "noise_train    = data['noise_train']\n",
        "\n",
        "original_test = data['original_test'] \n",
        "noise_test    = data['noise_test']\n",
        "\n",
        "print('*************************************************')\n",
        "print('size of noise_train    : ', noise_train.shape)\n",
        "print('*************************************************')\n",
        "print('size of noise_test    : ', noise_test.shape)\n",
        "print('*************************************************')\n",
        "print('number of training image :', noise_train.shape[0])\n",
        "print('height of training image :', noise_train.shape[1])\n",
        "print('width of training image  :', noise_train.shape[2])\n",
        "print('*************************************************')\n",
        "print('number of testing image :', noise_test.shape[0])\n",
        "print('height of testing image :', noise_test.shape[1])\n",
        "print('width of testing image  :', noise_test.shape[2])\n",
        "print('*************************************************')\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
            "*************************************************\n",
            "size of noise_train    :  (2000, 64, 64)\n",
            "*************************************************\n",
            "size of noise_test    :  (900, 64, 64)\n",
            "*************************************************\n",
            "number of training image : 2000\n",
            "height of training image : 64\n",
            "width of training image  : 64\n",
            "*************************************************\n",
            "number of testing image : 900\n",
            "height of testing image : 64\n",
            "width of testing image  : 64\n",
            "*************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20TiZsA_A9KO"
      },
      "source": [
        "## Hyper parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtDynD4nA9KP"
      },
      "source": [
        "device        = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# ==================================================\n",
        "# determine optimal hyper-parameters to obtain best testing performance\n",
        "number_epoch    = 200\n",
        "size_minibatch  = 50\n",
        "learning_rate   = 0.1\n",
        "momentum        = 0.9\n",
        "weight_decay    = 0.001\n",
        "weight_total_variation = 0.5\n",
        "# =================================================="
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AbHCZPpA9KR"
      },
      "source": [
        "## Costumize dataloader for pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UveCjrBA9KS"
      },
      "source": [
        "class dataset (Dataset):\n",
        "  def  __init__(self, original,noise):\n",
        "\n",
        "    self.original = original\n",
        "    self.noise    = noise\n",
        "        \n",
        "  def __getitem__(self, index):\n",
        "\n",
        "    original    = self.original[index]\n",
        "    noise       = self.noise[index]\n",
        "    \n",
        "    original   = torch.FloatTensor(original).unsqueeze(dim=0)\n",
        "    noise      = torch.FloatTensor(noise).unsqueeze(dim=0)\n",
        "\n",
        "\n",
        "    return (original , noise)\n",
        "  \n",
        "  def __len__(self):\n",
        "\n",
        "     return self.original.shape[0]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAG5Ks0eMjfM"
      },
      "source": [
        "from torchvision.transforms import Compose, ToTensor, ToPILImage, Resize, Lambda, Normalize, Grayscale\n",
        "\n",
        "class argu_dataset (Dataset):\n",
        "  def  __init__(self, original, noise, transform):\n",
        "\n",
        "    original = original.astype('float32')\n",
        "    noise    = noise.astype('float32')\n",
        "       \n",
        "  def __getitem__(self, index):\n",
        "\n",
        "    original    = self.original[index]\n",
        "    noise       = self.noise[index]\n",
        "\n",
        "    original = original.astype('float32')\n",
        "    noise = noise.astype('float32')\n",
        "\n",
        "    original   = self.transform(original)\n",
        "    noise      = self.transform(noise)\n",
        "\n",
        "    original   = torch.FloatTensor(original).unsqueeze(dim=0)\n",
        "    noise      = torch.FloatTensor(noise).unsqueeze(dim=0)\n",
        "    \n",
        "    return (original , noise)\n",
        "  \n",
        "  def __len__(self):\n",
        "\n",
        "     return self.original.shape[0]\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "357alwWgA9KU"
      },
      "source": [
        "## Construct datasets and dataloaders for training and testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7JDqjSVA9KU"
      },
      "source": [
        "dataset_train = dataset(original_train, noise_train) \n",
        "dataset_test  = dataset(original_test, noise_test) \n",
        "\n",
        "dataloader_train = DataLoader(dataset_train, batch_size=size_minibatch, shuffle=True, drop_last=True, num_workers=2)\n",
        "dataloader_test  = DataLoader(dataset_test,  batch_size=size_minibatch, shuffle=False, drop_last=True, num_workers=2) "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DQ3_o-VA9KV"
      },
      "source": [
        "## Shape of the data with data loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCNL_T-HA9KW",
        "outputId": "185535fe-c270-41b8-b72b-4c20f1188742"
      },
      "source": [
        "(original_train, noise_train)   = dataset_train[0]\n",
        "(original_test, noise_test)     = dataset_test[0]\n",
        "\n",
        "print('************************************************************')\n",
        "print('shape of the original image in the training dataset:', original_train.shape)\n",
        "print('shape of the noisy image in the training dataset:', noise_train.shape)\n",
        "print('************************************************************')\n",
        "print('shape of the original image in the testing dataset:', original_test.shape)\n",
        "print('shape of the noisy image in the testing dataset:', noise_test.shape)\n",
        "print('************************************************************')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "************************************************************\n",
            "shape of the original image in the training dataset: torch.Size([1, 64, 64])\n",
            "shape of the noisy image in the training dataset: torch.Size([1, 64, 64])\n",
            "************************************************************\n",
            "shape of the original image in the testing dataset: torch.Size([1, 64, 64])\n",
            "shape of the noisy image in the testing dataset: torch.Size([1, 64, 64])\n",
            "************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Yn9jMLrA9KX"
      },
      "source": [
        "## Class for the neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Esyw9M--A9KY"
      },
      "source": [
        "class conv2DBatchNormRelu(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_channels,\n",
        "            n_filters,\n",
        "            k_size,\n",
        "            stride,\n",
        "            padding,\n",
        "            bias=True,\n",
        "            dilation=1,\n",
        "            with_bn=True,\n",
        "    ):\n",
        "        super(conv2DBatchNormRelu, self).__init__()\n",
        "\n",
        "        conv_mod = nn.Conv2d(int(in_channels),\n",
        "                             int(n_filters),\n",
        "                             kernel_size=k_size,\n",
        "                             padding=padding,\n",
        "                             stride=stride,\n",
        "                             bias=bias,\n",
        "                             dilation=dilation, )\n",
        "\n",
        "        if with_bn:\n",
        "            self.cbr_unit = nn.Sequential(conv_mod,\n",
        "                                          nn.BatchNorm2d(int(n_filters)),\n",
        "                                          nn.ReLU(inplace=True))\n",
        "        else:\n",
        "            self.cbr_unit = nn.Sequential(conv_mod, nn.ReLU(inplace=True))\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        outputs = self.cbr_unit(inputs)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class segnetDown2(nn.Module):\n",
        "    def __init__(self, in_size, out_size):\n",
        "        super(segnetDown2, self).__init__()\n",
        "        self.conv1 = conv2DBatchNormRelu(in_size, out_size, 3, 1, 1)\n",
        "        self.conv2 = conv2DBatchNormRelu(out_size, out_size, 3, 1, 1)\n",
        "        self.maxpool_with_argmax = nn.MaxPool2d(2, 2, return_indices=True)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        outputs = self.conv1(inputs)\n",
        "        outputs = self.conv2(outputs)\n",
        "        unpooled_shape = outputs.size()\n",
        "        outputs, indices = self.maxpool_with_argmax(outputs)\n",
        "        return outputs, indices, unpooled_shape\n",
        "\n",
        "\n",
        "class segnetDown3(nn.Module):\n",
        "    def __init__(self, in_size, out_size):\n",
        "        super(segnetDown3, self).__init__()\n",
        "        self.conv1 = conv2DBatchNormRelu(in_size, out_size, 3, 1, 1)\n",
        "        self.conv2 = conv2DBatchNormRelu(out_size, out_size, 3, 1, 1)\n",
        "        self.conv3 = conv2DBatchNormRelu(out_size, out_size, 3, 1, 1)\n",
        "        self.maxpool_with_argmax = nn.MaxPool2d(2, 2, return_indices=True)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        outputs = self.conv1(inputs)\n",
        "        outputs = self.conv2(outputs)\n",
        "        outputs = self.conv3(outputs)\n",
        "        unpooled_shape = outputs.size()\n",
        "        outputs, indices = self.maxpool_with_argmax(outputs)\n",
        "        return outputs, indices, unpooled_shape\n",
        "\n",
        "\n",
        "class segnetUp2(nn.Module):\n",
        "    def __init__(self, in_size, out_size):\n",
        "        super(segnetUp2, self).__init__()\n",
        "        self.unpool = nn.MaxUnpool2d(2, 2)\n",
        "        self.conv1 = conv2DBatchNormRelu(in_size, in_size, 3, 1, 1)\n",
        "        self.conv2 = conv2DBatchNormRelu(in_size, out_size, 3, 1, 1)\n",
        "\n",
        "    def forward(self, inputs, indices, output_shape):\n",
        "        outputs = self.unpool(input=inputs, indices=indices, output_size=output_shape)\n",
        "        outputs = self.conv1(outputs)\n",
        "        outputs = self.conv2(outputs)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class segnetUp3(nn.Module):\n",
        "    def __init__(self, in_size, out_size):\n",
        "        super(segnetUp3, self).__init__()\n",
        "        self.unpool = nn.MaxUnpool2d(2, 2)\n",
        "        self.conv1 = conv2DBatchNormRelu(in_size, in_size, 3, 1, 1)\n",
        "        self.conv2 = conv2DBatchNormRelu(in_size, in_size, 3, 1, 1)\n",
        "        self.conv3 = conv2DBatchNormRelu(in_size, out_size, 3, 1, 1)\n",
        "\n",
        "    def forward(self, inputs, indices, output_shape):\n",
        "        outputs = self.unpool(input=inputs, indices=indices, output_size=output_shape)\n",
        "        outputs = self.conv1(outputs)\n",
        "        outputs = self.conv2(outputs)\n",
        "        outputs = self.conv3(outputs)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class SegNet(nn.Module):\n",
        "    def __init__(self, n_classes=1, in_channels=1, is_unpooling=True):\n",
        "        super(SegNet, self).__init__()\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.is_unpooling = is_unpooling\n",
        "\n",
        "        self.down1 = segnetDown2(self.in_channels, 64)\n",
        "        self.down2 = segnetDown2(64, 128)\n",
        "        self.down3 = segnetDown3(128, 256)\n",
        "        self.down4 = segnetDown3(256, 512)\n",
        "        self.down5 = segnetDown3(512, 512)\n",
        "\n",
        "        self.up5 = segnetUp3(512, 512)\n",
        "        self.up4 = segnetUp3(512, 256)\n",
        "        self.up3 = segnetUp3(256, 128)\n",
        "        self.up2 = segnetUp2(128, 64)\n",
        "        self.up1 = segnetUp2(64, n_classes)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "\n",
        "        down1, indices_1, unpool_shape1 = self.down1(inputs)\n",
        "        down2, indices_2, unpool_shape2 = self.down2(down1)\n",
        "        down3, indices_3, unpool_shape3 = self.down3(down2)\n",
        "        down4, indices_4, unpool_shape4 = self.down4(down3)\n",
        "        down5, indices_5, unpool_shape5 = self.down5(down4)\n",
        "\n",
        "        up5 = self.up5(down5, indices_5, unpool_shape5)\n",
        "        up4 = self.up4(up5, indices_4, unpool_shape4)\n",
        "        up3 = self.up3(up4, indices_3, unpool_shape3)\n",
        "        up2 = self.up2(up3, indices_2, unpool_shape2)\n",
        "        up1 = self.up1(up2, indices_1, unpool_shape1)\n",
        "\n",
        "        return up1"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTptbDGZA9KZ"
      },
      "source": [
        "## Build the network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MgUbMJdA9KZ"
      },
      "source": [
        "model = SegNet().to(device)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum , weight_decay=weight_decay)\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAMmqhrUA9KZ"
      },
      "source": [
        "## Compute prediction (denoised image)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sE58VDWA9Ka"
      },
      "source": [
        "def compute_prediction(input, model):\n",
        "    \n",
        "    # ==================================================\n",
        "    # fill up the blank\n",
        "    prediction = model(input)\n",
        "    # ==================================================\n",
        "    \n",
        "    return prediction"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4MU2uUwA9Kc"
      },
      "source": [
        "## Compute loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzLZ5_ijB13L"
      },
      "source": [
        "def compute_fidelity(input, prediction):\n",
        "    \n",
        "    mse = nn.MSELoss()\n",
        "    \n",
        "    # ==================================================\n",
        "    # fill up the blank\n",
        "    loss_mse = mse(prediction, input)\n",
        "    # ==================================================\n",
        "    \n",
        "    loss_mse_value = loss_mse.item() \n",
        "    \n",
        "    return loss_mse, loss_mse_value"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-J9-SmZeDDHW"
      },
      "source": [
        "def compute_regularization(data, weight):\n",
        "    \n",
        "    bs_img, c_img, h_img, w_img = data.size()\n",
        "   \n",
        "    tv_height = torch.abs(data[:, :, 1:, :] - data[:, :, :- 1, :]).sum()\n",
        "    tv_width  = torch.abs(data[:, :, :, 1:] - data[:, :, :, :-1]).sum()\n",
        "\n",
        "    total_variation = (tv_height + tv_width) / (bs_img * c_img * h_img * w_img)\n",
        "    \n",
        "    # ==================================================\n",
        "    # fill up the blank\n",
        "    loss_regularization = weight * total_variation\n",
        "    # ==================================================\n",
        "    \n",
        "    loss_regularization_value = loss_regularization.item()\n",
        "     \n",
        "    return loss_regularization, loss_regularization_value"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grQ11S2uA9Kc"
      },
      "source": [
        "def compute_loss(input, prediction, weight):\n",
        "\n",
        "    (loss_fidelity, loss_fidelity_value) = compute_fidelity(input, prediction)\n",
        "    (loss_regularization, loss_regularization_value) = compute_regularization(prediction, weight) \n",
        "\n",
        "    # ==================================================\n",
        "    # fill up the blank\n",
        "    loss = loss_fidelity + loss_regularization\n",
        "    # ==================================================\n",
        "    \n",
        "    loss_value = loss.item()\n",
        "    \n",
        "    return loss, loss_value , loss_fidelity_value, loss_regularization_value"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1EWYXG6A9Kc"
      },
      "source": [
        "## Compute PSNR metric"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtVX7nLCA9Kd"
      },
      "source": [
        "def compute_PSNR(mse):\n",
        "\n",
        "    if (mse==0.):\n",
        "        PSNR=100    \n",
        "    else :\n",
        "        PSNR=10*log10(1.0 / mse)\n",
        "        \n",
        "    return PSNR"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfi3toLTA9Kd"
      },
      "source": [
        "## Variable for the learning curves"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "re9vSZWLA9Kd"
      },
      "source": [
        "loss_fidelity_mean_train       = np.zeros(number_epoch)\n",
        "loss_fidelity_std_train        = np.zeros(number_epoch)\n",
        "loss_regularization_mean_train = np.zeros(number_epoch)\n",
        "loss_regularization_std_train  = np.zeros(number_epoch)\n",
        "loss_mean_train                = np.zeros(number_epoch)\n",
        "loss_std_train                 = np.zeros(number_epoch)\n",
        "PSNR_mean_train                = np.zeros(number_epoch)\n",
        "PSNR_std_train                 = np.zeros(number_epoch)\n",
        "\n",
        "loss_fidelity_mean_test       = np.zeros(number_epoch)\n",
        "loss_fidelity_std_test        = np.zeros(number_epoch)\n",
        "loss_regularization_mean_test = np.zeros(number_epoch)\n",
        "loss_regularization_std_test  = np.zeros(number_epoch)\n",
        "loss_mean_test                = np.zeros(number_epoch)\n",
        "loss_std_test                 = np.zeros(number_epoch)\n",
        "PSNR_mean_test                = np.zeros(number_epoch)\n",
        "PSNR_std_test                 = np.zeros(number_epoch)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2DK1BgcA9Ke"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6z6RHIxcA9Ke"
      },
      "source": [
        "def train(model, dataloader):\n",
        "\n",
        "    loss_epoch          = []\n",
        "    loss_fidelity_epoch = []\n",
        "    loss_reg_epoch      = []\n",
        "    psnr_epoch          = []\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for index_batch, (original, noise) in enumerate(dataloader):\n",
        "\n",
        "        original = original.to(device)\n",
        "        noise    = noise.to(device)\n",
        "        \n",
        "        prediction = compute_prediction(noise, model)\n",
        "        (loss, loss_value, loss_fidelity_value, loss_regularization_value) = compute_loss(noise, prediction, weight_total_variation)\n",
        "        \n",
        "        # ==================================================\n",
        "        # fill up the blank\n",
        "        (mse, mse_value)    = compute_fidelity(original, prediction)\n",
        "        psnr                = compute_PSNR(mse_value)\n",
        "        # ==================================================\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_epoch.append(loss_value)\n",
        "        loss_fidelity_epoch.append(loss_fidelity_value)\n",
        "        loss_reg_epoch.append(loss_regularization_value)\n",
        "        psnr_epoch.append(psnr)\n",
        "\n",
        "    loss_mean_epoch     = np.mean(loss_epoch)\n",
        "    loss_std_epoch      = np.std(loss_epoch)\n",
        "    \n",
        "    loss_fidelity_mean_epoch     = np.mean(loss_fidelity_epoch)\n",
        "    loss_fidelity_std_epoch      = np.std(loss_fidelity_epoch)\n",
        "    \n",
        "    loss_reg_mean_epoch     = np.mean(loss_reg_epoch)\n",
        "    loss_reg_std_epoch      = np.std(loss_reg_epoch)\n",
        "\n",
        "    psnr_mean_epoch = np.mean(psnr_epoch)\n",
        "    psnr_std_epoch  = np.std(psnr_epoch)\n",
        "\n",
        "    loss                       = {'mean' : loss_mean_epoch, 'std' : loss_std_epoch}\n",
        "    loss_fidelity              = {'mean' : loss_fidelity_mean_epoch, 'std' : loss_fidelity_std_epoch}\n",
        "    loss_regularization        = {'mean' : loss_reg_mean_epoch, 'std' : loss_reg_std_epoch}\n",
        "    psnr                       = {'mean' : psnr_mean_epoch, 'std' : psnr_std_epoch}\n",
        "\n",
        "    return (loss, loss_fidelity, loss_regularization, psnr)    \n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z76W6RQWA9Ke"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9FO3jlwA9Ke"
      },
      "source": [
        "def test(model, dataloader):\n",
        "\n",
        "    loss_epoch          = []\n",
        "    loss_fidelity_epoch = []\n",
        "    loss_reg_epoch      = []\n",
        "    psnr_epoch          = []\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    for index_batch, (original, noise) in enumerate(dataloader):\n",
        "\n",
        "        original = original.to(device)\n",
        "        noise    = noise.to(device)\n",
        "\n",
        "        prediction = compute_prediction(noise, model)\n",
        "        (loss, loss_value, loss_fidelity_value, loss_regularization_value) = compute_loss(noise, prediction, weight_total_variation)\n",
        "        \n",
        "        # ==================================================\n",
        "        # fill up the blank\n",
        "        (mse, mse_value)    = compute_fidelity(original, prediction)\n",
        "        psnr                = compute_PSNR(mse_value)\n",
        "        # ==================================================\n",
        "\n",
        "        loss_epoch.append(loss_value)\n",
        "        loss_fidelity_epoch.append(loss_fidelity_value)\n",
        "        loss_reg_epoch.append(loss_regularization_value)\n",
        "        psnr_epoch.append(psnr)\n",
        "\n",
        "    loss_mean_epoch     = np.mean(loss_epoch)\n",
        "    loss_std_epoch      = np.std(loss_epoch)\n",
        "    \n",
        "    loss_fidelity_mean_epoch     = np.mean(loss_fidelity_epoch)\n",
        "    loss_fidelity_std_epoch      = np.std(loss_fidelity_epoch)\n",
        "    \n",
        "    loss_reg_mean_epoch     = np.mean(loss_reg_epoch)\n",
        "    loss_reg_std_epoch      = np.std(loss_reg_epoch)\n",
        "\n",
        "    psnr_mean_epoch = np.mean(psnr_epoch)\n",
        "    psnr_std_epoch  = np.std(psnr_epoch)\n",
        "\n",
        "    loss                       = {'mean' : loss_mean_epoch, 'std' : loss_std_epoch}\n",
        "    loss_fidelity              = {'mean' : loss_fidelity_mean_epoch, 'std' : loss_fidelity_std_epoch}\n",
        "    loss_regularization        = {'mean' : loss_reg_mean_epoch, 'std' : loss_reg_std_epoch}\n",
        "    psnr                       = {'mean' : psnr_mean_epoch, 'std' : psnr_std_epoch}\n",
        "\n",
        "    return (loss, loss_fidelity, loss_regularization, psnr)    \n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nl_p_5B2A9Kf"
      },
      "source": [
        "## train and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 985,
          "referenced_widgets": [
            "c7dd830e296b45cd9e092bf59dbbc653",
            "844e7e5dfb7e41d593b7e958cb75eb2f",
            "2099b337ee7544f6b9a9bb5c5a83e0eb",
            "9d1ede75fdb745b6a169b3822bdddce7",
            "20e6be85f5ac4a428e24000f9a350914",
            "70709925d8e9445bba3fa1280aaa8fb7",
            "6d5bac7f8ffe43daaf32c017f3212d6a",
            "c4b1d3fa540e400eb4d9be245104f612",
            "4a0fd79e295d4fa0a43fe6198f3a18bc",
            "090ec7ad58d9411f987f73fc4196fbef",
            "151ce427841a4157af4cc2343793f755"
          ]
        },
        "id": "-kpLyy6bA9Kf",
        "outputId": "d35c1eb9-8030-4cac-bfde-126e1458fba3"
      },
      "source": [
        "# ================================================================================\n",
        "# \n",
        "# iterations for epochs\n",
        "#\n",
        "# ================================================================================\n",
        "for i in tqdm(range(number_epoch)):\n",
        "    \n",
        "    # ================================================================================\n",
        "    # \n",
        "    # training\n",
        "    #\n",
        "    # ================================================================================\n",
        "    (loss_train, loss_fidelity_train, loss_reg_train, psnr_train) = train(model, dataloader_train)\n",
        "\n",
        "    loss_mean_train[i] = loss_train['mean']\n",
        "    loss_std_train[i]  = loss_train['std']\n",
        "    \n",
        "    loss_fidelity_mean_train[i] = loss_fidelity_train['mean']\n",
        "    loss_fidelity_std_train[i]  = loss_fidelity_train['std']\n",
        "    \n",
        "    loss_regularization_mean_train[i] = loss_reg_train['mean']\n",
        "    loss_regularization_std_train[i]  = loss_reg_train['std']\n",
        "\n",
        "    PSNR_mean_train[i]  = psnr_train['mean']\n",
        "    PSNR_std_train[i]   = psnr_train['std']\n",
        "\n",
        "    # ================================================================================\n",
        "    # \n",
        "    # testing\n",
        "    #\n",
        "    # ================================================================================\n",
        "    (loss_test, loss_fidelity_test, loss_reg_test, psnr_test) = test(model, dataloader_test)\n",
        "\n",
        "    loss_mean_test[i] = loss_test['mean']\n",
        "    loss_std_test[i]  = loss_test['std']\n",
        "\n",
        "    loss_fidelity_mean_test[i] = loss_fidelity_test['mean']\n",
        "    loss_fidelity_std_test[i]  = loss_fidelity_test['std']\n",
        "    \n",
        "    loss_regularization_mean_test[i] = loss_reg_test['mean']\n",
        "    loss_regularization_std_test[i]  = loss_reg_test['std']\n",
        "\n",
        "    PSNR_mean_test[i]  = psnr_test['mean']\n",
        "    PSNR_std_test[i]   = psnr_test['std']\n",
        "\n",
        "    print(f\"epooch : {i}, train loss : {loss_train['mean']}, train PSNR : {PSNR_mean_train[i]}, test loss : {loss_mean_test[i]}, test PSNR : {PSNR_mean_test[i]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c7dd830e296b45cd9e092bf59dbbc653",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/200 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epooch : 0, train loss : 0.12717695720493793, train PSNR : 10.202666701339446, test loss : 0.07869481460915671, test PSNR : 11.293313713189967\n",
            "epooch : 1, train loss : 0.07467324621975421, train PSNR : 11.54627330281157, test loss : 0.07789627545409733, test PSNR : 11.317520127807743\n",
            "epooch : 2, train loss : 0.07361710984259844, train PSNR : 11.617341177921247, test loss : 0.07868800229496425, test PSNR : 11.272526614806715\n",
            "epooch : 3, train loss : 0.0721221650019288, train PSNR : 11.75607185136207, test loss : 0.07854165136814117, test PSNR : 11.284432797640555\n",
            "epooch : 4, train loss : 0.06865197867155075, train PSNR : 12.12844104391856, test loss : 0.0768346596095297, test PSNR : 11.466450463920005\n",
            "epooch : 5, train loss : 0.06332303211092949, train PSNR : 12.760148322988703, test loss : 0.07572030731373364, test PSNR : 11.601898792066613\n",
            "epooch : 6, train loss : 0.05564730390906334, train PSNR : 13.861800637544098, test loss : 0.06465555603305499, test PSNR : 12.883829240842408\n",
            "epooch : 7, train loss : 0.048729439824819566, train PSNR : 15.091124689359793, test loss : 0.048383920143047966, test PSNR : 14.962619308909492\n",
            "epooch : 8, train loss : 0.04509606473147869, train PSNR : 15.683709353091857, test loss : 0.047439807198113866, test PSNR : 15.039281800672796\n",
            "epooch : 9, train loss : 0.04357895795255899, train PSNR : 15.89932648119713, test loss : 0.044482952604691185, test PSNR : 15.443515226797384\n",
            "epooch : 10, train loss : 0.042299291677773, train PSNR : 16.107055024780344, test loss : 0.0430649688674344, test PSNR : 15.776621893464624\n",
            "epooch : 11, train loss : 0.041262305341660976, train PSNR : 16.26268282961708, test loss : 0.04214828254448043, test PSNR : 15.897146807205464\n",
            "epooch : 12, train loss : 0.040699441079050304, train PSNR : 16.39616304708472, test loss : 0.04247018715573682, test PSNR : 15.848151301569487\n",
            "epooch : 13, train loss : 0.040293691027909515, train PSNR : 16.430878368333325, test loss : 0.042129032106863126, test PSNR : 15.843413979073192\n",
            "epooch : 14, train loss : 0.039244727790355684, train PSNR : 16.59100398669103, test loss : 0.04027854175203376, test PSNR : 16.202915447886706\n",
            "epooch : 15, train loss : 0.03871448244899511, train PSNR : 16.664164911565575, test loss : 0.03990074267817868, test PSNR : 16.12841964884968\n",
            "epooch : 16, train loss : 0.0378135641105473, train PSNR : 16.810585659325916, test loss : 0.03819733299314976, test PSNR : 16.4615031047135\n",
            "epooch : 17, train loss : 0.036636955104768276, train PSNR : 17.026642364188426, test loss : 0.0373639226373699, test PSNR : 16.619057227132235\n",
            "epooch : 18, train loss : 0.036414020042866466, train PSNR : 17.03796012863131, test loss : 0.037194394403033786, test PSNR : 16.547997235197574\n",
            "epooch : 19, train loss : 0.03581027835607529, train PSNR : 17.166832417523104, test loss : 0.03694275187121497, test PSNR : 16.502151180014053\n",
            "epooch : 20, train loss : 0.03530945517122745, train PSNR : 17.254825667955867, test loss : 0.038027883196870484, test PSNR : 16.37620299138237\n",
            "epooch : 21, train loss : 0.03570193499326706, train PSNR : 17.151600262746353, test loss : 0.03784208082490497, test PSNR : 16.275921147224196\n",
            "epooch : 22, train loss : 0.036010618321597576, train PSNR : 17.067342895738697, test loss : 0.03639369457960129, test PSNR : 16.58926409350083\n",
            "epooch : 23, train loss : 0.03504242999479175, train PSNR : 17.296332228728087, test loss : 0.03638383394314183, test PSNR : 16.575569190374654\n",
            "epooch : 24, train loss : 0.034454749710857865, train PSNR : 17.39112012564446, test loss : 0.03540948198901282, test PSNR : 16.853661108835016\n",
            "epooch : 25, train loss : 0.034349881764501336, train PSNR : 17.39302584610057, test loss : 0.03687763834993044, test PSNR : 16.625984288506018\n",
            "epooch : 26, train loss : 0.034835471352562306, train PSNR : 17.28534262850561, test loss : 0.03514573723077774, test PSNR : 16.895013553221403\n",
            "epooch : 27, train loss : 0.03466619164682925, train PSNR : 17.32221803880243, test loss : 0.0357898800737328, test PSNR : 16.669014050257417\n",
            "epooch : 28, train loss : 0.03412159411236644, train PSNR : 17.445067035818607, test loss : 0.0352282722791036, test PSNR : 16.805807449793683\n",
            "epooch : 29, train loss : 0.033518784353509544, train PSNR : 17.57783600108889, test loss : 0.03502885272933377, test PSNR : 16.858276299771298\n",
            "epooch : 30, train loss : 0.033948477497324346, train PSNR : 17.47256816471773, test loss : 0.03490664209756586, test PSNR : 16.845728129715038\n",
            "epooch : 31, train loss : 0.03357308926060796, train PSNR : 17.523628517304992, test loss : 0.03544920579426818, test PSNR : 16.928129561301255\n",
            "epooch : 32, train loss : 0.03366656214930117, train PSNR : 17.509362608443134, test loss : 0.03418513242569235, test PSNR : 17.325387223222474\n",
            "epooch : 33, train loss : 0.03322120467200875, train PSNR : 17.606104799417587, test loss : 0.035581022707952395, test PSNR : 16.599832373924528\n",
            "epooch : 34, train loss : 0.03278314676135778, train PSNR : 17.730027886849843, test loss : 0.03422955651250151, test PSNR : 17.052902476953182\n",
            "epooch : 35, train loss : 0.0329488645773381, train PSNR : 17.68085731671999, test loss : 0.03401679628425174, test PSNR : 17.152563267032455\n",
            "epooch : 36, train loss : 0.032970902137458326, train PSNR : 17.639765333722785, test loss : 0.033685688550273575, test PSNR : 17.269951743508237\n",
            "epooch : 37, train loss : 0.032797645404934885, train PSNR : 17.727190748963842, test loss : 0.03552583480874697, test PSNR : 16.54780175699494\n",
            "epooch : 38, train loss : 0.03319791438989341, train PSNR : 17.620482802513028, test loss : 0.03999260606037246, test PSNR : 15.859334566039049\n",
            "epooch : 39, train loss : 0.03374502463266253, train PSNR : 17.51015541190631, test loss : 0.034414766977230705, test PSNR : 16.980534414183087\n",
            "epooch : 40, train loss : 0.03294707583263516, train PSNR : 17.680168732333243, test loss : 0.03452269360423088, test PSNR : 16.956544056305923\n",
            "epooch : 41, train loss : 0.0330608980730176, train PSNR : 17.618784992377368, test loss : 0.03319002750019232, test PSNR : 17.43807886954984\n",
            "epooch : 42, train loss : 0.03308985545299947, train PSNR : 17.645706061462032, test loss : 0.0344870440247986, test PSNR : 17.12461987515461\n",
            "epooch : 43, train loss : 0.033089631656184795, train PSNR : 17.62744048016329, test loss : 0.03388236235413286, test PSNR : 17.118217670005215\n",
            "epooch : 44, train loss : 0.03325867876410484, train PSNR : 17.586881669690676, test loss : 0.0332017263604535, test PSNR : 17.385491166270256\n",
            "epooch : 45, train loss : 0.03224101746454835, train PSNR : 17.834524065190692, test loss : 0.03290919897456964, test PSNR : 17.368180380743016\n",
            "epooch : 46, train loss : 0.03229523771442473, train PSNR : 17.827988745782246, test loss : 0.032963033869034715, test PSNR : 17.34879157131349\n",
            "epooch : 47, train loss : 0.03218626603484154, train PSNR : 17.84521714516462, test loss : 0.034079664283328585, test PSNR : 17.023755776532504\n",
            "epooch : 48, train loss : 0.032289157062768935, train PSNR : 17.808245677141873, test loss : 0.033445501079161964, test PSNR : 17.109892412768748\n",
            "epooch : 49, train loss : 0.032248066272586584, train PSNR : 17.807422380434986, test loss : 0.032767234784033566, test PSNR : 17.42219445518583\n",
            "epooch : 50, train loss : 0.03176783546805382, train PSNR : 17.92873244909099, test loss : 0.03275594373957978, test PSNR : 17.47639457444732\n",
            "epooch : 51, train loss : 0.03225512937642634, train PSNR : 17.832998207709252, test loss : 0.03366858884692192, test PSNR : 17.140446040341132\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxek0ypkA9Kf"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4N32do7A9Kg"
      },
      "source": [
        "# functions for visualizing the results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cY1ul6ttA9Kg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgD-FF4MA9Kg"
      },
      "source": [
        "## Plot functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7SzyQnaA9Kg"
      },
      "source": [
        "def plot_data_grid(data, index_data, nRow, nCol):\n",
        "    \n",
        "    fig, axes = plt.subplots(nRow, nCol, constrained_layout=True, figsize=(nCol * 3, nRow * 3))\n",
        "\n",
        "    for i in range(nRow):\n",
        "        for j in range(nCol):\n",
        "\n",
        "            k       = i * nCol + j\n",
        "            index   = index_data[k]\n",
        "\n",
        "            axes[i, j].imshow(data[index], cmap='gray', vmin=0, vmax=1)\n",
        "            axes[i, j].xaxis.set_visible(False)\n",
        "            axes[i, j].yaxis.set_visible(False)\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVYA6nBGA9Kh"
      },
      "source": [
        "def plot_data_tensor_grid(data, index_data, nRow, nCol):\n",
        "    \n",
        "    fig, axes = plt.subplots(nRow, nCol, constrained_layout=True, figsize=(nCol * 3, nRow * 3))\n",
        "\n",
        "    data = data.detach().cpu().squeeze(axis=1)\n",
        "\n",
        "    for i in range(nRow):\n",
        "        for j in range(nCol):\n",
        "\n",
        "            k       = i * nCol + j\n",
        "            index   = index_data[k]\n",
        "\n",
        "            axes[i, j].imshow(data[index], cmap='gray', vmin=0, vmax=1)\n",
        "            axes[i, j].xaxis.set_visible(False)\n",
        "            axes[i, j].yaxis.set_visible(False)\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r06Lf6UNA9Kh"
      },
      "source": [
        "def plot_curve_error(data_mean, data_std, x_label, y_label, title):\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.title(title)\n",
        "\n",
        "    alpha = 0.3\n",
        "    \n",
        "    plt.plot(range(len(data_mean)), data_mean, '-', color = 'red')\n",
        "    plt.fill_between(range(len(data_mean)), data_mean - data_std, data_mean + data_std, facecolor = 'blue', alpha = alpha) \n",
        "    \n",
        "    plt.xlabel(x_label)\n",
        "    plt.ylabel(y_label)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBmjFM9tA9Kh"
      },
      "source": [
        "def print_curve(data, index):\n",
        "    \n",
        "    for i in range(len(index)):\n",
        "\n",
        "        idx = index[i]\n",
        "        val = data[idx]\n",
        "\n",
        "        print('index = %2d, value = %12.10f' % (idx, val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5_ePFORA9Ki"
      },
      "source": [
        "def get_data_last(data, index_start):\n",
        "\n",
        "    data_last = data[index_start:]\n",
        "\n",
        "    return data_last"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFNAv5nnA9Ki"
      },
      "source": [
        "def get_max_last_range(data, index_start):\n",
        "\n",
        "    data_range = get_data_last(data, index_start)\n",
        "    value = data_range.max()\n",
        "\n",
        "    return value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0FhjnneA9Ki"
      },
      "source": [
        "def get_min_last_range(data, index_start):\n",
        "\n",
        "    data_range = get_data_last(data, index_start)\n",
        "    value = data_range.min()\n",
        "\n",
        "    return value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZR2U5WquA9Kj"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnBT9fWXA9Kj"
      },
      "source": [
        "# functions for presenting the results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ak9mEQ11A9Kj"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mSysHDcA9Kk"
      },
      "source": [
        "def function_result_01():\n",
        "\n",
        "    print('[plot examples of the training images]')\n",
        "    print('') \n",
        "\n",
        "    nRow = 5 \n",
        "    nCol = 4\n",
        "    index_data  = np.arange(0, nRow * nCol)\n",
        "    image_train = dataset_train.noise[index_data]\n",
        "\n",
        "    plot_data_grid(image_train, index_data, nRow, nCol)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gG5Dv-BtA9Kk"
      },
      "source": [
        "def function_result_02():\n",
        "\n",
        "    print('[plot examples of the training denoising results]')\n",
        "    print('') \n",
        "\n",
        "    nRow = 5\n",
        "    nCol = 4\n",
        "    index_data          = np.arange(0, nRow * nCol)\n",
        "    image_train         = torch.FloatTensor(dataset_train.noise[index_data]).unsqueeze(dim=1).to(device)\n",
        "    prediction_train    = compute_prediction(image_train, model)\n",
        "    \n",
        "    plot_data_tensor_grid(prediction_train, index_data, nRow, nCol)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AOFrS4wA9Kk"
      },
      "source": [
        "def function_result_03():\n",
        "\n",
        "    print('[plot examples of the testing images]')\n",
        "    print('') \n",
        "    \n",
        "    nRow = 5\n",
        "    nCol = 4\n",
        "    index_data = np.arange(0, nRow * nCol)\n",
        "    image_test = dataset_test.noise[index_data]\n",
        "\n",
        "    plot_data_grid(image_test, index_data, nRow, nCol)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2jAuO-HA9Kl"
      },
      "source": [
        "def function_result_04():\n",
        "\n",
        "    print('[plot examples of the testing denoising results]')\n",
        "    print('') \n",
        "\n",
        "    nRow = 5\n",
        "    nCol = 4\n",
        "    index_data      = np.arange(0, nRow * nCol)\n",
        "    image_test      = torch.FloatTensor(dataset_test.noise[index_data]).unsqueeze(dim=1).to(device)\n",
        "    prediction_test = compute_prediction(image_test, model)\n",
        "\n",
        "    plot_data_tensor_grid(prediction_test, index_data, nRow, nCol)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4q_8MOVKA9Kl"
      },
      "source": [
        "def function_result_05():\n",
        "\n",
        "    print('[plot the training loss]')\n",
        "    print('') \n",
        "\n",
        "    plot_curve_error(loss_mean_train, loss_std_train, 'epoch', 'loss', 'training loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4aH0BefQhUi"
      },
      "source": [
        "def function_result_06():\n",
        "\n",
        "    print('[plot the training fidelity loss]')\n",
        "    print('') \n",
        "\n",
        "    plot_curve_error(loss_fidelity_mean_train, loss_fidelity_std_train, 'epoch', 'loss', 'training loss (fidelity)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBHnNBNcQw9a"
      },
      "source": [
        "def function_result_07():\n",
        "\n",
        "    print('[plot the training regularization loss]')\n",
        "    print('') \n",
        "\n",
        "    plot_curve_error(loss_regularization_mean_train, loss_regularization_std_train, 'epoch', 'loss', 'training loss (regularization)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvRfH6ejA9Kl"
      },
      "source": [
        "def function_result_08():\n",
        "\n",
        "    print('[plot the training PSNR]')\n",
        "    print('') \n",
        "    \n",
        "    plot_curve_error(PSNR_mean_train, PSNR_std_train, 'epoch', 'PSNR', 'training PSNR')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08ILy67PA9Km"
      },
      "source": [
        "def function_result_09():\n",
        "    \n",
        "    print('[plot the testing loss]')\n",
        "    print('') \n",
        "    \n",
        "    plot_curve_error(loss_mean_test, loss_std_test, 'epoch', 'loss', 'testing loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzRElUYcREsr"
      },
      "source": [
        "def function_result_10():\n",
        "    \n",
        "    print('[plot the testing fidelity loss]')\n",
        "    print('') \n",
        "    \n",
        "    plot_curve_error(loss_fidelity_mean_test, loss_fidelity_std_test, 'epoch', 'loss', 'testing loss (fidelity)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cp0_wwYyRM8f"
      },
      "source": [
        "def function_result_11():\n",
        "    \n",
        "    print('[plot the testing regularization loss]')\n",
        "    print('') \n",
        "    \n",
        "    plot_curve_error(loss_regularization_mean_test, loss_regularization_std_test, 'epoch', 'loss', 'testing loss (regularization)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHDnt4ndA9Km"
      },
      "source": [
        "def function_result_12():\n",
        "    \n",
        "    print('[plot the testing PSNR]') \n",
        "    print('') \n",
        "    \n",
        "    plot_curve_error(PSNR_mean_test, PSNR_std_test, 'epoch', 'PSNR', 'testing PSNR')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HlkP764A9Km"
      },
      "source": [
        "def function_result_13():\n",
        "    \n",
        "    print('[print the training loss (mean) at the last 10 epochs]')\n",
        "    print('') \n",
        "\n",
        "    data_last = get_data_last(loss_mean_train, -10)\n",
        "    index = np.arange(0, 10)\n",
        "    print_curve(data_last, index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JAxHIggA9Kn"
      },
      "source": [
        "def function_result_14():\n",
        "    \n",
        "    print('[print the training PSNR (mean) at the last 10 epochs]')\n",
        "    print('') \n",
        "    \n",
        "    data_last = get_data_last(PSNR_mean_train, -10)\n",
        "    index = np.arange(0, 10)\n",
        "    print_curve(data_last, index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHP4nAgxA9Kn"
      },
      "source": [
        "def function_result_15():\n",
        "    \n",
        "    print('[print the testing loss (mean) at the last 10 epochs]')\n",
        "    print('') \n",
        "    \n",
        "    data_last = get_data_last(loss_mean_test, -10)\n",
        "    index = np.arange(0, 10)\n",
        "    print_curve(data_last, index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lslaY-A1A9Ko"
      },
      "source": [
        "def function_result_16():\n",
        "    \n",
        "    print('[print the testing PSNR (mean) at the last 10 epochs]')\n",
        "    print('') \n",
        "    \n",
        "    data_last = get_data_last(PSNR_mean_test, -10)\n",
        "    index = np.arange(0, 10)\n",
        "    print_curve(data_last, index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_xYLoXkA9Ko"
      },
      "source": [
        "def function_result_17():\n",
        "    \n",
        "    print('[print the best training PSNR (mean) within the last 10 epochs]')\n",
        "    print('') \n",
        "\n",
        "    value = get_max_last_range(PSNR_mean_train, -10)\n",
        "    print('best training PSNR = %12.10f' % (value))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-d0Zf7lsA9Ko"
      },
      "source": [
        "def function_result_18():\n",
        "    \n",
        "    print('[print the best testing PSNR (mean) within the last 10 epochs]')\n",
        "    print('') \n",
        "    \n",
        "    value = get_max_last_range(PSNR_mean_test, -10)\n",
        "    print('best testing PSNR = %12.10f' % (value))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-Nw6A2jA9Kp"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oa6-Ktz8A9Kp"
      },
      "source": [
        "# RESULTS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHREmjnvA9Kp"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cGBA-ccA9Kp"
      },
      "source": [
        "number_result = 18\n",
        "\n",
        "for i in range(number_result):\n",
        "\n",
        "    title           = '# RESULT # {:02d}'.format(i+1) \n",
        "    name_function   = 'function_result_{:02d}()'.format(i+1)\n",
        "\n",
        "    print('') \n",
        "    print('################################################################################')\n",
        "    print('#') \n",
        "    print(title)\n",
        "    print('#') \n",
        "    print('################################################################################')\n",
        "    print('') \n",
        "\n",
        "    eval(name_function)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjCUpOCI0gzH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPqP95L2U2Mh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}