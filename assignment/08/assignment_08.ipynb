{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "assignment_08_solution_latest.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "rgD-FF4MA9Kg",
        "SnBT9fWXA9Kj"
      ]
    },
    "interpreter": {
      "hash": "84bbda367bac7e7bffd9b7890a44d65326aaedad40e5a9021c2651157391b1ef"
    },
    "kernelspec": {
      "display_name": "Python 3.8.5 64-bit ('base': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e752e8c3c793411292d993699f9ef1a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_05a507dc9a9740d09fcf8779efdfbb48",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b226d9b0c9244ea6a9f7941f77ae035b",
              "IPY_MODEL_580aba4973874cd8b21958ecc3460b78",
              "IPY_MODEL_c107f2cab23f4561b98949ca851c58c2"
            ]
          }
        },
        "05a507dc9a9740d09fcf8779efdfbb48": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b226d9b0c9244ea6a9f7941f77ae035b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_453f806534e74eefb2de7b6d28016fa1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 57%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_839b849473e9411a90a398803c533597"
          }
        },
        "580aba4973874cd8b21958ecc3460b78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9b76c073d1024263b5a778fcdcbd8a58",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 300,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 171,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_eb695659a26c435da08a2126fcde7d16"
          }
        },
        "c107f2cab23f4561b98949ca851c58c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_aff91ab7db75448480147fe4713e9121",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 171/300 [15:32&lt;11:43,  5.45s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_422063876e2845118fa5c708ee1493fe"
          }
        },
        "453f806534e74eefb2de7b6d28016fa1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "839b849473e9411a90a398803c533597": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9b76c073d1024263b5a778fcdcbd8a58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "eb695659a26c435da08a2126fcde7d16": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "aff91ab7db75448480147fe4713e9121": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "422063876e2845118fa5c708ee1493fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQGQXmOpA9J-"
      },
      "source": [
        "# Unsupervised image denoising"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGBcGR2LA9KI"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNpI2DZ6A9KJ"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset\n",
        "from os import listdir\n",
        "from os.path import join\n",
        "from torchvision.transforms import Compose, ToTensor, ToPILImage, Resize, Lambda, Normalize, Grayscale\n",
        "from torch.utils.data import DataLoader\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from math import log10\n",
        "from tqdm.notebook import tqdm\n",
        "import os"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OU32jTFuA9KM"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gf-s-8nrA9KN",
        "outputId": "f5c92854-bf43-4723-f871-49fe9f14c2d0"
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "directory_data  = './drive/MyDrive/Machine_Learning/'\n",
        "filename_data   = 'assignment_08_data.npz'\n",
        "data            = np.load(os.path.join(directory_data, filename_data))\n",
        "\n",
        "\n",
        "original_train = data['original_train'] \n",
        "noise_train    = data['noise_train']\n",
        "\n",
        "original_test = data['original_test'] \n",
        "noise_test    = data['noise_test']\n",
        "\n",
        "print('*************************************************')\n",
        "print('size of noise_train    : ', noise_train.shape)\n",
        "print('*************************************************')\n",
        "print('size of noise_test    : ', noise_test.shape)\n",
        "print('*************************************************')\n",
        "print('number of training image :', noise_train.shape[0])\n",
        "print('height of training image :', noise_train.shape[1])\n",
        "print('width of training image  :', noise_train.shape[2])\n",
        "print('*************************************************')\n",
        "print('number of testing image :', noise_test.shape[0])\n",
        "print('height of testing image :', noise_test.shape[1])\n",
        "print('width of testing image  :', noise_test.shape[2])\n",
        "print('*************************************************')\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
            "*************************************************\n",
            "size of noise_train    :  (2000, 64, 64)\n",
            "*************************************************\n",
            "size of noise_test    :  (900, 64, 64)\n",
            "*************************************************\n",
            "number of training image : 2000\n",
            "height of training image : 64\n",
            "width of training image  : 64\n",
            "*************************************************\n",
            "number of testing image : 900\n",
            "height of testing image : 64\n",
            "width of testing image  : 64\n",
            "*************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20TiZsA_A9KO"
      },
      "source": [
        "## Hyper parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtDynD4nA9KP"
      },
      "source": [
        "device        = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# ==================================================\n",
        "# determine optimal hyper-parameters to obtain best testing performance\n",
        "number_epoch    = 300\n",
        "size_minibatch  = 50\n",
        "learning_rate   = 0.1\n",
        "momentum        = 0.9\n",
        "weight_decay    = 0.0001\n",
        "weight_total_variation = 0.001\n",
        "# =================================================="
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AbHCZPpA9KR"
      },
      "source": [
        "## Costumize dataloader for pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UveCjrBA9KS"
      },
      "source": [
        "class dataset (Dataset):\n",
        "  def  __init__(self, original,noise):\n",
        "\n",
        "    self.original = original\n",
        "    self.noise    = noise\n",
        "        \n",
        "  def __getitem__(self, index):\n",
        "\n",
        "    original    = self.original[index]\n",
        "    noise       = self.noise[index]\n",
        "    \n",
        "    original   = torch.FloatTensor(original).unsqueeze(dim=0)\n",
        "    noise      = torch.FloatTensor(noise).unsqueeze(dim=0)\n",
        "\n",
        "\n",
        "    return (original , noise)\n",
        "  \n",
        "  def __len__(self):\n",
        "\n",
        "     return self.original.shape[0]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "357alwWgA9KU"
      },
      "source": [
        "## Construct datasets and dataloaders for training and testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7JDqjSVA9KU"
      },
      "source": [
        "dataset_train = dataset(original_train, noise_train) \n",
        "dataset_test  = dataset(original_test, noise_test) \n",
        "\n",
        "dataloader_train = DataLoader(dataset_train, batch_size=size_minibatch, shuffle=True, drop_last=True, num_workers=2)\n",
        "dataloader_test  = DataLoader(dataset_test,  batch_size=size_minibatch, shuffle=False, drop_last=True, num_workers=2) "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DQ3_o-VA9KV"
      },
      "source": [
        "## Shape of the data with data loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCNL_T-HA9KW",
        "outputId": "3b961389-8575-410d-86a2-74d44b1a8bed"
      },
      "source": [
        "(original_train, noise_train)   = dataset_train[0]\n",
        "(original_test, noise_test)     = dataset_test[0]\n",
        "\n",
        "print('************************************************************')\n",
        "print('shape of the original image in the training dataset:', original_train.shape)\n",
        "print('shape of the noisy image in the training dataset:', noise_train.shape)\n",
        "print('************************************************************')\n",
        "print('shape of the original image in the testing dataset:', original_test.shape)\n",
        "print('shape of the noisy image in the testing dataset:', noise_test.shape)\n",
        "print('************************************************************')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "************************************************************\n",
            "shape of the original image in the training dataset: torch.Size([1, 64, 64])\n",
            "shape of the noisy image in the training dataset: torch.Size([1, 64, 64])\n",
            "************************************************************\n",
            "shape of the original image in the testing dataset: torch.Size([1, 64, 64])\n",
            "shape of the noisy image in the testing dataset: torch.Size([1, 64, 64])\n",
            "************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Yn9jMLrA9KX"
      },
      "source": [
        "## Class for the neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Esyw9M--A9KY"
      },
      "source": [
        "class conv2DBatchNormRelu(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_channels,\n",
        "            n_filters,\n",
        "            k_size,\n",
        "            stride,\n",
        "            padding,\n",
        "            bias=True,\n",
        "            dilation=1,\n",
        "            with_bn=True,\n",
        "    ):\n",
        "        super(conv2DBatchNormRelu, self).__init__()\n",
        "\n",
        "        conv_mod = nn.Conv2d(int(in_channels),\n",
        "                             int(n_filters),\n",
        "                             kernel_size=k_size,\n",
        "                             padding=padding,\n",
        "                             stride=stride,\n",
        "                             bias=bias,\n",
        "                             dilation=dilation, )\n",
        "\n",
        "        if with_bn:\n",
        "            self.cbr_unit = nn.Sequential(conv_mod,\n",
        "                                          nn.BatchNorm2d(int(n_filters)),\n",
        "                                          nn.ReLU(inplace=True))\n",
        "        else:\n",
        "            self.cbr_unit = nn.Sequential(conv_mod, nn.ReLU(inplace=True))\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        outputs = self.cbr_unit(inputs)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class segnetDown2(nn.Module):\n",
        "    def __init__(self, in_size, out_size):\n",
        "        super(segnetDown2, self).__init__()\n",
        "        self.conv1 = conv2DBatchNormRelu(in_size, out_size, 3, 1, 1)\n",
        "        self.conv2 = conv2DBatchNormRelu(out_size, out_size, 3, 1, 1)\n",
        "        self.maxpool_with_argmax = nn.MaxPool2d(2, 2, return_indices=True)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        outputs = self.conv1(inputs)\n",
        "        outputs = self.conv2(outputs)\n",
        "        unpooled_shape = outputs.size()\n",
        "        outputs, indices = self.maxpool_with_argmax(outputs)\n",
        "        return outputs, indices, unpooled_shape\n",
        "\n",
        "\n",
        "class segnetDown3(nn.Module):\n",
        "    def __init__(self, in_size, out_size):\n",
        "        super(segnetDown3, self).__init__()\n",
        "        self.conv1 = conv2DBatchNormRelu(in_size, out_size, 3, 1, 1)\n",
        "        self.conv2 = conv2DBatchNormRelu(out_size, out_size, 3, 1, 1)\n",
        "        self.conv3 = conv2DBatchNormRelu(out_size, out_size, 3, 1, 1)\n",
        "        self.maxpool_with_argmax = nn.MaxPool2d(2, 2, return_indices=True)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        outputs = self.conv1(inputs)\n",
        "        outputs = self.conv2(outputs)\n",
        "        outputs = self.conv3(outputs)\n",
        "        unpooled_shape = outputs.size()\n",
        "        outputs, indices = self.maxpool_with_argmax(outputs)\n",
        "        return outputs, indices, unpooled_shape\n",
        "\n",
        "\n",
        "class segnetUp2(nn.Module):\n",
        "    def __init__(self, in_size, out_size):\n",
        "        super(segnetUp2, self).__init__()\n",
        "        self.unpool = nn.MaxUnpool2d(2, 2)\n",
        "        self.conv1 = conv2DBatchNormRelu(in_size, in_size, 3, 1, 1)\n",
        "        self.conv2 = conv2DBatchNormRelu(in_size, out_size, 3, 1, 1)\n",
        "\n",
        "    def forward(self, inputs, indices, output_shape):\n",
        "        outputs = self.unpool(input=inputs, indices=indices, output_size=output_shape)\n",
        "        outputs = self.conv1(outputs)\n",
        "        outputs = self.conv2(outputs)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class segnetUp3(nn.Module):\n",
        "    def __init__(self, in_size, out_size):\n",
        "        super(segnetUp3, self).__init__()\n",
        "        self.unpool = nn.MaxUnpool2d(2, 2)\n",
        "        self.conv1 = conv2DBatchNormRelu(in_size, in_size, 3, 1, 1)\n",
        "        self.conv2 = conv2DBatchNormRelu(in_size, in_size, 3, 1, 1)\n",
        "        self.conv3 = conv2DBatchNormRelu(in_size, out_size, 3, 1, 1)\n",
        "\n",
        "    def forward(self, inputs, indices, output_shape):\n",
        "        outputs = self.unpool(input=inputs, indices=indices, output_size=output_shape)\n",
        "        outputs = self.conv1(outputs)\n",
        "        outputs = self.conv2(outputs)\n",
        "        outputs = self.conv3(outputs)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class SegNet(nn.Module):\n",
        "    def __init__(self, n_classes=1, in_channels=1, is_unpooling=True):\n",
        "        super(SegNet, self).__init__()\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.is_unpooling = is_unpooling\n",
        "\n",
        "        self.down1 = segnetDown2(self.in_channels, 64)\n",
        "        self.down2 = segnetDown2(64, 128)\n",
        "        self.down3 = segnetDown3(128, 256)\n",
        "        self.down4 = segnetDown3(256, 512)\n",
        "        self.down5 = segnetDown3(512, 512)\n",
        "\n",
        "        self.up5 = segnetUp3(512, 512)\n",
        "        self.up4 = segnetUp3(512, 256)\n",
        "        self.up3 = segnetUp3(256, 128)\n",
        "        self.up2 = segnetUp2(128, 64)\n",
        "        self.up1 = segnetUp2(64, n_classes)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "\n",
        "        down1, indices_1, unpool_shape1 = self.down1(inputs)\n",
        "        down2, indices_2, unpool_shape2 = self.down2(down1)\n",
        "        down3, indices_3, unpool_shape3 = self.down3(down2)\n",
        "        down4, indices_4, unpool_shape4 = self.down4(down3)\n",
        "        down5, indices_5, unpool_shape5 = self.down5(down4)\n",
        "\n",
        "        up5 = self.up5(down5, indices_5, unpool_shape5)\n",
        "        up4 = self.up4(up5, indices_4, unpool_shape4)\n",
        "        up3 = self.up3(up4, indices_3, unpool_shape3)\n",
        "        up2 = self.up2(up3, indices_2, unpool_shape2)\n",
        "        up1 = self.up1(up2, indices_1, unpool_shape1)\n",
        "\n",
        "        return up1"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTptbDGZA9KZ"
      },
      "source": [
        "## Build the network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MgUbMJdA9KZ"
      },
      "source": [
        "model = SegNet().to(device)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum , weight_decay=weight_decay)\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAMmqhrUA9KZ"
      },
      "source": [
        "## Compute prediction (denoised image)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sE58VDWA9Ka"
      },
      "source": [
        "def compute_prediction(input, model):\n",
        "    \n",
        "    # ==================================================\n",
        "    # fill up the blank\n",
        "    prediction = model(input)\n",
        "    # ==================================================\n",
        "    \n",
        "    return prediction"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4MU2uUwA9Kc"
      },
      "source": [
        "## Compute loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzLZ5_ijB13L"
      },
      "source": [
        "def compute_fidelity(input, prediction):\n",
        "    \n",
        "    mse = nn.MSELoss()\n",
        "    \n",
        "    # ==================================================\n",
        "    # fill up the blank\n",
        "    loss_mse = mse(prediction, input)\n",
        "    # ==================================================\n",
        "    \n",
        "    loss_mse_value = loss_mse.item() \n",
        "    \n",
        "    return loss_mse, loss_mse_value"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-J9-SmZeDDHW"
      },
      "source": [
        "def compute_regularization(data, weight):\n",
        "    \n",
        "    bs_img, c_img, h_img, w_img = data.size()\n",
        "   \n",
        "    tv_height = torch.abs(data[:, :, 1:, :] - data[:, :, :- 1, :]).sum()\n",
        "    tv_width  = torch.abs(data[:, :, :, 1:] - data[:, :, :, :-1]).sum()\n",
        "\n",
        "    total_variation = (tv_height + tv_width) / (bs_img * c_img * h_img * w_img)\n",
        "    \n",
        "    # ==================================================\n",
        "    # fill up the blank\n",
        "    loss_regularization = weight * total_variation\n",
        "    # ==================================================\n",
        "    \n",
        "    loss_regularization_value = loss_regularization.item()\n",
        "     \n",
        "    return loss_regularization, loss_regularization_value"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grQ11S2uA9Kc"
      },
      "source": [
        "def compute_loss(input, prediction, weight):\n",
        "\n",
        "    (loss_fidelity, loss_fidelity_value) = compute_fidelity(input, prediction)\n",
        "    (loss_regularization, loss_regularization_value) = compute_regularization(prediction, weight) \n",
        "\n",
        "    # ==================================================\n",
        "    # fill up the blank\n",
        "    loss = loss_fidelity + loss_regularization\n",
        "    # ==================================================\n",
        "    \n",
        "    loss_value = loss.item()\n",
        "    \n",
        "    return loss, loss_value , loss_fidelity_value, loss_regularization_value"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1EWYXG6A9Kc"
      },
      "source": [
        "## Compute PSNR metric"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtVX7nLCA9Kd"
      },
      "source": [
        "def compute_PSNR(mse):\n",
        "\n",
        "    if (mse==0.):\n",
        "        PSNR=100    \n",
        "    else :\n",
        "        PSNR=10*log10(1.0 / mse)\n",
        "        \n",
        "    return PSNR"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfi3toLTA9Kd"
      },
      "source": [
        "## Variable for the learning curves"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "re9vSZWLA9Kd"
      },
      "source": [
        "loss_fidelity_mean_train       = np.zeros(number_epoch)\n",
        "loss_fidelity_std_train        = np.zeros(number_epoch)\n",
        "loss_regularization_mean_train = np.zeros(number_epoch)\n",
        "loss_regularization_std_train  = np.zeros(number_epoch)\n",
        "loss_mean_train                = np.zeros(number_epoch)\n",
        "loss_std_train                 = np.zeros(number_epoch)\n",
        "PSNR_mean_train                = np.zeros(number_epoch)\n",
        "PSNR_std_train                 = np.zeros(number_epoch)\n",
        "\n",
        "loss_fidelity_mean_test       = np.zeros(number_epoch)\n",
        "loss_fidelity_std_test        = np.zeros(number_epoch)\n",
        "loss_regularization_mean_test = np.zeros(number_epoch)\n",
        "loss_regularization_std_test  = np.zeros(number_epoch)\n",
        "loss_mean_test                = np.zeros(number_epoch)\n",
        "loss_std_test                 = np.zeros(number_epoch)\n",
        "PSNR_mean_test                = np.zeros(number_epoch)\n",
        "PSNR_std_test                 = np.zeros(number_epoch)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2DK1BgcA9Ke"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6z6RHIxcA9Ke"
      },
      "source": [
        "def train(model, dataloader):\n",
        "\n",
        "    loss_epoch          = []\n",
        "    loss_fidelity_epoch = []\n",
        "    loss_reg_epoch      = []\n",
        "    psnr_epoch          = []\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for index_batch, (original, noise) in enumerate(dataloader):\n",
        "\n",
        "        original = original.to(device)\n",
        "        noise    = noise.to(device)\n",
        "        \n",
        "        prediction = compute_prediction(noise, model)\n",
        "        (loss, loss_value, loss_fidelity_value, loss_regularization_value) = compute_loss(noise, prediction, weight_total_variation)\n",
        "        \n",
        "        # ==================================================\n",
        "        # fill up the blank\n",
        "        (mse, mse_value)    = compute_fidelity(original, prediction)\n",
        "        psnr                = compute_PSNR(mse_value)\n",
        "        # ==================================================\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_epoch.append(loss_value)\n",
        "        loss_fidelity_epoch.append(loss_fidelity_value)\n",
        "        loss_reg_epoch.append(loss_regularization_value)\n",
        "        psnr_epoch.append(psnr)\n",
        "\n",
        "    loss_mean_epoch     = np.mean(loss_epoch)\n",
        "    loss_std_epoch      = np.std(loss_epoch)\n",
        "    \n",
        "    loss_fidelity_mean_epoch     = np.mean(loss_fidelity_epoch)\n",
        "    loss_fidelity_std_epoch      = np.std(loss_fidelity_epoch)\n",
        "    \n",
        "    loss_reg_mean_epoch     = np.mean(loss_reg_epoch)\n",
        "    loss_reg_std_epoch      = np.std(loss_reg_epoch)\n",
        "\n",
        "    psnr_mean_epoch = np.mean(psnr_epoch)\n",
        "    psnr_std_epoch  = np.std(psnr_epoch)\n",
        "\n",
        "    loss                       = {'mean' : loss_mean_epoch, 'std' : loss_std_epoch}\n",
        "    loss_fidelity              = {'mean' : loss_fidelity_mean_epoch, 'std' : loss_fidelity_std_epoch}\n",
        "    loss_regularization        = {'mean' : loss_reg_mean_epoch, 'std' : loss_reg_std_epoch}\n",
        "    psnr                       = {'mean' : psnr_mean_epoch, 'std' : psnr_std_epoch}\n",
        "\n",
        "    return (loss, loss_fidelity, loss_regularization, psnr)    \n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z76W6RQWA9Ke"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9FO3jlwA9Ke"
      },
      "source": [
        "def test(model, dataloader):\n",
        "\n",
        "    loss_epoch          = []\n",
        "    loss_fidelity_epoch = []\n",
        "    loss_reg_epoch      = []\n",
        "    psnr_epoch          = []\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    for index_batch, (original, noise) in enumerate(dataloader):\n",
        "\n",
        "        original = original.to(device)\n",
        "        noise    = noise.to(device)\n",
        "\n",
        "        prediction = compute_prediction(noise, model)\n",
        "        (loss, loss_value, loss_fidelity_value, loss_regularization_value) = compute_loss(noise, prediction, weight_total_variation)\n",
        "        \n",
        "        # ==================================================\n",
        "        # fill up the blank\n",
        "        (mse, mse_value)    = compute_fidelity(original, prediction)\n",
        "        psnr                = compute_PSNR(mse_value)\n",
        "        # ==================================================\n",
        "\n",
        "        loss_epoch.append(loss_value)\n",
        "        loss_fidelity_epoch.append(loss_fidelity_value)\n",
        "        loss_reg_epoch.append(loss_regularization_value)\n",
        "        psnr_epoch.append(psnr)\n",
        "\n",
        "    loss_mean_epoch     = np.mean(loss_epoch)\n",
        "    loss_std_epoch      = np.std(loss_epoch)\n",
        "    \n",
        "    loss_fidelity_mean_epoch     = np.mean(loss_fidelity_epoch)\n",
        "    loss_fidelity_std_epoch      = np.std(loss_fidelity_epoch)\n",
        "    \n",
        "    loss_reg_mean_epoch     = np.mean(loss_reg_epoch)\n",
        "    loss_reg_std_epoch      = np.std(loss_reg_epoch)\n",
        "\n",
        "    psnr_mean_epoch = np.mean(psnr_epoch)\n",
        "    psnr_std_epoch  = np.std(psnr_epoch)\n",
        "\n",
        "    loss                       = {'mean' : loss_mean_epoch, 'std' : loss_std_epoch}\n",
        "    loss_fidelity              = {'mean' : loss_fidelity_mean_epoch, 'std' : loss_fidelity_std_epoch}\n",
        "    loss_regularization        = {'mean' : loss_reg_mean_epoch, 'std' : loss_reg_std_epoch}\n",
        "    psnr                       = {'mean' : psnr_mean_epoch, 'std' : psnr_std_epoch}\n",
        "\n",
        "    return (loss, loss_fidelity, loss_regularization, psnr)    \n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nl_p_5B2A9Kf"
      },
      "source": [
        "## train and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 735,
          "referenced_widgets": [
            "e752e8c3c793411292d993699f9ef1a1",
            "05a507dc9a9740d09fcf8779efdfbb48",
            "b226d9b0c9244ea6a9f7941f77ae035b",
            "580aba4973874cd8b21958ecc3460b78",
            "c107f2cab23f4561b98949ca851c58c2",
            "453f806534e74eefb2de7b6d28016fa1",
            "839b849473e9411a90a398803c533597",
            "9b76c073d1024263b5a778fcdcbd8a58",
            "eb695659a26c435da08a2126fcde7d16",
            "aff91ab7db75448480147fe4713e9121",
            "422063876e2845118fa5c708ee1493fe"
          ]
        },
        "id": "-kpLyy6bA9Kf",
        "outputId": "9f99f739-ad17-43be-ceef-5d8d054a94f5"
      },
      "source": [
        "# ================================================================================\n",
        "# \n",
        "# iterations for epochs\n",
        "#\n",
        "# ================================================================================\n",
        "for i in tqdm(range(number_epoch)):\n",
        "    \n",
        "    # ================================================================================\n",
        "    # \n",
        "    # training\n",
        "    #\n",
        "    # ================================================================================\n",
        "    (loss_train, loss_fidelity_train, loss_reg_train, psnr_train) = train(model, dataloader_train)\n",
        "\n",
        "    loss_mean_train[i] = loss_train['mean']\n",
        "    loss_std_train[i]  = loss_train['std']\n",
        "    \n",
        "    loss_fidelity_mean_train[i] = loss_fidelity_train['mean']\n",
        "    loss_fidelity_std_train[i]  = loss_fidelity_train['std']\n",
        "    \n",
        "    loss_regularization_mean_train[i] = loss_reg_train['mean']\n",
        "    loss_regularization_std_train[i]  = loss_reg_train['std']\n",
        "\n",
        "    PSNR_mean_train[i]  = psnr_train['mean']\n",
        "    PSNR_std_train[i]   = psnr_train['std']\n",
        "\n",
        "    # ================================================================================\n",
        "    # \n",
        "    # testing\n",
        "    #\n",
        "    # ================================================================================\n",
        "    (loss_test, loss_fidelity_test, loss_reg_test, psnr_test) = test(model, dataloader_test)\n",
        "\n",
        "    loss_mean_test[i] = loss_test['mean']\n",
        "    loss_std_test[i]  = loss_test['std']\n",
        "\n",
        "    loss_fidelity_mean_test[i] = loss_fidelity_test['mean']\n",
        "    loss_fidelity_std_test[i]  = loss_fidelity_test['std']\n",
        "    \n",
        "    loss_regularization_mean_test[i] = loss_reg_test['mean']\n",
        "    loss_regularization_std_test[i]  = loss_reg_test['std']\n",
        "\n",
        "    PSNR_mean_test[i]  = psnr_test['mean']\n",
        "    PSNR_std_test[i]   = psnr_test['std']\n",
        "\n",
        "    if i % 5 == 0:\n",
        "        print(f\"epooch : {i}, train loss : {loss_train['mean']}, train PSNR : {PSNR_mean_train[i]}, test loss : {loss_mean_test[i]}, test PSNR : {PSNR_mean_test[i]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e752e8c3c793411292d993699f9ef1a1",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/300 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:693: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch._C._nn.max_pool2d_with_indices(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epooch : 0, train loss : 0.09872839553281665, train PSNR : 12.23716683803965, test loss : 0.02457203757431772, test PSNR : 16.510573767601784\n",
            "epooch : 5, train loss : 0.010338308615610003, train PSNR : 19.659744712790847, test loss : 0.009656354578004943, test PSNR : 19.859492525626827\n",
            "epooch : 10, train loss : 0.006968858407344669, train PSNR : 20.405755487122196, test loss : 0.006662040845387512, test PSNR : 20.594636697484916\n",
            "epooch : 15, train loss : 0.005878703691996634, train PSNR : 20.74183800382239, test loss : 0.005713503885393341, test PSNR : 20.970840545215342\n",
            "epooch : 20, train loss : 0.005424312851391733, train PSNR : 20.928155106104292, test loss : 0.005336463787696428, test PSNR : 20.88783391398368\n",
            "epooch : 25, train loss : 0.005085810751188547, train PSNR : 21.08851204885396, test loss : 0.0046195681206882, test PSNR : 21.34662757900828\n",
            "epooch : 30, train loss : 0.004710574424825609, train PSNR : 21.280540556115664, test loss : 0.004647737627642022, test PSNR : 21.469612453603343\n",
            "epooch : 35, train loss : 0.00449414782342501, train PSNR : 21.407962731812827, test loss : 0.004867327735862798, test PSNR : 21.086444171197556\n",
            "epooch : 40, train loss : 0.004413175425725058, train PSNR : 21.45751672219807, test loss : 0.0038798105282088122, test PSNR : 21.80772295664266\n",
            "epooch : 45, train loss : 0.00394245968782343, train PSNR : 21.713817784266304, test loss : 0.003766950090519256, test PSNR : 21.76534796189113\n",
            "epooch : 50, train loss : 0.003883235948160291, train PSNR : 21.80237615672295, test loss : 0.0036448483717524344, test PSNR : 22.023000385418488\n",
            "epooch : 55, train loss : 0.0037000897980760785, train PSNR : 21.87835892996249, test loss : 0.003589360547872881, test PSNR : 21.997258371765408\n",
            "epooch : 60, train loss : 0.003723890706896782, train PSNR : 21.885505359483542, test loss : 0.0034939082882677517, test PSNR : 21.926563161088083\n",
            "epooch : 65, train loss : 0.003864437888842076, train PSNR : 21.818102069015918, test loss : 0.0034813427288706103, test PSNR : 22.23571803189933\n",
            "epooch : 70, train loss : 0.004060561041114852, train PSNR : 21.696446256114534, test loss : 0.0032233845106222564, test PSNR : 22.16504663452749\n",
            "epooch : 75, train loss : 0.003590403188718483, train PSNR : 21.987306268575658, test loss : 0.0031209163781669405, test PSNR : 22.27130787013793\n",
            "epooch : 80, train loss : 0.0037218095210846515, train PSNR : 21.911033526124616, test loss : 0.0036351403169747856, test PSNR : 22.028058213249253\n",
            "epooch : 85, train loss : 0.0034318337915465237, train PSNR : 22.113414891001653, test loss : 0.0033902506628591153, test PSNR : 22.16215171075818\n",
            "epooch : 90, train loss : 0.0034412696841172874, train PSNR : 22.088722468750753, test loss : 0.0030269489054464633, test PSNR : 22.27851989012117\n",
            "epooch : 95, train loss : 0.0029117282247170806, train PSNR : 22.444948903418002, test loss : 0.0029253287033902276, test PSNR : 22.538084127072047\n",
            "epooch : 100, train loss : 0.0035192588693462312, train PSNR : 22.02844074921567, test loss : 0.0028024419831732907, test PSNR : 22.526089827367553\n",
            "epooch : 105, train loss : 0.0029910226294305176, train PSNR : 22.38744536397162, test loss : 0.002806144055082566, test PSNR : 22.641333202072396\n",
            "epooch : 110, train loss : 0.0029240717994980514, train PSNR : 22.435034882398618, test loss : 0.002715899725444615, test PSNR : 22.597775648337617\n",
            "epooch : 115, train loss : 0.0031153324409388004, train PSNR : 22.317781224220905, test loss : 0.0028317933643443715, test PSNR : 22.606883642504947\n",
            "epooch : 120, train loss : 0.0033954280836042015, train PSNR : 22.15084773298089, test loss : 0.002739980092479123, test PSNR : 22.755349854035483\n",
            "epooch : 125, train loss : 0.0031698555045295506, train PSNR : 22.313921833129495, test loss : 0.002711990097951558, test PSNR : 22.550715581115785\n",
            "epooch : 130, train loss : 0.0029268762620631604, train PSNR : 22.440316282943922, test loss : 0.002635014916045798, test PSNR : 22.595255709711417\n",
            "epooch : 135, train loss : 0.0030843725660815837, train PSNR : 22.38430620545396, test loss : 0.0025099256955501107, test PSNR : 22.729281304654304\n",
            "epooch : 140, train loss : 0.004233331250725314, train PSNR : 21.695573626145826, test loss : 0.0032914114805559316, test PSNR : 22.233491082802615\n",
            "epooch : 145, train loss : 0.0029248913400806487, train PSNR : 22.48226160912494, test loss : 0.0025182245428570444, test PSNR : 22.686641480914197\n",
            "epooch : 150, train loss : 0.002724837337154895, train PSNR : 22.57503695523666, test loss : 0.002525542694557872, test PSNR : 22.707727090845093\n",
            "epooch : 155, train loss : 0.002869301673490554, train PSNR : 22.48632338471059, test loss : 0.002724265408081313, test PSNR : 22.383481488536855\n",
            "epooch : 160, train loss : 0.0027702314662747087, train PSNR : 22.568980052218937, test loss : 0.0024134917184710503, test PSNR : 22.65816363065366\n",
            "epooch : 165, train loss : 0.0027769303182139994, train PSNR : 22.569942664355096, test loss : 0.0026649739463917082, test PSNR : 22.59764492195186\n",
            "epooch : 170, train loss : 0.002397875307360664, train PSNR : 22.853861244612926, test loss : 0.0024734840650732317, test PSNR : 23.052991514134558\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxek0ypkA9Kf"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4N32do7A9Kg"
      },
      "source": [
        "# functions for visualizing the results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cY1ul6ttA9Kg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgD-FF4MA9Kg"
      },
      "source": [
        "## Plot functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7SzyQnaA9Kg"
      },
      "source": [
        "def plot_data_grid(data, index_data, nRow, nCol):\n",
        "    \n",
        "    fig, axes = plt.subplots(nRow, nCol, constrained_layout=True, figsize=(nCol * 3, nRow * 3))\n",
        "\n",
        "    for i in range(nRow):\n",
        "        for j in range(nCol):\n",
        "\n",
        "            k       = i * nCol + j\n",
        "            index   = index_data[k]\n",
        "\n",
        "            axes[i, j].imshow(data[index], cmap='gray', vmin=0, vmax=1)\n",
        "            axes[i, j].xaxis.set_visible(False)\n",
        "            axes[i, j].yaxis.set_visible(False)\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVYA6nBGA9Kh"
      },
      "source": [
        "def plot_data_tensor_grid(data, index_data, nRow, nCol):\n",
        "    \n",
        "    fig, axes = plt.subplots(nRow, nCol, constrained_layout=True, figsize=(nCol * 3, nRow * 3))\n",
        "\n",
        "    data = data.detach().cpu().squeeze(axis=1)\n",
        "\n",
        "    for i in range(nRow):\n",
        "        for j in range(nCol):\n",
        "\n",
        "            k       = i * nCol + j\n",
        "            index   = index_data[k]\n",
        "\n",
        "            axes[i, j].imshow(data[index], cmap='gray', vmin=0, vmax=1)\n",
        "            axes[i, j].xaxis.set_visible(False)\n",
        "            axes[i, j].yaxis.set_visible(False)\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r06Lf6UNA9Kh"
      },
      "source": [
        "def plot_curve_error(data_mean, data_std, x_label, y_label, title):\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.title(title)\n",
        "\n",
        "    alpha = 0.3\n",
        "    \n",
        "    plt.plot(range(len(data_mean)), data_mean, '-', color = 'red')\n",
        "    plt.fill_between(range(len(data_mean)), data_mean - data_std, data_mean + data_std, facecolor = 'blue', alpha = alpha) \n",
        "    \n",
        "    plt.xlabel(x_label)\n",
        "    plt.ylabel(y_label)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBmjFM9tA9Kh"
      },
      "source": [
        "def print_curve(data, index):\n",
        "    \n",
        "    for i in range(len(index)):\n",
        "\n",
        "        idx = index[i]\n",
        "        val = data[idx]\n",
        "\n",
        "        print('index = %2d, value = %12.10f' % (idx, val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5_ePFORA9Ki"
      },
      "source": [
        "def get_data_last(data, index_start):\n",
        "\n",
        "    data_last = data[index_start:]\n",
        "\n",
        "    return data_last"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFNAv5nnA9Ki"
      },
      "source": [
        "def get_max_last_range(data, index_start):\n",
        "\n",
        "    data_range = get_data_last(data, index_start)\n",
        "    value = data_range.max()\n",
        "\n",
        "    return value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0FhjnneA9Ki"
      },
      "source": [
        "def get_min_last_range(data, index_start):\n",
        "\n",
        "    data_range = get_data_last(data, index_start)\n",
        "    value = data_range.min()\n",
        "\n",
        "    return value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZR2U5WquA9Kj"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnBT9fWXA9Kj"
      },
      "source": [
        "# functions for presenting the results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ak9mEQ11A9Kj"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mSysHDcA9Kk"
      },
      "source": [
        "def function_result_01():\n",
        "\n",
        "    print('[plot examples of the training images]')\n",
        "    print('') \n",
        "\n",
        "    nRow = 5 \n",
        "    nCol = 4\n",
        "    index_data  = np.arange(0, nRow * nCol)\n",
        "    image_train = dataset_train.noise[index_data]\n",
        "\n",
        "    plot_data_grid(image_train, index_data, nRow, nCol)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gG5Dv-BtA9Kk"
      },
      "source": [
        "def function_result_02():\n",
        "\n",
        "    print('[plot examples of the training denoising results]')\n",
        "    print('') \n",
        "\n",
        "    nRow = 5\n",
        "    nCol = 4\n",
        "    index_data          = np.arange(0, nRow * nCol)\n",
        "    image_train         = torch.FloatTensor(dataset_train.noise[index_data]).unsqueeze(dim=1).to(device)\n",
        "    prediction_train    = compute_prediction(model, image_train)\n",
        "    \n",
        "    plot_data_tensor_grid(prediction_train, index_data, nRow, nCol)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AOFrS4wA9Kk"
      },
      "source": [
        "def function_result_03():\n",
        "\n",
        "    print('[plot examples of the testing images]')\n",
        "    print('') \n",
        "    \n",
        "    nRow = 5\n",
        "    nCol = 4\n",
        "    index_data = np.arange(0, nRow * nCol)\n",
        "    image_test = dataset_test.noise[index_data]\n",
        "\n",
        "    plot_data_grid(image_test, index_data, nRow, nCol)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2jAuO-HA9Kl"
      },
      "source": [
        "def function_result_04():\n",
        "\n",
        "    print('[plot examples of the testing denoising results]')\n",
        "    print('') \n",
        "\n",
        "    nRow = 5\n",
        "    nCol = 4\n",
        "    index_data      = np.arange(0, nRow * nCol)\n",
        "    image_test      = torch.FloatTensor(dataset_test.noise[index_data]).unsqueeze(dim=1).to(device)\n",
        "    prediction_test = compute_prediction(model, image_test)\n",
        "\n",
        "    plot_data_tensor_grid(prediction_test, index_data, nRow, nCol)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4q_8MOVKA9Kl"
      },
      "source": [
        "def function_result_05():\n",
        "\n",
        "    print('[plot the training loss]')\n",
        "    print('') \n",
        "\n",
        "    plot_curve_error(loss_mean_train, loss_std_train, 'epoch', 'loss', 'training loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4aH0BefQhUi"
      },
      "source": [
        "def function_result_06():\n",
        "\n",
        "    print('[plot the training fidelity loss]')\n",
        "    print('') \n",
        "\n",
        "    plot_curve_error(loss_fidelity_mean_train, loss_fidelity_std_train, 'epoch', 'loss', 'training loss (fidelity)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBHnNBNcQw9a"
      },
      "source": [
        "def function_result_07():\n",
        "\n",
        "    print('[plot the training regularization loss]')\n",
        "    print('') \n",
        "\n",
        "    plot_curve_error(loss_regularization_mean_train, loss_regularization_std_train, 'epoch', 'loss', 'training loss (regularization)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvRfH6ejA9Kl"
      },
      "source": [
        "def function_result_08():\n",
        "\n",
        "    print('[plot the training PSNR]')\n",
        "    print('') \n",
        "    \n",
        "    plot_curve_error(PSNR_mean_train, PSNR_std_train, 'epoch', 'PSNR', 'training PSNR')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08ILy67PA9Km"
      },
      "source": [
        "def function_result_09():\n",
        "    \n",
        "    print('[plot the testing loss]')\n",
        "    print('') \n",
        "    \n",
        "    plot_curve_error(loss_mean_test, loss_std_test, 'epoch', 'loss', 'testing loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzRElUYcREsr"
      },
      "source": [
        "def function_result_10():\n",
        "    \n",
        "    print('[plot the testing fidelity loss]')\n",
        "    print('') \n",
        "    \n",
        "    plot_curve_error(loss_fidelity_mean_test, loss_fidelity_std_test, 'epoch', 'loss', 'testing loss (fidelity)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cp0_wwYyRM8f"
      },
      "source": [
        "def function_result_11():\n",
        "    \n",
        "    print('[plot the testing regularization loss]')\n",
        "    print('') \n",
        "    \n",
        "    plot_curve_error(loss_regularization_mean_test, loss_regularization_std_test, 'epoch', 'loss', 'testing loss (regularization)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHDnt4ndA9Km"
      },
      "source": [
        "def function_result_12():\n",
        "    \n",
        "    print('[plot the testing PSNR]') \n",
        "    print('') \n",
        "    \n",
        "    plot_curve_error(PSNR_mean_test, PSNR_std_test, 'epoch', 'PSNR', 'testing PSNR')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HlkP764A9Km"
      },
      "source": [
        "def function_result_13():\n",
        "    \n",
        "    print('[print the training loss (mean) at the last 10 epochs]')\n",
        "    print('') \n",
        "\n",
        "    data_last = get_data_last(loss_mean_train, -10)\n",
        "    index = np.arange(0, 10)\n",
        "    print_curve(data_last, index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JAxHIggA9Kn"
      },
      "source": [
        "def function_result_14():\n",
        "    \n",
        "    print('[print the training PSNR (mean) at the last 10 epochs]')\n",
        "    print('') \n",
        "    \n",
        "    data_last = get_data_last(PSNR_mean_train, -10)\n",
        "    index = np.arange(0, 10)\n",
        "    print_curve(data_last, index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHP4nAgxA9Kn"
      },
      "source": [
        "def function_result_15():\n",
        "    \n",
        "    print('[print the testing loss (mean) at the last 10 epochs]')\n",
        "    print('') \n",
        "    \n",
        "    data_last = get_data_last(loss_mean_test, -10)\n",
        "    index = np.arange(0, 10)\n",
        "    print_curve(data_last, index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lslaY-A1A9Ko"
      },
      "source": [
        "def function_result_16():\n",
        "    \n",
        "    print('[print the testing PSNR (mean) at the last 10 epochs]')\n",
        "    print('') \n",
        "    \n",
        "    data_last = get_data_last(PSNR_mean_test, -10)\n",
        "    index = np.arange(0, 10)\n",
        "    print_curve(data_last, index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_xYLoXkA9Ko"
      },
      "source": [
        "def function_result_17():\n",
        "    \n",
        "    print('[print the best training PSNR (mean) within the last 10 epochs]')\n",
        "    print('') \n",
        "\n",
        "    value = get_max_last_range(PSNR_mean_train, -10)\n",
        "    print('best training PSNR = %12.10f' % (value))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-d0Zf7lsA9Ko"
      },
      "source": [
        "def function_result_18():\n",
        "    \n",
        "    print('[print the best testing PSNR (mean) within the last 10 epochs]')\n",
        "    print('') \n",
        "    \n",
        "    value = get_max_last_range(PSNR_mean_test, -10)\n",
        "    print('best testing PSNR = %12.10f' % (value))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-Nw6A2jA9Kp"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oa6-Ktz8A9Kp"
      },
      "source": [
        "# RESULTS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHREmjnvA9Kp"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cGBA-ccA9Kp"
      },
      "source": [
        "number_result = 18\n",
        "\n",
        "for i in range(number_result):\n",
        "\n",
        "    title           = '# RESULT # {:02d}'.format(i+1) \n",
        "    name_function   = 'function_result_{:02d}()'.format(i+1)\n",
        "\n",
        "    print('') \n",
        "    print('################################################################################')\n",
        "    print('#') \n",
        "    print(title)\n",
        "    print('#') \n",
        "    print('################################################################################')\n",
        "    print('') \n",
        "\n",
        "    eval(name_function)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPqP95L2U2Mh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}