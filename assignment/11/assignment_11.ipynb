{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment_02.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "interpreter": {
      "hash": "17ed1555cfbb96ddcf655400d6c25a9cebe961c1b69daf25bae91d698acdd2a7"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 64-bit ('hsh': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8459efec99e9426fbd500b961418b9ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_5c87c1c2ab354810aadf2a4233669930",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_6f94af875b184a11814f1f1531218c18",
              "IPY_MODEL_17303bf8aa144a8893c69ffc968fb84a",
              "IPY_MODEL_b6e2739a9208424591058f1e298595d0"
            ]
          }
        },
        "5c87c1c2ab354810aadf2a4233669930": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6f94af875b184a11814f1f1531218c18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_547990e994c14d6288f8afa6bb3221cd",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "  0%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ba06544c87f642e3b4df61207b66baf4"
          }
        },
        "17303bf8aa144a8893c69ffc968fb84a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1795bb4ae03d40b6b4aae81227b45cca",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 150,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bd581e685ccb475f8a4734fbb385fcb1"
          }
        },
        "b6e2739a9208424591058f1e298595d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b4ddf1cbe70d42b59be2f079c8b48ba8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0/150 [00:02&lt;?, ?it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_670eb6ba294c4a348995f192698cd701"
          }
        },
        "547990e994c14d6288f8afa6bb3221cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ba06544c87f642e3b4df61207b66baf4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1795bb4ae03d40b6b4aae81227b45cca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bd581e685ccb475f8a4734fbb385fcb1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b4ddf1cbe70d42b59be2f079c8b48ba8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "670eb6ba294c4a348995f192698cd701": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K29I-OwCEYzW",
        "outputId": "db8b297c-a4b8-457a-b43f-7570135306c7"
      },
      "source": [
        "# Image De-blurring by Supervised Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIf5Ir7Y58d7"
      },
      "source": [
        "## import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PipKN2o558d7"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from math import log10\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "import random\n",
        "import os\n",
        "import functools"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YgxGDxz58d8"
      },
      "source": [
        "## load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tWoZVzc58d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad29e3f0-7438-4ed2-fd2d-8231c081a6b1"
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "directory_data  = './drive/MyDrive/Machine_Learning/'\n",
        "filename_data   = 'assignment_11_data.npz'\n",
        "data            = np.load(os.path.join(directory_data, filename_data))\n",
        "\n",
        "original_train  = data['original_train']\n",
        "blur_train      = data['blur_train']\n",
        "\n",
        "original_test   = data['original_test']\n",
        "blur_test       = data['blur_test']\n",
        "\n",
        "num_data_train  = original_train.shape[0]\n",
        "num_data_test   = original_test.shape[0]\n",
        "\n",
        "print('*************************************************')\n",
        "print('size of original_train :', original_train.shape)\n",
        "print('size of blur_train :', blur_train.shape)\n",
        "print('*************************************************')\n",
        "print('size of original_test :', original_test.shape)\n",
        "print('size of blur_test :', blur_test.shape)\n",
        "print('*************************************************')\n",
        "print('number of training image :', original_train.shape[0])\n",
        "print('height of training image :', original_train.shape[1])\n",
        "print('width of training image :', original_train.shape[2])\n",
        "print('*************************************************')\n",
        "print('number of testing image :', original_test.shape[0])\n",
        "print('height of testing image :', original_test.shape[1])\n",
        "print('width of testing image :', original_test.shape[2])\n",
        "print('*************************************************')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
            "*************************************************\n",
            "size of original_train : (200, 256, 256)\n",
            "size of blur_train : (200, 256, 256)\n",
            "*************************************************\n",
            "size of original_test : (100, 256, 256)\n",
            "size of blur_test : (100, 256, 256)\n",
            "*************************************************\n",
            "number of training image : 200\n",
            "height of training image : 256\n",
            "width of training image : 256\n",
            "*************************************************\n",
            "number of testing image : 100\n",
            "height of testing image : 256\n",
            "width of testing image : 256\n",
            "*************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gjKHmw158d-"
      },
      "source": [
        "## hyper-parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcrXwaxm58d-"
      },
      "source": [
        "device          = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "number_epoch    = 150\n",
        "size_minibatch  = 32\n",
        "learning_rate   = 0.1\n",
        "weight_decay    = 0.1"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIte042j58d_"
      },
      "source": [
        "## custom data loader for the PyTorch framework"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGu6lL6E58d_"
      },
      "source": [
        "class dataset(Dataset):\n",
        "    \n",
        "    def __init__(self, original, blur, transform=False):\n",
        "        \n",
        "        self.original   = original\n",
        "        self.blur       = blur \n",
        "        self.transform = transform\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        \n",
        "        original    = self.original[index]\n",
        "        blur        = self.blur[index]\n",
        "        \n",
        "        original    = torch.FloatTensor(original).unsqueeze(dim=0)\n",
        "        blur        = torch.FloatTensor(blur).unsqueeze(dim=0)\n",
        "\n",
        "        if self.transform:\n",
        "\n",
        "            crop_size = [128, 128]\n",
        "            # random crop\n",
        "            top         = random.randint(0, original.shape[1] - crop_size[0])\n",
        "            left        = random.randint(0, original.shape[2] - crop_size[1])\n",
        "            original    = transforms.functional.crop(original, top, left, crop_size[0], crop_size[1])\n",
        "            blur        = transforms.functional.crop(blur, top, left, crop_size[0], crop_size[1])\n",
        "            \n",
        "            # random horizontal flip\n",
        "            if random.random() > 0.5: \n",
        "                original = transforms.functional.hflip(original)\n",
        "                blur = transforms.functional.hflip(blur)\n",
        "\n",
        "            # random vertical flip\n",
        "            if random.random() > 0.5: \n",
        "                original = transforms.functional.vflip(original)\n",
        "                blur = transforms.functional.vflip(blur)\n",
        "\n",
        "        return (original, blur)\n",
        "    \n",
        "    def __len__(self):\n",
        "        \n",
        "        return self.original.shape[0]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LplWTHGIIFkK"
      },
      "source": [
        "# original_train_2 = np.copy(original_train)\n",
        "# original_train_3 = np.copy(original_train)\n",
        "# x_train = np.concatenate([original_train, original_train_2, original_train_3], axis=0)\n",
        "# x_train.shape\n",
        "\n",
        "# blur_train_2 = np.copy(blur_train)\n",
        "# blur_train_3 = np.copy(blur_train)\n",
        "# y_train = np.concatenate([blur_train, blur_train_2, blur_train_3], axis=0)\n",
        "# y_train.shape"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPxgholv58eI"
      },
      "source": [
        "## construct datasets and dataloaders for training and testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xP694DZ158eJ"
      },
      "source": [
        "dataset_train_transform = dataset(original_train, original_train, transform=True)\n",
        "dataset_train           = dataset(original_train, original_train)\n",
        "dataset_test            = dataset(original_test, blur_test)\n",
        "\n",
        "dataloader_train_transform  = DataLoader(dataset_train_transform, batch_size=size_minibatch, shuffle=True, drop_last=True)\n",
        "dataloader_train            = DataLoader(dataset_train, batch_size=1, shuffle=False, drop_last=True)\n",
        "dataloader_test             = DataLoader(dataset_test, batch_size=1, shuffle=False, drop_last=True)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iM4oa2U658eJ"
      },
      "source": [
        "## shape of the data when using the data loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnYuBNnW58eJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62cf46dd-885d-4d60-8c59-84ac4da457b8"
      },
      "source": [
        "(original_train, blur_train)  = dataset_train[0]\n",
        "(original_test, blur_test)    = dataset_test[0]\n",
        "(original_train_transform, blur_train_transform)  = dataset_train_transform[0]\n",
        "print('*******************************************************************')\n",
        "print('shape of the original in the training dataset:', original_train.shape)\n",
        "print('shape of the blur in the training dataset:', blur_train.shape)\n",
        "print('*******************************************************************')\n",
        "print('shape of the original in the testing dataset:', original_test.shape)\n",
        "print('shape of the blur in the testing dataset:', blur_test.shape)\n",
        "print('*******************************************************************')\n",
        "print('shape of the original in the training transform dataset:', original_train_transform.shape)\n",
        "print('shape of the blur in the training transform dataset:', blur_train_transform.shape)\n",
        "print('*******************************************************************')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*******************************************************************\n",
            "shape of the original in the training dataset: torch.Size([1, 256, 256])\n",
            "shape of the blur in the training dataset: torch.Size([1, 256, 256])\n",
            "*******************************************************************\n",
            "shape of the original in the testing dataset: torch.Size([1, 256, 256])\n",
            "shape of the blur in the testing dataset: torch.Size([1, 256, 256])\n",
            "*******************************************************************\n",
            "shape of the original in the training transform dataset: torch.Size([1, 128, 128])\n",
            "shape of the blur in the training transform dataset: torch.Size([1, 128, 128])\n",
            "*******************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtcjhXFB58eK"
      },
      "source": [
        "## class for the neural network "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWhdRdjR58eK"
      },
      "source": [
        "def get_norm_layer(norm_type='batch'):\n",
        "    if norm_type == 'batch':\n",
        "        norm_layer = functools.partial(nn.BatchNorm2d, affine=True)\n",
        "    elif norm_type == 'instance':\n",
        "        norm_layer = functools.partial(nn.InstanceNorm2d, affine=False, track_running_stats=True)\n",
        "    else:\n",
        "        raise NotImplementedError('normalization layer [%s] is not found' % norm_type)\n",
        "    return norm_layer\n",
        "\n",
        "class ResnetGenerator(nn.Module):\n",
        "    def __init__(self, input_nc=1, output_nc=1, ngf=64, norm_layer=nn.BatchNorm2d, use_dropout=False, n_blocks=6, use_parallel=True, learn_residual=True, padding_type='reflect'):\n",
        "        assert(n_blocks >= 0)\n",
        "        super(ResnetGenerator, self).__init__()\n",
        "        self.input_nc = input_nc\n",
        "        self.output_nc = output_nc\n",
        "        self.ngf = ngf\n",
        "        self.use_parallel = use_parallel\n",
        "        self.learn_residual = learn_residual\n",
        "        if type(norm_layer) == functools.partial:\n",
        "            use_bias = norm_layer.func == nn.InstanceNorm2d\n",
        "        else:\n",
        "            use_bias = norm_layer == nn.InstanceNorm2d\n",
        "\n",
        "        model = [nn.ReflectionPad2d(3),\n",
        "                 nn.Conv2d(input_nc, ngf, kernel_size=7, padding=0,\n",
        "                           bias=use_bias),\n",
        "                 norm_layer(ngf),\n",
        "                 nn.ReLU(True)]\n",
        "\n",
        "        n_downsampling = 2\n",
        "        for i in range(n_downsampling):\n",
        "            mult = 2**i\n",
        "            model += [nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3,\n",
        "                                stride=2, padding=1, bias=use_bias),\n",
        "                      norm_layer(ngf * mult * 2),\n",
        "                      nn.ReLU(True)]\n",
        "\n",
        "        mult = 2**n_downsampling\n",
        "        for i in range(n_blocks):\n",
        "            model += [ResnetBlock(ngf * mult, padding_type=padding_type, norm_layer=norm_layer, use_dropout=use_dropout, use_bias=use_bias)]\n",
        "\n",
        "        for i in range(n_downsampling):\n",
        "            mult = 2**(n_downsampling - i)\n",
        "            model += [nn.ConvTranspose2d(ngf * mult, int(ngf * mult / 2),\n",
        "                                         kernel_size=3, stride=2,\n",
        "                                         padding=1, output_padding=1,\n",
        "                                         bias=use_bias),\n",
        "                      norm_layer(int(ngf * mult / 2)),\n",
        "                      nn.ReLU(True)]\n",
        "        model += [nn.ReflectionPad2d(3)]\n",
        "        model += [nn.Conv2d(ngf, output_nc, kernel_size=7, padding=0)]\n",
        "        model += [nn.Tanh()]\n",
        "\n",
        "        self.model = nn.Sequential(*model)\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = self.model(input)\n",
        "        if self.learn_residual:\n",
        "            output = input + output\n",
        "            output = torch.clamp(output,min = -1,max = 1)\n",
        "        return output\n",
        "\n",
        "\n",
        "# Define a resnet block\n",
        "class ResnetBlock(nn.Module):\n",
        "    def __init__(self, dim, padding_type, norm_layer, use_dropout, use_bias):\n",
        "        super(ResnetBlock, self).__init__()\n",
        "        self.conv_block = self.build_conv_block(dim, padding_type, norm_layer, use_dropout, use_bias)\n",
        "\n",
        "    def build_conv_block(self, dim, padding_type, norm_layer, use_dropout, use_bias):\n",
        "        conv_block = []\n",
        "        p = 0\n",
        "        if padding_type == 'reflect':\n",
        "            conv_block += [nn.ReflectionPad2d(1)]\n",
        "        elif padding_type == 'replicate':\n",
        "            conv_block += [nn.ReplicationPad2d(1)]\n",
        "        elif padding_type == 'zero':\n",
        "            p = 1\n",
        "        else:\n",
        "            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n",
        "\n",
        "        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias),\n",
        "                       norm_layer(dim),\n",
        "                       nn.ReLU(True)]\n",
        "        if use_dropout:\n",
        "            conv_block += [nn.Dropout(0.5)]\n",
        "\n",
        "        p = 0\n",
        "        if padding_type == 'reflect':\n",
        "            conv_block += [nn.ReflectionPad2d(1)]\n",
        "        elif padding_type == 'replicate':\n",
        "            conv_block += [nn.ReplicationPad2d(1)]\n",
        "        elif padding_type == 'zero':\n",
        "            p = 1\n",
        "        else:\n",
        "            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n",
        "        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias),\n",
        "                       norm_layer(dim)]\n",
        "\n",
        "        return nn.Sequential(*conv_block)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = x + self.conv_block(x)\n",
        "        return out\n",
        "\n",
        "# Defines the PatchGAN discriminator with the specified arguments.\n",
        "class NLayerDiscriminator(nn.Module):\n",
        "    def __init__(self, input_nc=1, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d, use_sigmoid=False, use_parallel=True):\n",
        "        super(NLayerDiscriminator, self).__init__()\n",
        "        self.use_parallel = use_parallel\n",
        "        if type(norm_layer) == functools.partial:\n",
        "            use_bias = norm_layer.func == nn.InstanceNorm2d\n",
        "        else:\n",
        "            use_bias = norm_layer == nn.InstanceNorm2d\n",
        "\n",
        "        kw = 3\n",
        "        padw = int(np.ceil((kw-1)/2))\n",
        "        sequence = [\n",
        "            nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=1, padding=padw),\n",
        "            nn.LeakyReLU(0.2, True)\n",
        "        ]\n",
        "\n",
        "        nf_mult = 1\n",
        "        for n in range(1, n_layers):\n",
        "            nf_mult_prev = nf_mult\n",
        "            nf_mult = min(2**n, 8)\n",
        "            sequence += [\n",
        "                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult,\n",
        "                          kernel_size=kw, stride=1, padding=padw, bias=use_bias),\n",
        "                norm_layer(ndf * nf_mult),\n",
        "                nn.LeakyReLU(0.2, True)\n",
        "            ]\n",
        "\n",
        "        nf_mult_prev = nf_mult\n",
        "        nf_mult = min(2**n_layers, 8)\n",
        "        sequence += [\n",
        "            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult,\n",
        "                      kernel_size=kw, stride=1, padding=padw, bias=use_bias),\n",
        "            norm_layer(ndf * nf_mult),\n",
        "            nn.LeakyReLU(0.2, True)\n",
        "        ]\n",
        "\n",
        "        sequence += [nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)]\n",
        "\n",
        "        if use_sigmoid:\n",
        "            sequence += [nn.Sigmoid()]\n",
        "\n",
        "        self.model = nn.Sequential(*sequence)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.model(input)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoSBRkyC58eL"
      },
      "source": [
        "## build network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfnusTr458eL"
      },
      "source": [
        "generator = ResnetGenerator(norm_layer=get_norm_layer()).to(device)\n",
        "discriminator = NLayerDiscriminator(use_sigmoid=False).to(device)\n",
        "\n",
        "optimizer_generator     = torch.optim.Adam(generator.parameters(), lr=learning_rate)\n",
        "optimizer_discriminator = torch.optim.Adam(discriminator.parameters(), lr=learning_rate)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lw3RDh_Q-e2K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "147fa66a-cef4-4e2a-bd7b-41c9c1ed65ff"
      },
      "source": [
        "\n",
        "from torchsummary import summary\n",
        "summary(discriminator, (1, 128, 128))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 128, 128]             640\n",
            "         LeakyReLU-2         [-1, 64, 128, 128]               0\n",
            "            Conv2d-3        [-1, 128, 128, 128]          73,728\n",
            "       BatchNorm2d-4        [-1, 128, 128, 128]             256\n",
            "         LeakyReLU-5        [-1, 128, 128, 128]               0\n",
            "            Conv2d-6        [-1, 256, 128, 128]         294,912\n",
            "       BatchNorm2d-7        [-1, 256, 128, 128]             512\n",
            "         LeakyReLU-8        [-1, 256, 128, 128]               0\n",
            "            Conv2d-9        [-1, 512, 128, 128]       1,179,648\n",
            "      BatchNorm2d-10        [-1, 512, 128, 128]           1,024\n",
            "        LeakyReLU-11        [-1, 512, 128, 128]               0\n",
            "           Conv2d-12          [-1, 1, 128, 128]           4,609\n",
            "================================================================\n",
            "Total params: 1,555,329\n",
            "Trainable params: 1,555,329\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.06\n",
            "Forward/backward pass size (MB): 352.12\n",
            "Params size (MB): 5.93\n",
            "Estimated Total Size (MB): 358.12\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zh67yF8b58eM"
      },
      "source": [
        "## compute the prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjcB2BQZ58eM"
      },
      "source": [
        "def compute_prediction(model, input):\n",
        "    # ==================================================\n",
        "    # fill up the blank\n",
        "    #\n",
        "    prediction = model(input) \n",
        "    # \n",
        "    # ==================================================\n",
        "\n",
        "    return prediction"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01PxjMFy58eM"
      },
      "source": [
        "## compute the loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25NLckq058eM"
      },
      "source": [
        "def compute_bce_loss(prediction, label):\n",
        "    # ==================================================\n",
        "    # fill up the blank\n",
        "    #\n",
        "    criterion = nn.BCELoss()\n",
        "    # \n",
        "    # ==================================================\n",
        "    loss        = criterion(prediction, label)\n",
        "\n",
        "    return loss"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62kaDJkXe03a"
      },
      "source": [
        "def compute_mse_loss(prediction, label):\n",
        "    # ==================================================\n",
        "    # fill up the blank\n",
        "    #\n",
        "    criterion = nn.MSELoss()\n",
        "    # \n",
        "    # ==================================================\n",
        "    loss        = criterion(prediction, label)\n",
        "\n",
        "    return loss"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wN_lrKE58eN"
      },
      "source": [
        "## compute the accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnZR1Nnc58eN"
      },
      "source": [
        "def compute_accuracy(prediction, label):\n",
        "\n",
        "    prediction  = prediction.squeeze(axis=1)\n",
        "    label       = label.squeeze(axis=1)\n",
        "    mse_loss    = torch.mean((prediction - label) ** 2)\n",
        "\n",
        "    if mse_loss == 0.0:\n",
        "        psnr = 100\n",
        "    else:\n",
        "        psnr = 10 * torch.log10(1 / mse_loss)\n",
        "\n",
        "    psnr = psnr.item()\n",
        "    \n",
        "    return psnr"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMB3mjKN58eN"
      },
      "source": [
        "## variables for the learning curve"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVgQ4Uzf58eN"
      },
      "source": [
        "loss_mean_train     = np.zeros(number_epoch)\n",
        "loss_std_train      = np.zeros(number_epoch)\n",
        "accuracy_mean_train = np.zeros(number_epoch)\n",
        "accuracy_std_train  = np.zeros(number_epoch)\n",
        "\n",
        "loss_mean_test      = np.zeros(number_epoch)\n",
        "loss_std_test       = np.zeros(number_epoch)\n",
        "accuracy_mean_test  = np.zeros(number_epoch)\n",
        "accuracy_std_test   = np.zeros(number_epoch)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SRRTxcB58eN"
      },
      "source": [
        "## train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUxTaZWJ58eO"
      },
      "source": [
        "def train(dataloader):\n",
        "\n",
        "    loss_epoch      = []\n",
        "    accuracy_epoch  = []\n",
        "\n",
        "    generator.train()\n",
        "    discriminator.train()\n",
        "\n",
        "    for index_batch, (original, blur) in enumerate(dataloader):\n",
        "\n",
        "        yb    = original.to(device)\n",
        "        xb    = blur.to(device)\n",
        "    \n",
        "        yb_real = torch.ones_like(yb).to(device)\n",
        "        yb_fake = torch.zeros_like(yb).to(device)\n",
        "\n",
        "        # generator\n",
        "        optimizer_generator.zero_grad()\n",
        "        noise = torch.rand(size_minibatch, 1, 128, 128, device=device)\n",
        "        out_gen = generator(noise)\n",
        "        out_dis = discriminator(out_gen)\n",
        "\n",
        "        loss_gen = compute_mse_loss(out_dis, yb_real)\n",
        "        loss_gen.backward()\n",
        "        optimizer_generator.step()\n",
        "\n",
        "        # discriminator\n",
        "        optimizer_discriminator.zero_grad()\n",
        "        out_real = discriminator(xb)\n",
        "        out_fake = discriminator(out_gen.detach())\n",
        "        loss_real = compute_bce_loss(out_real, yb_real)\n",
        "        loss_fake = compute_bce_loss(out_fake, yb_fake)\n",
        "        loss_dis  = (loss_real + loss_fake) / 2\n",
        "\n",
        "        loss_dis.backward()\n",
        "        optimizer_discriminator.step()\n",
        "\n",
        "        accuracy    = compute_accuracy(out_real, yb)\n",
        "        loss_epoch.append(loss_dis.item())\n",
        "        accuracy_epoch.append(accuracy)\n",
        "\n",
        "    loss_mean_epoch     = np.mean(loss_epoch)\n",
        "    loss_std_epoch      = np.std(loss_epoch)\n",
        "\n",
        "    accuracy_mean_epoch = np.mean(accuracy_epoch)\n",
        "    accuracy_std_epoch  = np.std(accuracy_epoch)\n",
        "\n",
        "    loss        = {'mean' : loss_mean_epoch, 'std' : loss_std_epoch}\n",
        "    accuracy    = {'mean' : accuracy_mean_epoch, 'std' : accuracy_std_epoch}\n",
        "\n",
        "    return (loss, accuracy)    \n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-naZoRkh58eO"
      },
      "source": [
        "## test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8OT6sad58eO"
      },
      "source": [
        "def test(dataloader):\n",
        "\n",
        "    loss_epoch      = []\n",
        "    accuracy_epoch  = []\n",
        "\n",
        "    generator.eval()\n",
        "    discriminator.eval()\n",
        "\n",
        "    for index_batch, (original, blur) in enumerate(dataloader):\n",
        "\n",
        "        original    = original.to(device)\n",
        "        blur        = blur.to(device)\n",
        "        \n",
        "        deblurred = discriminator(blur)\n",
        "        loss = compute_mse_loss(deblurred, original)\n",
        "        accuracy = compute_accuracy(deblurred, original)\n",
        "\n",
        "        loss_epoch.append(loss.item())\n",
        "        accuracy_epoch.append(accuracy)\n",
        "\n",
        "    loss_mean_epoch     = np.mean(loss_epoch)\n",
        "    loss_std_epoch      = np.std(loss_epoch)\n",
        "\n",
        "    accuracy_mean_epoch = np.mean(accuracy_epoch)\n",
        "    accuracy_std_epoch  = np.std(accuracy_epoch)\n",
        "\n",
        "    loss        = {'mean' : loss_mean_epoch, 'std' : loss_std_epoch}\n",
        "    accuracy    = {'mean' : accuracy_mean_epoch, 'std' : accuracy_std_epoch}\n",
        "\n",
        "    return (loss, accuracy)    "
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcwWEt6O58eO"
      },
      "source": [
        "## train and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZBzxh0T58eO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524,
          "referenced_widgets": [
            "8459efec99e9426fbd500b961418b9ef",
            "5c87c1c2ab354810aadf2a4233669930",
            "6f94af875b184a11814f1f1531218c18",
            "17303bf8aa144a8893c69ffc968fb84a",
            "b6e2739a9208424591058f1e298595d0",
            "547990e994c14d6288f8afa6bb3221cd",
            "ba06544c87f642e3b4df61207b66baf4",
            "1795bb4ae03d40b6b4aae81227b45cca",
            "bd581e685ccb475f8a4734fbb385fcb1",
            "b4ddf1cbe70d42b59be2f079c8b48ba8",
            "670eb6ba294c4a348995f192698cd701"
          ]
        },
        "outputId": "87483789-93d6-4045-eb4c-85c86e071c7e"
      },
      "source": [
        "# ================================================================================\n",
        "# \n",
        "# iterations for epochs\n",
        "#\n",
        "# ================================================================================\n",
        "for i in tqdm_notebook(range(number_epoch)):\n",
        "    \n",
        "    # ================================================================================\n",
        "    # \n",
        "    # training\n",
        "    #\n",
        "    # ================================================================================\n",
        "    (loss_train, accuracy_train) = train(dataloader_train_transform)\n",
        "\n",
        "    loss_mean_train[i]      = loss_train['mean']\n",
        "    loss_std_train[i]       = loss_train['std']\n",
        "\n",
        "    accuracy_mean_train[i]  = accuracy_train['mean']\n",
        "    accuracy_std_train[i]   = accuracy_train['std']\n",
        "\n",
        "    # ================================================================================\n",
        "    # \n",
        "    # testing\n",
        "    #\n",
        "    # ================================================================================\n",
        "    (loss_test, accuracy_test) = test(dataloader_test)\n",
        "\n",
        "    loss_mean_test[i]      = loss_test['mean']\n",
        "    loss_std_test[i]       = loss_test['std']\n",
        "\n",
        "    accuracy_mean_test[i]  = accuracy_test['mean']\n",
        "    accuracy_std_test[i]   = accuracy_test['std']\n",
        "\n",
        "    print(f\"epoch : {i}\")\n",
        "    print(f\"\\ttrain loss : {loss_train['mean']}, train acc : {accuracy_train['mean']}\")\n",
        "    print(f\"\\ttest loss  : {loss_test['mean']},  test acc  : {accuracy_test['mean']}\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8459efec99e9426fbd500b961418b9ef",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/150 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-974d4f568738>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# ================================================================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0mloss_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_train\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader_train_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mloss_mean_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m      \u001b[0;34m=\u001b[0m \u001b[0mloss_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mean'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-20bb8b3ca124>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataloader)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mloss_dis\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mloss_real\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_fake\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mloss_dis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0moptimizer_discriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea5pwNOU58eP"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuWudbs158eP"
      },
      "source": [
        "# functions for visualizing the results "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4d6Aq5zw58eP"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_R69pYH58eP"
      },
      "source": [
        "## plot curve"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDekOCIK58eP"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzBOKbpV58eQ"
      },
      "source": [
        "def plot_data_grid(data, index_data, nRow, nCol):\n",
        "    \n",
        "    size_col = 1.5\n",
        "    size_row = 1.5\n",
        "\n",
        "    fig, axes = plt.subplots(nRow, nCol, constrained_layout=True, figsize=(nCol * size_col, nRow * size_row))\n",
        "\n",
        "    for i in range(nRow):\n",
        "        for j in range(nCol):\n",
        "\n",
        "            k       = i * nCol + j\n",
        "            index   = index_data[k]\n",
        "\n",
        "            axes[i, j].imshow(data[index], cmap='gray', vmin=0, vmax=1)\n",
        "            axes[i, j].xaxis.set_visible(False)\n",
        "            axes[i, j].yaxis.set_visible(False)\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bvZjZ0G58eQ"
      },
      "source": [
        "def plot_data_tensor_grid(data, index_data, nRow, nCol):\n",
        "    \n",
        "    size_col = 1.5\n",
        "    size_row = 1.5\n",
        "\n",
        "    fig, axes = plt.subplots(nRow, nCol, constrained_layout=True, figsize=(nCol * size_col, nRow * size_row))\n",
        "\n",
        "    data = data.detach().cpu().squeeze(axis=1)\n",
        "\n",
        "    for i in range(nRow):\n",
        "        for j in range(nCol):\n",
        "\n",
        "            k       = i * nCol + j\n",
        "            index   = index_data[k]\n",
        "\n",
        "            axes[i, j].imshow(data[index], cmap='gray', vmin=0, vmax=1)\n",
        "            axes[i, j].xaxis.set_visible(False)\n",
        "            axes[i, j].yaxis.set_visible(False)\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sgAQsD_58eQ"
      },
      "source": [
        "def plot_curve_error(data_mean, data_std, x_label, y_label, title):\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.title(title)\n",
        "\n",
        "    alpha = 0.3\n",
        "    \n",
        "    plt.plot(range(len(data_mean)), data_mean, '-', color = 'red')\n",
        "    plt.fill_between(range(len(data_mean)), data_mean - data_std, data_mean + data_std, facecolor = 'blue', alpha = alpha) \n",
        "    \n",
        "    plt.xlabel(x_label)\n",
        "    plt.ylabel(y_label)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NMaZ_uH58eQ"
      },
      "source": [
        "def print_curve(data, index):\n",
        "    \n",
        "    for i in range(len(index)):\n",
        "\n",
        "        idx = index[i]\n",
        "        val = data[idx]\n",
        "\n",
        "        print('index = %2d, value = %12.10f' % (idx, val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GkIwDwC58eQ"
      },
      "source": [
        "def get_data_last(data, index_start):\n",
        "\n",
        "    data_last = data[index_start:]\n",
        "\n",
        "    return data_last"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gpm6E3d58eT"
      },
      "source": [
        "def get_max_last_range(data, index_start):\n",
        "\n",
        "    data_range = get_data_last(data, index_start)\n",
        "    value = data_range.max()\n",
        "\n",
        "    return value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zu0MZ7uH58eT"
      },
      "source": [
        "def get_min_last_range(data, index_start):\n",
        "\n",
        "    data_range = get_data_last(data, index_start)\n",
        "    value = data_range.min()\n",
        "\n",
        "    return value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QLnyYN758eU"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oJ-UrKg58eU"
      },
      "source": [
        "# functions for presenting the results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WfKjuvM58eU"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZVmZxu558eU"
      },
      "source": [
        "def function_result_01():\n",
        "\n",
        "    print('[plot examples of the training original images]')\n",
        "    print('') \n",
        "\n",
        "    nRow = 8\n",
        "    nCol = 6\n",
        "    index_data      = np.arange(0, nRow * nCol)\n",
        "    data_train, _   = dataset_train[index_data]\n",
        "    data_train      = data_train[0]\n",
        "    \n",
        "    plot_data_grid(data_train, index_data, nRow, nCol)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNoGMHWC58eV"
      },
      "source": [
        "def function_result_02():\n",
        "\n",
        "    print('[plot examples of the training blur images]')\n",
        "    print('') \n",
        "    \n",
        "    nRow = 8\n",
        "    nCol = 6\n",
        "    index_data      = np.arange(0, nRow * nCol)\n",
        "    _, data_train   = dataset_train[index_data]\n",
        "    data_train      = data_train[0]\n",
        "    \n",
        "    plot_data_grid(data_train, index_data, nRow, nCol)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BamKLf458eV"
      },
      "source": [
        "def function_result_03():\n",
        "\n",
        "    print('[plot examples of the training de-blurring results]')\n",
        "    print('') \n",
        "\n",
        "    nRow = 8\n",
        "    nCol = 6\n",
        "    index_data          = np.arange(0, nRow * nCol)\n",
        "    _, data_train       = dataset_train[index_data] \n",
        "    data_train          = data_train[0].unsqueeze(dim=1).to(device)\n",
        "    prediction_train    = compute_prediction(model, data_train)\n",
        "    \n",
        "    plot_data_tensor_grid(prediction_train, index_data, nRow, nCol)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OO8euXmt58eV"
      },
      "source": [
        "def function_result_04():\n",
        "\n",
        "    print('[plot examples of the testing original images]')\n",
        "    print('') \n",
        "    \n",
        "    nRow = 8 \n",
        "    nCol = 6\n",
        "    index_data      = np.arange(0, nRow * nCol)\n",
        "    data_test, _    = dataset_test[index_data]\n",
        "    data_test       = data_test[0]\n",
        "    \n",
        "    plot_data_grid(data_test, index_data, nRow, nCol)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSKiJORv58eV"
      },
      "source": [
        "def function_result_05():\n",
        "\n",
        "    print('[plot examples of the testing blur images]')\n",
        "    print('') \n",
        "\n",
        "    nRow = 8\n",
        "    nCol = 6\n",
        "    index_data      = np.arange(0, nRow * nCol)\n",
        "    _, data_test    = dataset_test[index_data]\n",
        "    data_test       = data_test[0]\n",
        "    \n",
        "    plot_data_grid(data_test, index_data, nRow, nCol)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sB-ptES58eW"
      },
      "source": [
        "def function_result_06():\n",
        "\n",
        "    print('[plot examples of the testing de-blurring results]')\n",
        "    print('') \n",
        "\n",
        "    nRow = 8\n",
        "    nCol = 6\n",
        "    index_data      = np.arange(0, nRow * nCol)\n",
        "    _, data_test    = dataset_test[index_data]\n",
        "    data_test       = data_test[0].unsqueeze(dim=1).to(device)\n",
        "    prediction_test = compute_prediction(model, data_test)\n",
        "    \n",
        "    plot_data_tensor_grid(prediction_test, index_data, nRow, nCol)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7A079kx58eW"
      },
      "source": [
        "def function_result_07():\n",
        "\n",
        "    print('[plot the training loss]')\n",
        "    print('') \n",
        "\n",
        "    plot_curve_error(loss_mean_train, loss_std_train, 'epoch', 'loss', 'loss (training)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7a9O577Q58eW"
      },
      "source": [
        "def function_result_08():\n",
        "\n",
        "    print('[plot the training accuracy]')\n",
        "    print('') \n",
        "    \n",
        "    plot_curve_error(accuracy_mean_train, accuracy_std_train, 'epoch', 'accuracy', 'accuracy (training)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5roDSkaV58eX"
      },
      "source": [
        "def function_result_09():\n",
        "    \n",
        "    print('[plot the testing loss]')\n",
        "    print('') \n",
        "    \n",
        "    plot_curve_error(loss_mean_test, loss_std_test, 'epoch', 'loss', 'loss (testing)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfYekVlp58eX"
      },
      "source": [
        "def function_result_10():\n",
        "    \n",
        "    print('[plot the testing accuracy]') \n",
        "    print('') \n",
        "    \n",
        "    plot_curve_error(accuracy_mean_test, accuracy_std_test, 'epoch', 'accuracy', 'accuracy (testing)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxZxr1wj58eX"
      },
      "source": [
        "def function_result_11():\n",
        "    \n",
        "    print('[print the training loss at the last 10 epochs]')\n",
        "    print('') \n",
        "\n",
        "    data_last   = get_data_last(loss_mean_train, -10)\n",
        "    index       = np.arange(0, 10)\n",
        "    print_curve(data_last, index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwkyywIr58eX"
      },
      "source": [
        "def function_result_12():\n",
        "    \n",
        "    print('[print the training accuracy at the last 10 epochs]')\n",
        "    print('') \n",
        "    \n",
        "    data_last   = get_data_last(accuracy_mean_train, -10)\n",
        "    index       = np.arange(0, 10)\n",
        "    print_curve(data_last, index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOQ5plEg58eY"
      },
      "source": [
        "def function_result_13():\n",
        "    \n",
        "    print('[print the testing loss at the last 10 epochs]')\n",
        "    print('') \n",
        "    \n",
        "    data_last   = get_data_last(loss_mean_test, -10)\n",
        "    index       = np.arange(0, 10)\n",
        "    print_curve(data_last, index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwuWrssa58eY"
      },
      "source": [
        "def function_result_14():\n",
        "    \n",
        "    print('[print the testing accuracy at the last 10 epochs]')\n",
        "    print('') \n",
        "    \n",
        "    data_last   = get_data_last(accuracy_mean_test, -10)\n",
        "    index       = np.arange(0, 10)\n",
        "    print_curve(data_last, index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMKzKoyF58eY"
      },
      "source": [
        "def function_result_15():\n",
        "    \n",
        "    print('[print the best training accuracy within the last 10 epochs]')\n",
        "    print('') \n",
        "\n",
        "    value = get_max_last_range(accuracy_mean_train, -10)\n",
        "    print('best training accuracy = %12.10f' % (value))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXinqYhe58eY"
      },
      "source": [
        "def function_result_16():\n",
        "    \n",
        "    print('[print the best testing accuracy within the last 10 epochs]')\n",
        "    print('') \n",
        "    \n",
        "    value = get_max_last_range(accuracy_mean_test, -10)\n",
        "    print('best testing accuracy = %12.10f' % (value))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTm7CUry58eY"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzfDmQ3558eY"
      },
      "source": [
        "# RESULTS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckjvMUc358eZ"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqTW6sNu58eZ"
      },
      "source": [
        "number_result = 16\n",
        "\n",
        "for i in range(number_result):\n",
        "\n",
        "    title           = '# RESULT # {:02d}'.format(i+1) \n",
        "    name_function   = 'function_result_{:02d}()'.format(i+1)\n",
        "\n",
        "    print('') \n",
        "    print('################################################################################')\n",
        "    print('#') \n",
        "    print(title)\n",
        "    print('#') \n",
        "    print('################################################################################')\n",
        "    print('') \n",
        "\n",
        "    eval(name_function)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvi866JF58eZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}