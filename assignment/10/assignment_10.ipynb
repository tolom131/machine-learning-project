{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment_02.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "interpreter": {
      "hash": "17ed1555cfbb96ddcf655400d6c25a9cebe961c1b69daf25bae91d698acdd2a7"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 64-bit ('hsh': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f96684be320641d5855a1642b70c478c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_85e89f14c8364f928941f34a4093ff82",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d99a6880f1f34d3ebce996563333afe0",
              "IPY_MODEL_19a957aa92864e32944386bdd91ee130",
              "IPY_MODEL_09ce947a15124af3ade2d847631a5d5a"
            ]
          }
        },
        "85e89f14c8364f928941f34a4093ff82": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d99a6880f1f34d3ebce996563333afe0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_aeb7e1043d4644679656580d9a280b05",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 10%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f95deede59254969927af4c5ff448fd9"
          }
        },
        "19a957aa92864e32944386bdd91ee130": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_0dad535820eb4ba09cbfa70ef8dd3102",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 200,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 19,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ba5451d663a349448446692575692b29"
          }
        },
        "09ce947a15124af3ade2d847631a5d5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_458580b8be2c4065a7fe7c53c3e536dc",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 19/200 [01:57&lt;17:55,  5.94s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5df72fb93a674036be37380aec6086f9"
          }
        },
        "aeb7e1043d4644679656580d9a280b05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f95deede59254969927af4c5ff448fd9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0dad535820eb4ba09cbfa70ef8dd3102": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ba5451d663a349448446692575692b29": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "458580b8be2c4065a7fe7c53c3e536dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5df72fb93a674036be37380aec6086f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K29I-OwCEYzW",
        "outputId": "db8b297c-a4b8-457a-b43f-7570135306c7"
      },
      "source": [
        "# Image Segmentation by unsupervised Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYX0qLKtJJ19"
      },
      "source": [
        "## import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9ZR2R55JJ1-"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "import random\n",
        "import os"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMFQfo7MJJ1_"
      },
      "source": [
        "## load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzjEayuoJJ2A",
        "outputId": "48310896-e87f-4662-c746-48300284a887"
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "directory_data  = './drive/MyDrive/Machine_Learning/'\n",
        "filename_data   = 'assignment_10_data.npz'\n",
        "\n",
        "data            = np.load(os.path.join(directory_data, filename_data))\n",
        "image_clean     = torch.from_numpy(data['real_images']).float()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KId6NfjyJJ2B"
      },
      "source": [
        "## custom data loader for the PyTorch framework"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-DczpSRJJ2B"
      },
      "source": [
        "class dataset (Dataset):\n",
        "    def  __init__(self, data, std_noise):\n",
        "\n",
        "        noise = torch.randn(data.size()) * std_noise\n",
        "\n",
        "        self.clean  = data\n",
        "        self.noisy  = data + noise\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        clean   = self.clean[index]\n",
        "        noisy   = self.noisy[index]\n",
        "        \n",
        "        clean = torch.FloatTensor(clean).unsqueeze(dim=0)\n",
        "        noisy = torch.FloatTensor(noisy).unsqueeze(dim=0)\n",
        "\n",
        "        return (clean, noisy)\n",
        "  \n",
        "    def __len__(self):\n",
        "        \n",
        "        return self.clean.shape[0]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pLwi-qZJJ2C"
      },
      "source": [
        "image_train = image_clean[::2]\n",
        "image_test  = image_clean[1::2]\n",
        "\n",
        "dataset_train   = dataset(image_train, 0.5)\n",
        "dataset_test    = dataset(image_test, 0.5)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4COmXCafJJ2C"
      },
      "source": [
        "## hyper-parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3k4HFCaaJJ2D"
      },
      "source": [
        "device          = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "number_epoch    = 200\n",
        "size_minibatch  = 50\n",
        "learning_rate   = 0.1\n",
        "weight_decay    = 0.0001\n",
        "weight_regular  = 0.0001"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e47OCNc3JJ2E"
      },
      "source": [
        "## construct datasets and dataloaders for training and testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Lz2f9TRJJ2E"
      },
      "source": [
        "dataloader_train            = DataLoader(dataset_train, batch_size=size_minibatch, shuffle=True, drop_last=True)\n",
        "dataloader_test             = DataLoader(dataset_test, batch_size=size_minibatch, shuffle=False, drop_last=True)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WjsCrg3JJ2F"
      },
      "source": [
        "## shape of the data when using the data loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnUSBKz7JJ2F",
        "outputId": "534ab9ce-82bf-4645-efdd-6842c65db56c"
      },
      "source": [
        "(image_train, label_train)  = dataset_train[0]\n",
        "(image_test, label_test)    = dataset_test[0]\n",
        "print('*******************************************************************')\n",
        "print('shape of the image in the training dataset:', image_train.shape)\n",
        "print('shape of the label in the training dataset:', label_train.shape)\n",
        "print('*******************************************************************')\n",
        "print('shape of the image in the testing dataset:', image_test.shape)\n",
        "print('shape of the label in the testing dataset:', label_test.shape)\n",
        "print('*******************************************************************')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*******************************************************************\n",
            "shape of the image in the training dataset: torch.Size([1, 32, 32])\n",
            "shape of the label in the training dataset: torch.Size([1, 32, 32])\n",
            "*******************************************************************\n",
            "shape of the image in the testing dataset: torch.Size([1, 32, 32])\n",
            "shape of the label in the testing dataset: torch.Size([1, 32, 32])\n",
            "*******************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LIxNzfqJJ2H"
      },
      "source": [
        "## class for the neural network "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVpI-EQWJJ2J"
      },
      "source": [
        "## build network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLcEtnudySLl"
      },
      "source": [
        "import torch.utils.model_zoo as model_zoo\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
        "                               padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers):\n",
        "        self.inplanes = 64\n",
        "        super(ResNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        c2 = self.layer1(x)\n",
        "        c3 = self.layer2(c2)\n",
        "        c4 = self.layer3(c3)\n",
        "        c5 = self.layer4(c4)\n",
        "\n",
        "        return c2, c3, c4, c5\n",
        "\n",
        "\n",
        "def resnet50(pretrained=False, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-50 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet(Bottleneck, [3, 4, 6, 3])\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['resnet50']), strict=False)\n",
        "    return model"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrnFToH5yduF"
      },
      "source": [
        "class fpn_module(nn.Module):\n",
        "    def __init__(self, numClass):\n",
        "        super(fpn_module, self).__init__()\n",
        "        # Top layer\n",
        "        self.toplayer = nn.Conv2d(2048, 256, kernel_size=1, stride=1, padding=0)  # Reduce channels\n",
        "\n",
        "        # Smooth layers\n",
        "        self.smooth1_1 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
        "        self.smooth2_1 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
        "        self.smooth3_1 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
        "        self.smooth4_1 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
        "        self.smooth1_2 = nn.Conv2d(256, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.smooth2_2 = nn.Conv2d(256, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.smooth3_2 = nn.Conv2d(256, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.smooth4_2 = nn.Conv2d(256, 128, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        # Lateral layers\n",
        "        self.latlayer1 = nn.Conv2d(1024, 256, kernel_size=1, stride=1, padding=0)\n",
        "        self.latlayer2 = nn.Conv2d(512, 256, kernel_size=1, stride=1, padding=0)\n",
        "        self.latlayer3 = nn.Conv2d(256, 256, kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "        # Classify layers\n",
        "        self.classify = nn.Conv2d(128*4, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.uppool1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2, padding=0)\n",
        "        self.uppool2 = nn.ConvTranspose2d(64, 1, kernel_size=2, stride=2, padding=0)\n",
        "\n",
        "    def _concatenate(self, p5, p4, p3, p2):\n",
        "        _, _, H, W = p2.size()\n",
        "        p5 = F.upsample(p5, size=(H, W), mode='bilinear')\n",
        "        p4 = F.upsample(p4, size=(H, W), mode='bilinear')\n",
        "        p3 = F.upsample(p3, size=(H, W), mode='bilinear')\n",
        "        return torch.cat([p5, p4, p3, p2], dim=1)\n",
        "\n",
        "    def _upsample_add(self, x, y):\n",
        "        _, _, H, W = y.size()\n",
        "        return F.upsample(x, size=(H, W), mode='bilinear') + y\n",
        "\n",
        "    def forward(self, c2, c3, c4, c5):\n",
        "        # Top-down\n",
        "        p5 = self.toplayer(c5)\n",
        "        p4 = self._upsample_add(p5, self.latlayer1(c4))\n",
        "        p3 = self._upsample_add(p4, self.latlayer2(c3))\n",
        "        p2 = self._upsample_add(p3, self.latlayer3(c2))\n",
        "        # Smooth\n",
        "        p5 = self.smooth1_2(self.smooth1_1(p5))\n",
        "        p4 = self.smooth2_2(self.smooth2_1(p4))\n",
        "        p3 = self.smooth3_2(self.smooth3_1(p3))\n",
        "        p2 = self.smooth4_2(self.smooth4_1(p2))\n",
        "        # Classify\n",
        "        x = self.classify(self._concatenate(p5, p4, p3, p2))\n",
        "        x = self.uppool1(x)\n",
        "        x = self.uppool2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class fpn(nn.Module):\n",
        "    def __init__(self, numClass):\n",
        "        super(fpn, self).__init__()\n",
        "        # Res net\n",
        "        self.resnet = resnet50(False)\n",
        "\n",
        "        # fpn module\n",
        "        self.fpn = fpn_module(numClass)\n",
        "\n",
        "        # # init fpn\n",
        "        # for m in self.fpn.children():\n",
        "        #     nn.init.normal_(m.weight, mean=0, std=0.01)\n",
        "        #     nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Top-down\n",
        "        c2, c3, c4, c5 = self.resnet.forward(x)\n",
        "        return self.fpn.forward(c2, c3, c4, c5)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAdh2DKayhg2"
      },
      "source": [
        "# model = fpn(numClass=1).to(device)\n",
        "# x = torch.rand(2, 1, 32, 32)\n",
        "# from torchsummary import summary\n",
        "# summary(model, (1, 32, 32))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTqHIFHDJJ2K"
      },
      "source": [
        "model       = fpn(numClass=1).to(device)\n",
        "optimizer   = torch.optim.Adam(model.parameters(), lr=0.01)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qwh6rOvcJJ2K"
      },
      "source": [
        "## compute the prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FztcGRP1JJ2L"
      },
      "source": [
        "def compute_prediction(model, input):\n",
        "    # ==================================================\n",
        "    # fill up the blank\n",
        "    #\n",
        "\n",
        "    prediction = model(input)\n",
        "    \n",
        "    # \n",
        "    # ==================================================\n",
        "\n",
        "    return prediction"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jP1PYiI9JJ2L"
      },
      "source": [
        "def compute_estimate(input, prediction):\n",
        "\n",
        "    number_phase = 2 # bi-partitioning\n",
        "    (batch, channel, height, width) = input.size()\n",
        "\n",
        "    estimate = torch.zeros(number_phase, batch, channel).to(device)\n",
        "    \n",
        "    prediction_inside   = prediction\n",
        "    prediction_outside  = 1 - prediction\n",
        "\n",
        "    # ==================================================\n",
        "    # fill up the blank for the estimate of the inside of segmenting region\n",
        "    #\n",
        "    inside = input * prediction_inside\n",
        "    #print(inside.shape)\n",
        "    inside = torch.sum(torch.sum(inside, dim=-1), dim=-1)\n",
        "    inside = inside / torch.sum(torch.sum(prediction_inside, dim=-1), dim=-1)\n",
        "    estimate[0] = inside\n",
        "    # \n",
        "    # ==================================================\n",
        "    \n",
        "    # ==================================================\n",
        "    # fill up the blank for the estimate of the outside of segmenting region\n",
        "    #\n",
        "    outside = input * prediction_outside\n",
        "    outside = torch.sum(torch.sum(outside, dim=-1), dim=-1)\n",
        "    outside = outside / torch.sum(torch.sum(prediction_outside, dim=-1), dim=-1)\n",
        "    estimate[1] = outside  \n",
        "    # \n",
        "    # ==================================================\n",
        "    \n",
        "    return estimate"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7ACkCgXJJ2M"
      },
      "source": [
        "def compute_loss_data(input, prediction):\n",
        "\n",
        "    (batch, channel, height, width) = input.size()\n",
        "    estimate = compute_estimate(input, prediction)\n",
        "\n",
        "    prediction_inside   = prediction\n",
        "    prediction_outside  = 1 - prediction\n",
        "\n",
        "    estimate0           = torch.unsqueeze(torch.unsqueeze(estimate[0], dim=-1), dim=-1) \n",
        "    estimate1           = torch.unsqueeze(torch.unsqueeze(estimate[1], dim=-1), dim=-1) \n",
        "\n",
        "    residual_inside     = torch.square(input - estimate0)\n",
        "    residual_outside    = torch.square(input - estimate1)\n",
        "\n",
        "    # print(\"shape of residual_inside : \", residual_inside.shape)\n",
        "    # print(\"shape of residual_outside : \", residual_outside.shape)\n",
        "    \n",
        "    # ==================================================\n",
        "    # fill up the blank for the data fidelity of the inside of segmenting region\n",
        "    #\n",
        "    fidelity_inside = residual_inside * prediction_inside\n",
        "    # \n",
        "    # ==================================================\n",
        "    \n",
        "    # ==================================================\n",
        "    # fill up the blank for the data fidelity of the inside of segmenting region\n",
        "    #\n",
        "    fidelity_outside = residual_outside * prediction_outside\n",
        "    # \n",
        "    # ==================================================\n",
        "\n",
        "    fidelity = fidelity_inside + fidelity_outside\n",
        "    fidelity = torch.sum(fidelity)\n",
        "\n",
        "    loss_data       = fidelity / (batch * channel * height * width)\n",
        "    loss_data_value = loss_data.item()\n",
        "\n",
        "    return loss_data, loss_data_value"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-f5e-jGIJJ2M"
      },
      "source": [
        "def compute_regularization(prediction):\n",
        "\n",
        "    (batch, channel, height, width) = prediction.size()\n",
        "   \n",
        "    gradient_height = torch.abs(prediction[:, :, 1:, :] - prediction[:, :, :- 1, :]).sum()\n",
        "    gradient_width  = torch.abs(prediction[:, :, :, 1:] - prediction[:, :, :, :-1]).sum()\n",
        "\n",
        "    loss_regularization         = (gradient_height + gradient_width) / (batch * channel * height * width)\n",
        "    loss_regularization_value   = loss_regularization.item()\n",
        "    \n",
        "    return loss_regularization, loss_regularization_value"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7sEFAzmJJ2N"
      },
      "source": [
        "## compute the loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcPxOg4nJJ2N"
      },
      "source": [
        "def compute_loss(input, prediction, alpha):\n",
        "\n",
        "    (loss_data, _)              = compute_loss_data(input, prediction)\n",
        "    (loss_regularization, _)    = compute_regularization(prediction)\n",
        "\n",
        "    # ==================================================\n",
        "    # fill up the blank for the loss that consists of the data fidelity and the regularization with a weight\n",
        "    #\n",
        "    loss        = loss_data + alpha * loss_regularization\n",
        "    # \n",
        "    # ==================================================\n",
        "\n",
        "    loss_value  = loss.item()\n",
        "\n",
        "    return loss, loss_value"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHCnQ7SBJJ2O"
      },
      "source": [
        "## compute the accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvWBEMpvJJ2O"
      },
      "source": [
        "def compute_accuracy(prediction, label):\n",
        "    \n",
        "    prediction  = prediction.squeeze(axis=1)\n",
        "    label       = label.squeeze(axis=1)\n",
        "\n",
        "    prediction_binary   = (prediction >= 0.5).cpu().numpy()\n",
        "    label               = label.bool().cpu().numpy()\n",
        "    \n",
        "    region_intersection = prediction_binary & label\n",
        "    region_union        = prediction_binary | label\n",
        "\n",
        "    area_intersection   = region_intersection.sum(axis=1).sum(axis=1).astype(float)\n",
        "    area_union          = region_union.sum(axis=1).sum(axis=1).astype(float)\n",
        "\n",
        "    eps         = np.finfo(float).eps\n",
        "    correct     = area_intersection / (area_union + eps)\n",
        "    accuracy    = correct.mean() * 100.0\n",
        "    \n",
        "    return accuracy"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYcZfv1zJJ2O"
      },
      "source": [
        "## variables for the learning curve"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATqrjzX4JJ2P"
      },
      "source": [
        "loss_mean_train     = np.zeros(number_epoch)\n",
        "loss_std_train      = np.zeros(number_epoch)\n",
        "accuracy_mean_train = np.zeros(number_epoch)\n",
        "accuracy_std_train  = np.zeros(number_epoch)\n",
        "\n",
        "loss_mean_test      = np.zeros(number_epoch)\n",
        "loss_std_test       = np.zeros(number_epoch)\n",
        "accuracy_mean_test  = np.zeros(number_epoch)\n",
        "accuracy_std_test   = np.zeros(number_epoch)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXYA-PGpJJ2P"
      },
      "source": [
        "## train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nczD8ADXJJ2P"
      },
      "source": [
        "def train(model, dataloader):\n",
        "\n",
        "    loss_epoch      = []\n",
        "    accuracy_epoch  = []\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for index_batch, (clean, noisy) in enumerate(dataloader):\n",
        "\n",
        "        clean = clean.to(device)\n",
        "        noisy = noisy.to(device)\n",
        "        \n",
        "        prediction          = compute_prediction(model, noisy)\n",
        "        loss, loss_value    = compute_loss(noisy, prediction, weight_regular)\n",
        "        accuracy1           = compute_accuracy(prediction, clean)\n",
        "        accuracy2           = compute_accuracy(1 - prediction, clean)\n",
        "        accuracy            = np.maximum(accuracy1, accuracy2)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_epoch.append(loss_value)\n",
        "        accuracy_epoch.append(accuracy)\n",
        "\n",
        "    loss_mean_epoch     = np.mean(loss_epoch)\n",
        "    loss_std_epoch      = np.std(loss_epoch)\n",
        "\n",
        "    accuracy_mean_epoch = np.mean(accuracy_epoch)\n",
        "    accuracy_std_epoch  = np.std(accuracy_epoch)\n",
        "\n",
        "    loss        = {'mean' : loss_mean_epoch, 'std' : loss_std_epoch}\n",
        "    accuracy    = {'mean' : accuracy_mean_epoch, 'std' : accuracy_std_epoch}\n",
        "\n",
        "    return (loss, accuracy)    \n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7IynYknJJ2Q"
      },
      "source": [
        "## test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvZrdQYOJJ2Q"
      },
      "source": [
        "def test(model, dataloader):\n",
        "\n",
        "    loss_epoch      = []\n",
        "    accuracy_epoch  = []\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    for index_batch, (clean, noisy) in enumerate(dataloader):\n",
        "\n",
        "        clean = clean.to(device)\n",
        "        noisy = noisy.to(device)\n",
        "        \n",
        "        prediction          = compute_prediction(model, noisy)\n",
        "        loss, loss_value    = compute_loss(noisy, prediction, weight_regular)\n",
        "        accuracy1           = compute_accuracy(prediction, clean)\n",
        "        accuracy2           = compute_accuracy(1 - prediction, clean)\n",
        "        accuracy            = np.maximum(accuracy1, accuracy2)\n",
        "\n",
        "        loss_epoch.append(loss_value)\n",
        "        accuracy_epoch.append(accuracy)\n",
        "\n",
        "    loss_mean_epoch     = np.mean(loss_epoch)\n",
        "    loss_std_epoch      = np.std(loss_epoch)\n",
        "\n",
        "    accuracy_mean_epoch = np.mean(accuracy_epoch)\n",
        "    accuracy_std_epoch  = np.std(accuracy_epoch)\n",
        "\n",
        "    loss        = {'mean' : loss_mean_epoch, 'std' : loss_std_epoch}\n",
        "    accuracy    = {'mean' : accuracy_mean_epoch, 'std' : accuracy_std_epoch}\n",
        "\n",
        "    return (loss, accuracy)    "
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLM2ynO8JJ2R"
      },
      "source": [
        "## train and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "f96684be320641d5855a1642b70c478c",
            "85e89f14c8364f928941f34a4093ff82",
            "d99a6880f1f34d3ebce996563333afe0",
            "19a957aa92864e32944386bdd91ee130",
            "09ce947a15124af3ade2d847631a5d5a",
            "aeb7e1043d4644679656580d9a280b05",
            "f95deede59254969927af4c5ff448fd9",
            "0dad535820eb4ba09cbfa70ef8dd3102",
            "ba5451d663a349448446692575692b29",
            "458580b8be2c4065a7fe7c53c3e536dc",
            "5df72fb93a674036be37380aec6086f9"
          ]
        },
        "id": "3s4xxgQnJJ2R",
        "outputId": "5518a851-c609-4d5a-b4c1-b2bc969f1868"
      },
      "source": [
        "# ================================================================================\n",
        "# \n",
        "# iterations for epochs\n",
        "#\n",
        "# ================================================================================\n",
        "for i in tqdm_notebook(range(number_epoch)):\n",
        "    \n",
        "    # ================================================================================\n",
        "    # \n",
        "    # training\n",
        "    #\n",
        "    # ================================================================================\n",
        "    (loss_train, accuracy_train) = train(model, dataloader_train)\n",
        "\n",
        "    loss_mean_train[i]      = loss_train['mean']\n",
        "    loss_std_train[i]       = loss_train['std']\n",
        "\n",
        "    accuracy_mean_train[i]  = accuracy_train['mean']\n",
        "    accuracy_std_train[i]   = accuracy_train['std']\n",
        "\n",
        "    # ================================================================================\n",
        "    # \n",
        "    # testing\n",
        "    #\n",
        "    # ================================================================================\n",
        "    (loss_test, accuracy_test) = test(model, dataloader_test)\n",
        "\n",
        "    loss_mean_test[i]      = loss_test['mean']\n",
        "    loss_std_test[i]       = loss_test['std']\n",
        "\n",
        "    accuracy_mean_test[i]  = accuracy_test['mean']\n",
        "    accuracy_std_test[i]   = accuracy_test['std']\n",
        "\n",
        "    print(f\"epoch : {i}\")\n",
        "    print(f\"\\ttrain loss : {loss_train['mean']}, train acc : {accuracy_train['mean']}\")\n",
        "    print(f\"\\ttest loss  : {loss_test['mean']},  test acc  : {accuracy_test['mean']}\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f96684be320641d5855a1642b70c478c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/200 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3509: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
            "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3635: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch : 0\n",
            "\ttrain loss : 23.033206152361494, train acc : 13.776912067189416\n",
            "\ttest loss  : 669.5840987826502,  test acc  : 14.522654447434169\n",
            "epoch : 1\n",
            "\ttrain loss : 627.6230955512025, train acc : 15.409564184910497\n",
            "\ttest loss  : 2670230.7414153344,  test acc  : 13.638204582107608\n",
            "epoch : 2\n",
            "\ttrain loss : 3743.8421055993367, train acc : 14.627758563763027\n",
            "\ttest loss  : 3470.185467209927,  test acc  : 13.400573575412345\n",
            "epoch : 3\n",
            "\ttrain loss : 28962.570284821268, train acc : 14.424181109850615\n",
            "\ttest loss  : 608472.3658248547,  test acc  : 13.099451904370241\n",
            "epoch : 4\n",
            "\ttrain loss : 10831.593173714571, train acc : 14.025963181312498\n",
            "\ttest loss  : 214397.7711210029,  test acc  : 20.37351659759363\n",
            "epoch : 5\n",
            "\ttrain loss : 18326.576796420784, train acc : 14.545978166228332\n",
            "\ttest loss  : 21088.61657430959,  test acc  : 15.283743218896305\n",
            "epoch : 6\n",
            "\ttrain loss : 10930.511162336483, train acc : 12.441647664346283\n",
            "\ttest loss  : 10394.306754178779,  test acc  : 13.416843617842622\n",
            "epoch : 7\n",
            "\ttrain loss : 3447.791647267896, train acc : 13.219682901591238\n",
            "\ttest loss  : 818.1004283816316,  test acc  : 13.618329267768091\n",
            "epoch : 8\n",
            "\ttrain loss : 23318.335978663246, train acc : 13.19188781783602\n",
            "\ttest loss  : 3279.916975154433,  test acc  : 15.771882591502916\n",
            "epoch : 9\n",
            "\ttrain loss : 2266.816763944404, train acc : 13.764676434355785\n",
            "\ttest loss  : 1586.0795699718387,  test acc  : 15.421830658916667\n",
            "epoch : 10\n",
            "\ttrain loss : 1345.5694352970568, train acc : 12.86142420171223\n",
            "\ttest loss  : 673.7801456894986,  test acc  : 14.733518280503603\n",
            "epoch : 11\n",
            "\ttrain loss : 1381.7649734851925, train acc : 14.727306555406646\n",
            "\ttest loss  : 566.3998563899551,  test acc  : 13.98507839830639\n",
            "epoch : 12\n",
            "\ttrain loss : 580.2734509845112, train acc : 15.168728116967664\n",
            "\ttest loss  : 342.21271372950355,  test acc  : 14.450519561563533\n",
            "epoch : 13\n",
            "\ttrain loss : 267.7760300747184, train acc : 12.942852037635163\n",
            "\ttest loss  : 223.6043314379315,  test acc  : 13.023404389615497\n",
            "epoch : 14\n",
            "\ttrain loss : 399992.12278854015, train acc : 13.925684957994772\n",
            "\ttest loss  : 2229855.7325581396,  test acc  : 14.393759084302326\n",
            "epoch : 15\n",
            "\ttrain loss : 854459.3993459302, train acc : 14.077515466867855\n",
            "\ttest loss  : 216913.6348110465,  test acc  : 14.269027087680097\n",
            "epoch : 16\n",
            "\ttrain loss : 112849.06458938954, train acc : 14.405653007307455\n",
            "\ttest loss  : 61936.999818313954,  test acc  : 14.413667008960065\n",
            "epoch : 17\n",
            "\ttrain loss : 51140.56640625, train acc : 14.457677255229079\n",
            "\ttest loss  : 31387.128224927324,  test acc  : 16.608581337082907\n",
            "epoch : 18\n",
            "\ttrain loss : 34857.929846475294, train acc : 14.212743381155741\n",
            "\ttest loss  : 22310.382017623546,  test acc  : 15.323646135259397\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-0b01f2f70014>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# ================================================================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0mloss_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mloss_mean_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m      \u001b[0;34m=\u001b[0m \u001b[0mloss_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mean'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-d55f969b96cd>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(model, dataloader)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mindex_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mclean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoisy\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mclean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-6a3cdf6829d2>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mclean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mnoisy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoisy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mclean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoisy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaTHU7pkJJ2S"
      },
      "source": [
        "# functions for visualizing the results "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48fQ2nVqJJ2T"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zz37CiQLJJ2T"
      },
      "source": [
        "## plot curve"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awrJeSG7JJ2T"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uZnOrurJJ2T"
      },
      "source": [
        "def plot_data_grid(data, index_data, nRow, nCol):\n",
        "    \n",
        "    size_col = 1.5\n",
        "    size_row = 1.5\n",
        "\n",
        "    fig, axes = plt.subplots(nRow, nCol, constrained_layout=True, figsize=(nCol * size_col, nRow * size_row))\n",
        "\n",
        "    for i in range(nRow):\n",
        "        for j in range(nCol):\n",
        "\n",
        "            k       = i * nCol + j\n",
        "            index   = index_data[k]\n",
        "\n",
        "            axes[i, j].imshow(data[index], cmap='gray', vmin=0, vmax=1)\n",
        "            axes[i, j].xaxis.set_visible(False)\n",
        "            axes[i, j].yaxis.set_visible(False)\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKCPqJgaJJ2U"
      },
      "source": [
        "def plot_data_tensor_grid(data, index_data, nRow, nCol):\n",
        "    \n",
        "    size_col = 1.5\n",
        "    size_row = 1.5\n",
        "\n",
        "    fig, axes = plt.subplots(nRow, nCol, constrained_layout=True, figsize=(nCol * size_col, nRow * size_row))\n",
        "\n",
        "    data = data.detach().cpu().squeeze(axis=1)\n",
        "\n",
        "    for i in range(nRow):\n",
        "        for j in range(nCol):\n",
        "\n",
        "            k       = i * nCol + j\n",
        "            index   = index_data[k]\n",
        "\n",
        "            axes[i, j].imshow(data[index], cmap='gray', vmin=0, vmax=1)\n",
        "            axes[i, j].xaxis.set_visible(False)\n",
        "            axes[i, j].yaxis.set_visible(False)\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0n6hrbIJJ2U"
      },
      "source": [
        "def plot_curve_error(data_mean, data_std, x_label, y_label, title):\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.title(title)\n",
        "\n",
        "    alpha = 0.3\n",
        "    \n",
        "    plt.plot(range(len(data_mean)), data_mean, '-', color = 'red')\n",
        "    plt.fill_between(range(len(data_mean)), data_mean - data_std, data_mean + data_std, facecolor = 'blue', alpha = alpha) \n",
        "    \n",
        "    plt.xlabel(x_label)\n",
        "    plt.ylabel(y_label)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBXfnozEJJ2U"
      },
      "source": [
        "def print_curve(data, index):\n",
        "    \n",
        "    for i in range(len(index)):\n",
        "\n",
        "        idx = index[i]\n",
        "        val = data[idx]\n",
        "\n",
        "        print('index = %2d, value = %12.10f' % (idx, val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqotIPERJJ2V"
      },
      "source": [
        "def get_data_last(data, index_start):\n",
        "\n",
        "    data_last = data[index_start:]\n",
        "\n",
        "    return data_last"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fFsFFKDJJ2V"
      },
      "source": [
        "def get_max_last_range(data, index_start):\n",
        "\n",
        "    data_range = get_data_last(data, index_start)\n",
        "    value = data_range.max()\n",
        "\n",
        "    return value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2W3GlCt2JJ2V"
      },
      "source": [
        "def get_min_last_range(data, index_start):\n",
        "\n",
        "    data_range = get_data_last(data, index_start)\n",
        "    value = data_range.min()\n",
        "\n",
        "    return value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1TEOnoLJJ2W"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ziCTMzmJJ2W"
      },
      "source": [
        "# functions for presenting the results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FIMx1NTJJ2W"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myyWO1m0JJ2W"
      },
      "source": [
        "def function_result_01():\n",
        "\n",
        "    print('[plot examples of the training noisy images]')\n",
        "    print('') \n",
        "\n",
        "    nRow = 8\n",
        "    nCol = 6\n",
        "\n",
        "    number_data = len(dataset_train)\n",
        "    step        = np.floor(number_data / (nRow * nCol))\n",
        "    index_data  = np.arange(0, number_data, step)\n",
        "    index_plot  = np.arange(0, nRow * nCol)\n",
        "\n",
        "    _, data     = dataset_train[index_data]\n",
        "    data        = data[0]\n",
        "    \n",
        "    plot_data_grid(data, index_plot, nRow, nCol)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1RnAv48JJ2X"
      },
      "source": [
        "def function_result_02():\n",
        "\n",
        "    print('[plot examples of the training ground truth images]')\n",
        "    print('') \n",
        "    \n",
        "    nRow = 8\n",
        "    nCol = 6\n",
        "\n",
        "    number_data = len(dataset_train)\n",
        "    step        = np.floor(number_data / (nRow * nCol))\n",
        "    index_data  = np.arange(0, number_data, step)\n",
        "    index_plot  = np.arange(0, nRow * nCol)\n",
        "\n",
        "    data, _     = dataset_train[index_data]\n",
        "    data        = data[0]\n",
        "    \n",
        "    plot_data_grid(data, index_plot, nRow, nCol)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzKFsbw9JJ2X"
      },
      "source": [
        "def function_result_03():\n",
        "\n",
        "    print('[plot examples of the training segmentation results]')\n",
        "    print('') \n",
        "\n",
        "    nRow = 8\n",
        "    nCol = 6\n",
        "\n",
        "    number_data = len(dataset_train)\n",
        "    step        = np.floor(number_data / (nRow * nCol))\n",
        "    index_data  = np.arange(0, number_data, step)\n",
        "    index_plot  = np.arange(0, nRow * nCol)\n",
        "\n",
        "    _, data     = dataset_train[index_data] \n",
        "    data        = data[0].unsqueeze(dim=1).to(device)\n",
        "    prediction  = compute_prediction(model, data)\n",
        "    \n",
        "    plot_data_tensor_grid(prediction, index_plot, nRow, nCol)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nY1D7x3NJJ2X"
      },
      "source": [
        "def function_result_04():\n",
        "\n",
        "    print('[plot examples of the testing noisy images]')\n",
        "    print('') \n",
        "    \n",
        "    nRow = 8\n",
        "    nCol = 6\n",
        "\n",
        "    number_data = len(dataset_test)\n",
        "    step        = np.floor(number_data / (nRow * nCol))\n",
        "    index_data  = np.arange(0, number_data, step)\n",
        "    index_plot  = np.arange(0, nRow * nCol)\n",
        "\n",
        "    _, data     = dataset_test[index_data]\n",
        "    data        = data[0]\n",
        "    \n",
        "    plot_data_grid(data, index_plot, nRow, nCol)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVeG_ASaJJ2Y"
      },
      "source": [
        "def function_result_05():\n",
        "\n",
        "    print('[plot examples of the testing ground truth images]')\n",
        "    print('') \n",
        "\n",
        "    nRow = 8\n",
        "    nCol = 6\n",
        "\n",
        "    number_data = len(dataset_test)\n",
        "    step        = np.floor(number_data / (nRow * nCol))\n",
        "    index_data  = np.arange(0, number_data, step)\n",
        "    index_plot  = np.arange(0, nRow * nCol)\n",
        "   \n",
        "    data, _     = dataset_test[index_data]\n",
        "    data        = data[0]\n",
        "    \n",
        "    plot_data_grid(data, index_plot, nRow, nCol)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrGyPJaDJJ2Y"
      },
      "source": [
        "def function_result_06():\n",
        "\n",
        "    print('[plot examples of the testing segmentation results]')\n",
        "    print('') \n",
        "\n",
        "    nRow = 8\n",
        "    nCol = 6\n",
        "\n",
        "    number_data = len(dataset_test)\n",
        "    step        = np.floor(number_data / (nRow * nCol))\n",
        "    index_data  = np.arange(0, number_data, step)\n",
        "    index_plot  = np.arange(0, nRow * nCol)\n",
        "\n",
        "    _, data     = dataset_test[index_data]\n",
        "    data        = data[0].unsqueeze(dim=1).to(device)\n",
        "    prediction  = compute_prediction(model, data)\n",
        "    \n",
        "    plot_data_tensor_grid(prediction, index_plot, nRow, nCol)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5ReombcJJ2Y"
      },
      "source": [
        "def function_result_07():\n",
        "\n",
        "    print('[plot the training loss]')\n",
        "    print('') \n",
        "\n",
        "    plot_curve_error(loss_mean_train, loss_std_train, 'epoch', 'loss', 'loss (training)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-z41viADJJ2Z"
      },
      "source": [
        "def function_result_08():\n",
        "\n",
        "    print('[plot the training accuracy]')\n",
        "    print('') \n",
        "    \n",
        "    plot_curve_error(accuracy_mean_train, accuracy_std_train, 'epoch', 'accuracy', 'accuracy (training)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzmf2PeHJJ2Z"
      },
      "source": [
        "def function_result_09():\n",
        "    \n",
        "    print('[plot the testing loss]')\n",
        "    print('') \n",
        "    \n",
        "    plot_curve_error(loss_mean_test, loss_std_test, 'epoch', 'loss', 'loss (testing)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muZqxZRfJJ2Z"
      },
      "source": [
        "def function_result_10():\n",
        "    \n",
        "    print('[plot the testing accuracy]') \n",
        "    print('') \n",
        "    \n",
        "    plot_curve_error(accuracy_mean_test, accuracy_std_test, 'epoch', 'accuracy', 'accuracy (testing)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7V4y1YmJJ2a"
      },
      "source": [
        "def function_result_11():\n",
        "    \n",
        "    print('[print the training loss at the last 10 epochs]')\n",
        "    print('') \n",
        "\n",
        "    data_last   = get_data_last(loss_mean_train, -10)\n",
        "    index       = np.arange(0, 10)\n",
        "    \n",
        "    print_curve(data_last, index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XikG4C6yJJ2a"
      },
      "source": [
        "def function_result_12():\n",
        "    \n",
        "    print('[print the training accuracy at the last 10 epochs]')\n",
        "    print('') \n",
        "    \n",
        "    data_last   = get_data_last(accuracy_mean_train, -10)\n",
        "    index       = np.arange(0, 10)\n",
        "    \n",
        "    print_curve(data_last, index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjpmQujAJJ2a"
      },
      "source": [
        "def function_result_13():\n",
        "    \n",
        "    print('[print the testing loss at the last 10 epochs]')\n",
        "    print('') \n",
        "    \n",
        "    data_last   = get_data_last(loss_mean_test, -10)\n",
        "    index       = np.arange(0, 10)\n",
        "    \n",
        "    print_curve(data_last, index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ySuJq8sJJ2b"
      },
      "source": [
        "def function_result_14():\n",
        "    \n",
        "    print('[print the testing accuracy at the last 10 epochs]')\n",
        "    print('') \n",
        "    \n",
        "    data_last   = get_data_last(accuracy_mean_test, -10)\n",
        "    index       = np.arange(0, 10)\n",
        "    \n",
        "    print_curve(data_last, index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DN2NltY2JJ2b"
      },
      "source": [
        "def function_result_15():\n",
        "    \n",
        "    print('[print the best training accuracy within the last 10 epochs]')\n",
        "    print('') \n",
        "\n",
        "    value = get_max_last_range(accuracy_mean_train, -10)\n",
        "    print('best training accuracy = %12.10f' % (value))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hT8-R00vJJ2b"
      },
      "source": [
        "def function_result_16():\n",
        "    \n",
        "    print('[print the best testing accuracy within the last 10 epochs]')\n",
        "    print('') \n",
        "    \n",
        "    value = get_max_last_range(accuracy_mean_test, -10)\n",
        "    print('best testing accuracy = %12.10f' % (value))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "il6agliZJJ2b"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQ5_DyJAJJ2c"
      },
      "source": [
        "# RESULTS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRfvU1k1JJ2c"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phEjtos-JJ2c"
      },
      "source": [
        "number_result = 16\n",
        "\n",
        "for i in range(number_result):\n",
        "\n",
        "    title           = '# RESULT # {:02d}'.format(i+1) \n",
        "    name_function   = 'function_result_{:02d}()'.format(i+1)\n",
        "\n",
        "    print('') \n",
        "    print('################################################################################')\n",
        "    print('#') \n",
        "    print(title)\n",
        "    print('#') \n",
        "    print('################################################################################')\n",
        "    print('') \n",
        "\n",
        "    eval(name_function)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KipRWZB0JJ2d"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}