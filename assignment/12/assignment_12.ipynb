{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K29I-OwCEYzW",
        "outputId": "db8b297c-a4b8-457a-b43f-7570135306c7"
      },
      "source": [
        "# Image Generation via Generative Adversarial Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODiv8rJRZiYC"
      },
      "source": [
        "## import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9Wmr9dMLZiYD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import os\n",
        "from torchvision.utils import make_grid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QK2e8feGZiYE"
      },
      "source": [
        "## load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9xfI-4R9ZiYE",
        "outputId": "a60d8a20-2c37-4613-8eb7-9a28f3057235",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "directory_data  = './drive/MyDrive/Machine_Learning/'\n",
        "filename_data   = 'assignment_12_data.npz'\n",
        "data            = np.load(os.path.join(directory_data, filename_data))\n",
        "\n",
        "real            = torch.from_numpy(data['real_images']).float()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpOgjgv1ZiYF"
      },
      "source": [
        "## hyper-parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "uiMjm-PXZiYF"
      },
      "outputs": [],
      "source": [
        "device          = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "number_epoch    = 150\n",
        "size_minibatch  = 50\n",
        "dim_latent      = 50\n",
        "dim_channel     = 1\n",
        "learning_rate_discriminator = 0.001\n",
        "learning_rate_generator     = 0.001"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "def affine(image, shear=0, scale=1, rate=[10, 10]):\n",
        "\n",
        "    func_plt = transforms.functional.to_pil_image\n",
        "    func_affine = transforms.functional.affine\n",
        "    func_tensor = transforms.functional.to_tensor\n",
        "\n",
        "    for i in range(len(image)):\n",
        "\n",
        "        # random movement\n",
        "        if rate[0] != 0:\n",
        "            rate_1 = np.random.randint(-rate[0], rate[0]+1)\n",
        "            rate_2 = np.random.randint(-rate[1], rate[1]+1)\n",
        "            movement = [rate_1, rate_2]\n",
        "        else:\n",
        "            movement = rate\n",
        "\n",
        "        if isinstance(scale, list):\n",
        "            rescale = np.random.randint(scale[0], scale[1]+1) / 10\n",
        "        else:\n",
        "            rescale = scale\n",
        "\n",
        "\n",
        "        trans_image = func_plt(image[i])\n",
        "        trans_image = func_affine(trans_image, angle=0, shear=shear, scale=rescale, translate=movement)\n",
        "        trans_image = func_tensor(trans_image)\n",
        "        trans_image = trans_image.numpy()\n",
        "\n",
        "        if i == 0:\n",
        "            image_list = trans_image\n",
        "        else:\n",
        "            image_list = np.concatenate([image_list, trans_image], axis=0)\n",
        "\n",
        "    return image_list"
      ],
      "metadata": {
        "id": "hf7t8raoYE1C"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "real_image = real[::2]\n",
        "affine_12 = affine(real_image, scale=[10, 12], rate=[0, 0])\n",
        "affine_random = affine(real_image, scale=1, rate=[3, 3])"
      ],
      "metadata": {
        "id": "CDCTxOw1YNh1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrxulfRFZiYG"
      },
      "source": [
        "## custom data loader for the PyTorch framework"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "yNQe4OUCZiYH"
      },
      "outputs": [],
      "source": [
        "class dataset (Dataset):\n",
        "    def  __init__(self, data):\n",
        "\n",
        "        self.data = data\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        data = self.data[index]\n",
        "        data = torch.FloatTensor(data).unsqueeze(dim=0)\n",
        "\n",
        "        return data\n",
        "  \n",
        "    def __len__(self):\n",
        "        \n",
        "        return self.data.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2888l7eZiYI"
      },
      "source": [
        "## construct datasets and dataloaders for training and testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "UwcMO-WdZiYI"
      },
      "outputs": [],
      "source": [
        "# image_train = np.concatenate([real[1::2], affine_12, affine_random], axis=0)\n",
        "dataset_real    = dataset(real)\n",
        "dataloader_real = DataLoader(dataset_real, batch_size=size_minibatch, shuffle=True, drop_last=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# image_train.shape"
      ],
      "metadata": {
        "id": "UQiaccCThGlz"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSai363jZiYI"
      },
      "source": [
        "## shape of the data when using the data loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "mjAvXxDPZiYJ",
        "outputId": "5c92e024-b016-4d38-e941-0b7be4aca161",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*******************************************************************\n",
            "shape of the image in the training dataset: torch.Size([1, 32, 32])\n",
            "*******************************************************************\n"
          ]
        }
      ],
      "source": [
        "image_real = dataset_real[0]\n",
        "print('*******************************************************************')\n",
        "print('shape of the image in the training dataset:', image_real.shape)\n",
        "print('*******************************************************************')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlKo6SbwZiYK"
      },
      "source": [
        "## class for the neural network "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "xLd-pfJnZiYK"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module): \n",
        "\n",
        "\tdef __init__(self, in_channel=1, out_channel=1, dim_feature=128):\n",
        "        \n",
        "\t\tsuper(Discriminator, self).__init__()\n",
        "\n",
        "\t\tself.in_channel \t= in_channel\n",
        "\t\tself.out_channel\t= out_channel\n",
        "\t\tself.dim_feature\t= dim_feature\n",
        "\t\tthreshold_ReLU \t\t= 0.2\n",
        "\t\t\n",
        "\t\tself.feature = nn.Sequential(\n",
        "\t\t\t# ================================================================================\n",
        "\t\t\tnn.Conv2d(in_channel, dim_feature * 1, kernel_size=3, stride=2, padding=1, bias=True),\n",
        "\t\t\tnn.LeakyReLU(threshold_ReLU, inplace=True),\t\t\t\n",
        "\t\t\t# ================================================================================\n",
        "\t\t\tnn.Conv2d(dim_feature * 1, dim_feature * 2, kernel_size=3, stride=2, padding=1, bias=True),\n",
        "\t\t\tnn.LeakyReLU(threshold_ReLU, inplace=True),\t\t\t\n",
        "\t\t\t# ================================================================================\n",
        "\t\t\tnn.Conv2d(dim_feature * 2, dim_feature * 4, kernel_size=3, stride=2, padding=1, bias=True),\n",
        "\t\t\tnn.LeakyReLU(threshold_ReLU, inplace=True),\t\t\t\n",
        "\t\t\t# ================================================================================\n",
        "\t\t\tnn.Conv2d(dim_feature * 4, dim_feature * 8, kernel_size=3, stride=2, padding=1, bias=True),\n",
        "\t\t\tnn.LeakyReLU(threshold_ReLU, inplace=True),\t\t\t\n",
        "\t\t\t# ================================================================================\n",
        "\t\t\tnn.Conv2d(dim_feature * 8, dim_feature * 16, kernel_size=3, stride=2, padding=1, bias=True),\n",
        "\t\t\tnn.LeakyReLU(threshold_ReLU, inplace=True),\t\t\t\n",
        "\t\t\t# ================================================================================\n",
        "\t\t)\t\n",
        "\t\t\n",
        "\t\tself.classifier = nn.Sequential(\n",
        "\t\t\t# ================================================================================\n",
        "\t\t\tnn.Linear(dim_feature * 16, dim_feature * 8, bias=True),\n",
        "\t\t\tnn.LeakyReLU(threshold_ReLU, inplace=True),\t\t\t\n",
        "\t\t\t# ================================================================================\n",
        "\t\t\tnn.Linear(dim_feature * 8, dim_feature * 4, bias=True),\n",
        "\t\t\tnn.LeakyReLU(threshold_ReLU, inplace=True),\t\t\t\n",
        "\t\t\t# ================================================================================\n",
        "\t\t\tnn.Linear(dim_feature * 4, dim_feature * 2, bias=True),\n",
        "\t\t\tnn.LeakyReLU(threshold_ReLU, inplace=True),\t\t\t\n",
        "\t\t\t# ================================================================================\n",
        "\t\t\tnn.Linear(dim_feature * 2, dim_feature * 1, bias=True),\n",
        "\t\t\tnn.LeakyReLU(threshold_ReLU, inplace=True),\t\t\t\n",
        "\t\t\t# ================================================================================\n",
        "\t\t\tnn.Linear(dim_feature * 1, out_channel, bias=True),\n",
        "\t\t\t# ================================================================================\n",
        "\t\t) \n",
        "\n",
        "\t\tself.network = nn.Sequential(\n",
        "\t\t\tself.feature,\n",
        "\t\t\tnn.Flatten(),\n",
        "\t\t\tself.classifier,\n",
        "\t\t)\n",
        "\n",
        "\t\tself.initialize_weight()\n",
        "\n",
        "\t\t# *********************************************************************\n",
        "\t\t# forward propagation\n",
        "\t\t# *********************************************************************\n",
        "\tdef forward(self, x):\n",
        "\n",
        "\t\ty = self.network.forward(x)\n",
        "\n",
        "\t\treturn y\n",
        "\n",
        "\tdef initialize_weight(self):\n",
        "\t\n",
        "\t\tprint('initialize model parameters :', 'xavier_uniform')\n",
        "\n",
        "\t\tfor m in self.network.modules():\n",
        "\t\t\t\n",
        "\t\t\tif isinstance(m, nn.Conv2d):\n",
        "\t\t\t\t\n",
        "\t\t\t\tnn.init.xavier_uniform_(m.weight)\n",
        "\t\t\t\t\n",
        "\t\t\t\tif m.bias is not None:\n",
        "\n",
        "\t\t\t\t\tnn.init.constant_(m.bias, 1)\n",
        "\t\t\t\t\tpass\n",
        "\t\t\t\t\t\n",
        "\t\t\telif isinstance(m, nn.BatchNorm2d):\n",
        "\t\t\t\t\n",
        "\t\t\t\tnn.init.constant_(m.weight, 1)\n",
        "\t\t\t\tnn.init.constant_(m.bias, 1)\n",
        "\t\t\t\t\n",
        "\t\t\telif isinstance(m, nn.Linear):\n",
        "\t\t\t\t\n",
        "\t\t\t\tnn.init.xavier_uniform_(m.weight)\n",
        "\n",
        "\t\t\t\tif m.bias is not None:\n",
        "\t\t\t\t\t\n",
        "\t\t\t\t\tnn.init.constant_(m.bias, 1)\n",
        "\t\t\t\t\tpass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "3TkAH7d1ZiYM"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module): \n",
        "\n",
        "\tdef __init__(self, in_channel=1, out_channel=1, dim_feature=8):\n",
        "        \n",
        "\t\tsuper(Generator, self).__init__()\n",
        "\n",
        "\t\tself.in_channel \t= in_channel\n",
        "\t\tself.out_channel\t= out_channel\n",
        "\t\tself.dim_feature\t= dim_feature\n",
        "\t\tthreshold_ReLU \t\t= 0.2\n",
        "\n",
        "\t\tself.network = nn.Sequential(\n",
        "\t\t\t# ================================================================================\n",
        "\t\t\tnn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
        "\t\t\tnn.Conv2d(in_channel, dim_feature * 8, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "\t\t\tnn.BatchNorm2d(dim_feature * 8),\n",
        "\t\t\tnn.LeakyReLU(threshold_ReLU, inplace=True),\n",
        "\t\t\t# ================================================================================\n",
        "\t\t\tnn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
        "\t\t\tnn.Conv2d(dim_feature * 8, dim_feature * 4, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "\t\t\tnn.BatchNorm2d(dim_feature * 4),\n",
        "\t\t\tnn.LeakyReLU(threshold_ReLU, inplace=True),\n",
        "\t\t\t# ================================================================================\n",
        "\t\t\tnn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
        "\t\t\tnn.Conv2d(dim_feature * 4, dim_feature * 2, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "\t\t\tnn.BatchNorm2d(dim_feature * 2),\n",
        "\t\t\tnn.LeakyReLU(threshold_ReLU, inplace=True),\n",
        "\t\t\t# ================================================================================\n",
        "\t\t\tnn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
        "\t\t\tnn.Conv2d(dim_feature * 2, dim_feature * 1, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "\t\t\tnn.BatchNorm2d(dim_feature * 1),\n",
        "\t\t\tnn.LeakyReLU(threshold_ReLU, inplace=True),\n",
        "\t\t\t# ================================================================================\n",
        "\t\t\tnn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
        "\t\t\tnn.Conv2d(dim_feature * 1, out_channel, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "\t\t\tnn.BatchNorm2d(out_channel),\n",
        "\t\t\t# ================================================================================\n",
        "\t\t\tnn.Sigmoid(),\n",
        "\t\t\t# ================================================================================\n",
        "\t\t) \t\t\t\n",
        "\n",
        "\t\tself.initialize_weight()\n",
        "\t\t\n",
        "\t\t# *********************************************************************\n",
        "\t\t# forward propagation\n",
        "\t\t# *********************************************************************\n",
        "\tdef forward(self, x):\n",
        "\n",
        "\t\ty = self.network.forward(x)\n",
        "\n",
        "\t\treturn y\n",
        "\n",
        "\tdef initialize_weight(self):\n",
        "\t\n",
        "\t\tprint('initialize model parameters :', 'xavier_uniform')\n",
        "\n",
        "\t\tfor m in self.network.modules():\n",
        "\t\t\t\n",
        "\t\t\tif isinstance(m, nn.Conv2d):\n",
        "\t\t\t\t\n",
        "\t\t\t\tnn.init.constant_(m.weight, 1)\n",
        "\t\t\t\t\n",
        "\t\t\t\tif m.bias is not None:\n",
        "\n",
        "\t\t\t\t\tnn.init.constant_(m.bias, 1)\n",
        "\t\t\t\t\tpass\n",
        "\t\t\t\t\t\n",
        "\t\t\telif isinstance(m, nn.BatchNorm2d):\n",
        "\t\t\t\t\n",
        "\t\t\t\tnn.init.constant_(m.weight, 1)\n",
        "\t\t\t\tnn.init.constant_(m.bias, 1)\n",
        "\t\t\t\t\n",
        "\t\t\telif isinstance(m, nn.Linear):\n",
        "\t\t\t\t\n",
        "\t\t\t\tnn.init.constant_(m.weight, 1)\n",
        "\n",
        "\t\t\t\tif m.bias is not None:\n",
        "\t\t\t\t\t\n",
        "\t\t\t\t\tnn.init.constant_(m.bias, 1)\n",
        "\t\t\t\t\tpass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_KE-qN1ZiYP"
      },
      "source": [
        "## build network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "lA6RQiZfZiYQ",
        "outputId": "03b1d0d7-2035-4822-c581-b9b8a462b2f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initialize model parameters : xavier_uniform\n",
            "initialize model parameters : xavier_uniform\n"
          ]
        }
      ],
      "source": [
        "generator       = Generator(dim_latent, 1, 128).to(device)\n",
        "discriminator   = Discriminator(dim_channel, 1, 128).to(device)\n",
        "\n",
        "optimizer_generator     = torch.optim.Adam(generator.parameters(), lr=learning_rate_generator, betas=(0.5, 0.999))\n",
        "optimizer_discriminator = torch.optim.Adam(discriminator.parameters(), lr=learning_rate_discriminator, betas=(0.5, 0.999))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIm_yasuZiYS"
      },
      "source": [
        "## compute the prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "0mGxiPm4ZiYS"
      },
      "outputs": [],
      "source": [
        "def compute_prediction(model, input):\n",
        "\n",
        "    prediction = model(input)\n",
        "\n",
        "    return prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZQhbkdOZiYS"
      },
      "source": [
        "## compute the loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "2Gi0Von_ZiYS"
      },
      "outputs": [],
      "source": [
        "def compute_loss_discriminator(generator, discriminator, latent, data_real):\n",
        "\n",
        "    data_fake       = compute_prediction(generator, latent)\n",
        "    prediction_real = compute_prediction(discriminator, data_real)\n",
        "    prediction_fake = compute_prediction(discriminator, data_fake)\n",
        "\n",
        "    criterion   = nn.BCEWithLogitsLoss()\n",
        "    \n",
        "    label_real  = torch.ones_like(prediction_real)\n",
        "    label_fake  = torch.zeros_like(prediction_fake)\n",
        "\n",
        "    # ==================================================\n",
        "    # fill up the blank\n",
        "    #    \n",
        "    loss_real = criterion(prediction_real, label_real)\n",
        "    loss_fake = criterion(prediction_fake, label_fake)\n",
        "    # \n",
        "    # ==================================================\n",
        "\n",
        "    loss_discriminator = (loss_real + loss_fake) / 2.0\n",
        "\n",
        "    return loss_discriminator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "w67uMCXkZiYS"
      },
      "outputs": [],
      "source": [
        "def compute_loss_generator(generator, discriminator, latent):\n",
        "\n",
        "    data_fake       = compute_prediction(generator, latent)\n",
        "    prediction_fake = compute_prediction(discriminator, data_fake)\n",
        "\n",
        "    criterion       = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    label_real      = torch.ones_like(prediction_fake)\n",
        "\n",
        "    # ==================================================\n",
        "    # fill up the blank\n",
        "    #    \n",
        "    loss_generator  = criterion(prediction_fake, label_real)\n",
        "    # \n",
        "    # ==================================================\n",
        "\n",
        "    return loss_generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnACLB1JZiYT"
      },
      "source": [
        "## compute the accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "g5g3SxP_ZiYU"
      },
      "outputs": [],
      "source": [
        "def get_center_index(binary_image):\n",
        "    \n",
        "    area_square = np.sum(binary_image)\n",
        "\n",
        "    height = binary_image.shape[0]\n",
        "    width = binary_image.shape[1]\n",
        "\n",
        "    x = np.linspace(0, width - 1, width)\n",
        "    y = np.linspace(0, height - 1, height)\n",
        "    indices_X, indices_Y = np.meshgrid(x, y)\n",
        "\n",
        "    x_mean = np.sum(binary_image * indices_X) / area_square\n",
        "    y_mean = np.sum(binary_image * indices_Y) / area_square\n",
        "\n",
        "    return (x_mean, y_mean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "CF1W_HGhZiYU"
      },
      "outputs": [],
      "source": [
        "# create ideal square image which has the same area to the input image\n",
        "def create_label(binary_images):\n",
        "    \n",
        "    label = np.zeros_like(binary_images)\n",
        "    \n",
        "    for i, binary_image in enumerate(binary_images):\n",
        "        \n",
        "        image_height = binary_image.shape[0]\n",
        "        image_width = binary_image.shape[1]\n",
        "\n",
        "        square_image = np.zeros((image_height, image_width))\n",
        "        square_length = np.round(np.sqrt(np.sum(binary_image)))\n",
        "\n",
        "        if square_length == 0:\n",
        "            # when there is no square\n",
        "            return square_image\n",
        "\n",
        "        (square_center_x, square_center_y) = get_center_index(binary_image)\n",
        "\n",
        "        if square_center_x < 0 or square_center_x > image_width - 1 or square_center_y < 0 or square_center_y > image_height - 1:\n",
        "            return square_image\n",
        "\n",
        "        top = np.ceil(square_center_y - square_length / 2)\n",
        "        bottom = np.floor(square_center_y + square_length / 2)\n",
        "        left = np.ceil(square_center_x - square_length / 2)\n",
        "        right = np.floor(square_center_x + square_length / 2)\n",
        "\n",
        "        top = int(top) if top >= 0 else 0\n",
        "        bottom = int(bottom) if bottom <= image_height - 1 else image_height - 1\n",
        "        left = int(left) if left >= 0 else 0\n",
        "        right = int(right) if right <= image_width - 1 else image_width - 1\n",
        "\n",
        "        square_image[top : bottom + 1, left : right + 1] = 1\n",
        "        \n",
        "        label[i] = square_image\n",
        "        \n",
        "    return label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "gZ5GMpKqZiYU"
      },
      "outputs": [],
      "source": [
        "def compute_accuracy(prediction):\n",
        "\n",
        "    prediction  = prediction.squeeze(axis=1)\n",
        "    \n",
        "    prediction_binary   = (prediction >= 0.5).cpu().numpy().astype(int)\n",
        "    label               = create_label(prediction_binary).astype(int)\n",
        "    \n",
        "    region_intersection = prediction_binary & label\n",
        "    region_union        = prediction_binary | label\n",
        "\n",
        "    area_intersection   = region_intersection.sum(axis=1).sum(axis=1).astype(float)\n",
        "    area_union          = region_union.sum(axis=1).sum(axis=1).astype(float)\n",
        "\n",
        "    eps         = np.finfo(float).eps\n",
        "    correct     = area_intersection / (area_union + eps)\n",
        "    accuracy    = correct.mean() * 100.0\n",
        "    \n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2mS2JQyZiYU"
      },
      "source": [
        "## variables for the learning curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "OGibNn-WZiYU"
      },
      "outputs": [],
      "source": [
        "loss_generator_mean     = np.zeros(number_epoch)\n",
        "loss_generator_std      = np.zeros(number_epoch)\n",
        "loss_discriminator_mean = np.zeros(number_epoch)\n",
        "loss_discriminator_std  = np.zeros(number_epoch)\n",
        "\n",
        "accuracy_mean   = np.zeros(number_epoch)\n",
        "accuracy_std    = np.zeros(number_epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i44O38S9ZiYV"
      },
      "source": [
        "## train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "0D8yRcpxZiYV"
      },
      "outputs": [],
      "source": [
        "def train(generator, discriminator, dataloader):\n",
        "\n",
        "    loss_epoch_generator      = []\n",
        "    loss_epoch_discriminator  = []\n",
        "    accuracy_epoch = []\n",
        "    \n",
        "    for index_batch, data_real in enumerate(dataloader):\n",
        "\n",
        "        size_batch  = len(data_real)\n",
        "        data_real   = data_real.to(device)\n",
        "        \n",
        "        latent  = torch.randn(size_batch, dim_latent, device=device)\n",
        "        latent  = torch.reshape(latent, [size_batch, dim_latent, 1, 1])\n",
        "\n",
        "        # ---------------------------------------------------------------------------\n",
        "        #  \n",
        "        # update the generator\n",
        "        #  \n",
        "        # ---------------------------------------------------------------------------\n",
        "        generator.train()\n",
        "        discriminator.eval()\n",
        "\n",
        "        optimizer_generator.zero_grad()\n",
        "        loss_generator = compute_loss_generator(generator, discriminator, latent)\n",
        "        loss_generator.backward()\n",
        "        optimizer_generator.step()\n",
        "\n",
        "        # ---------------------------------------------------------------------------\n",
        "        #  \n",
        "        # update the discriminator\n",
        "        #  \n",
        "        # ---------------------------------------------------------------------------\n",
        "        generator.eval()\n",
        "        discriminator.train()\n",
        "\n",
        "        optimizer_discriminator.zero_grad()\n",
        "        loss_discriminator = compute_loss_discriminator(generator, discriminator, latent, data_real)\n",
        "        loss_discriminator.backward()\n",
        "        optimizer_discriminator.step()\n",
        "\n",
        "        data_fake   = compute_prediction(generator, latent)\n",
        "        accuracy    = compute_accuracy(data_fake)\n",
        "\n",
        "        loss_epoch_generator.append(loss_generator.item())\n",
        "        loss_epoch_discriminator.append(loss_discriminator.item())\n",
        "        accuracy_epoch.append(accuracy)\n",
        "\n",
        "    loss_generator_mean_epoch       = np.mean(loss_epoch_generator)\n",
        "    loss_generator_std_epoch        = np.std(loss_epoch_generator)\n",
        "    \n",
        "    loss_discriminator_mean_epoch   = np.mean(loss_epoch_discriminator)\n",
        "    loss_discriminator_std_epoch    = np.std(loss_epoch_discriminator)\n",
        "\n",
        "    accuracy_mean_epoch             = np.mean(accuracy_epoch)\n",
        "    accuracy_std_epoch              = np.std(accuracy_epoch)\n",
        "\n",
        "    loss_value_generator        = {'mean' : loss_generator_mean_epoch, 'std' : loss_generator_std_epoch}\n",
        "    loss_value_discriminator    = {'mean' : loss_discriminator_mean_epoch, 'std' : loss_discriminator_std_epoch}\n",
        "    accuracy_value              = {'mean' : accuracy_mean_epoch, 'std' : accuracy_std_epoch} \n",
        "\n",
        "    return loss_value_generator, loss_value_discriminator, accuracy_value\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7gyp-qHZiYV"
      },
      "source": [
        "## training epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WcbnvnevZiYV",
        "outputId": "0f8f20eb-8181-401a-c879-c8cab814ffa1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 1/150 [00:17<42:13, 17.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch : 0\n",
            "\tloss_value_discriminator : 2.6793725268785344, acc mean : 47.42990189632055\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|▏         | 2/150 [00:33<41:17, 16.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch : 1\n",
            "\tloss_value_discriminator : 132403.21152456797, acc mean : 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|▏         | 3/150 [00:50<40:48, 16.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch : 2\n",
            "\tloss_value_discriminator : 561.3294171663218, acc mean : 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  3%|▎         | 4/150 [01:06<40:26, 16.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch : 3\n",
            "\tloss_value_discriminator : 44.196709893643856, acc mean : 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  3%|▎         | 5/150 [01:23<40:06, 16.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch : 4\n",
            "\tloss_value_discriminator : 2.9095685741523023, acc mean : 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4%|▍         | 6/150 [01:39<39:47, 16.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch : 5\n",
            "\tloss_value_discriminator : 2.1523955931389898, acc mean : 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  5%|▍         | 7/150 [01:56<39:30, 16.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch : 6\n",
            "\tloss_value_discriminator : 0.940894321896944, acc mean : 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  5%|▌         | 8/150 [02:12<39:12, 16.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch : 7\n",
            "\tloss_value_discriminator : 1.1983904550623603, acc mean : 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  6%|▌         | 9/150 [02:29<38:55, 16.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch : 8\n",
            "\tloss_value_discriminator : 1.324414051108084, acc mean : 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  7%|▋         | 10/150 [02:45<38:37, 16.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch : 9\n",
            "\tloss_value_discriminator : 1.0190677340438536, acc mean : 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  7%|▋         | 11/150 [03:02<38:20, 16.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch : 10\n",
            "\tloss_value_discriminator : 0.962796982381506, acc mean : 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  8%|▊         | 12/150 [03:19<38:05, 16.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch : 11\n",
            "\tloss_value_discriminator : 0.7977839494162167, acc mean : 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  9%|▊         | 13/150 [03:35<37:49, 16.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch : 12\n",
            "\tloss_value_discriminator : 1.1163599531124828, acc mean : 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  9%|▉         | 14/150 [03:52<37:33, 16.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch : 13\n",
            "\tloss_value_discriminator : 0.9858241459675908, acc mean : 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|█         | 15/150 [04:08<37:16, 16.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch : 14\n",
            "\tloss_value_discriminator : 1.0825390496308056, acc mean : 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 11%|█         | 16/150 [04:25<36:59, 16.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch : 15\n",
            "\tloss_value_discriminator : 0.8985991751448228, acc mean : 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 11%|█▏        | 17/150 [04:41<36:42, 16.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch : 16\n",
            "\tloss_value_discriminator : 0.6634464244863701, acc mean : 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 12%|█▏        | 18/150 [04:58<36:26, 16.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch : 17\n",
            "\tloss_value_discriminator : 0.7664499485030171, acc mean : 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 13%|█▎        | 19/150 [05:15<36:08, 16.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch : 18\n",
            "\tloss_value_discriminator : 1.1163176064548737, acc mean : 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 13%|█▎        | 20/150 [05:31<35:51, 16.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch : 19\n",
            "\tloss_value_discriminator : 0.5708611236164985, acc mean : 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 14%|█▍        | 21/150 [05:48<35:37, 16.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch : 20\n",
            "\tloss_value_discriminator : 0.07473466000970864, acc mean : 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 15%|█▍        | 22/150 [06:04<35:19, 16.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch : 21\n",
            "\tloss_value_discriminator : 1.2730526153992137, acc mean : 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 15%|█▌        | 23/150 [06:21<35:02, 16.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch : 22\n",
            "\tloss_value_discriminator : 0.529024237194261, acc mean : 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 16%|█▌        | 24/150 [06:37<34:46, 16.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch : 23\n",
            "\tloss_value_discriminator : 0.21600026763653446, acc mean : 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 17%|█▋        | 25/150 [06:54<34:31, 16.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch : 24\n",
            "\tloss_value_discriminator : 0.5693319417332479, acc mean : 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 17%|█▋        | 26/150 [07:11<34:16, 16.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch : 25\n",
            "\tloss_value_discriminator : 0.5420518536949553, acc mean : 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 18%|█▊        | 27/150 [07:27<33:59, 16.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch : 26\n",
            "\tloss_value_discriminator : 0.19700906070693955, acc mean : 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 19%|█▊        | 28/150 [07:44<33:43, 16.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch : 27\n",
            "\tloss_value_discriminator : 0.09947788450880894, acc mean : 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 19%|█▉        | 29/150 [08:00<33:28, 16.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch : 28\n",
            "\tloss_value_discriminator : 96.2579687371445, acc mean : 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 30/150 [08:17<33:11, 16.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch : 29\n",
            "\tloss_value_discriminator : 5.739143462916619, acc mean : 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 21%|██        | 31/150 [08:34<32:55, 16.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch : 30\n",
            "\tloss_value_discriminator : 0.09967446205920952, acc mean : 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 21%|██▏       | 32/150 [08:50<32:38, 16.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch : 31\n",
            "\tloss_value_discriminator : 0.05790602539254482, acc mean : 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 22%|██▏       | 33/150 [09:07<32:22, 16.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch : 32\n",
            "\tloss_value_discriminator : 0.04167086906124686, acc mean : 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 23%|██▎       | 34/150 [09:23<32:06, 16.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch : 33\n",
            "\tloss_value_discriminator : 0.036630464165449836, acc mean : 0.0\n"
          ]
        }
      ],
      "source": [
        "# ================================================================================\n",
        "# \n",
        "# iterations for epochs\n",
        "#\n",
        "# ================================================================================\n",
        "for i in tqdm(range(number_epoch)):\n",
        "    \n",
        "    # ================================================================================\n",
        "    # \n",
        "    # training\n",
        "    #\n",
        "    # ================================================================================\n",
        "    (loss_value_generator, loss_value_discriminator, accuracy_value) = train(generator, discriminator, dataloader_real)\n",
        "\n",
        "    loss_generator_mean[i]      = loss_value_generator['mean']\n",
        "    loss_generator_std[i]       = loss_value_generator['std']\n",
        "\n",
        "    loss_discriminator_mean[i]  = loss_value_discriminator['mean']\n",
        "    loss_discriminator_std[i]   = loss_value_discriminator['std']\n",
        "\n",
        "    accuracy_mean[i]            = accuracy_value['mean']\n",
        "    accuracy_std[i]             = accuracy_value['std']\n",
        "\n",
        "    print(f\"epoch : {i}\")\n",
        "    print(f\"\\tloss_value_discriminator : {loss_value_discriminator['mean']}, acc mean : {accuracy_value['mean']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGpzL2sHZiYV"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sig1-Xv-ZiYW"
      },
      "source": [
        "# functions for visualizing the results "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mo8Kr3FzZiYW"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5BarCiNZiYW"
      },
      "source": [
        "## plot curve"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oh-F0VPWZiYW"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IllnUpY9ZiYW"
      },
      "outputs": [],
      "source": [
        "def plot_image_grid(data, nRow, nCol, filename=None):\n",
        "\n",
        "    size_col = 1.5\n",
        "    size_row = 1.5\n",
        "\n",
        "    fig, axes = plt.subplots(nRow, nCol, constrained_layout=True, figsize=(nCol * size_col, nRow * size_row))\n",
        "    \n",
        "    data = data.detach().cpu()\n",
        "\n",
        "    for i in range(nRow):\n",
        "        for j in range(nCol):\n",
        "\n",
        "            k       = i * nCol + j\n",
        "            image   = np.squeeze(data[k], axis=0)\n",
        "\n",
        "            axes[i, j].imshow(image, cmap='gray', vmin=0, vmax=1)\n",
        "            axes[i, j].xaxis.set_visible(False)\n",
        "            axes[i, j].yaxis.set_visible(False)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    if filename is not None:\n",
        "\n",
        "        fig.savefig(filename)\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qowDwzLAZiYW"
      },
      "outputs": [],
      "source": [
        "def plot_data_grid(data, index_data, nRow, nCol):\n",
        "    \n",
        "    size_col = 1.5\n",
        "    size_row = 1.5\n",
        "\n",
        "    fig, axes = plt.subplots(nRow, nCol, constrained_layout=True, figsize=(nCol * size_col, nRow * size_row))\n",
        "\n",
        "    for i in range(nRow):\n",
        "        for j in range(nCol):\n",
        "\n",
        "            k       = i * nCol + j\n",
        "            index   = index_data[k]\n",
        "\n",
        "            axes[i, j].imshow(data[index], cmap='gray', vmin=0, vmax=1)\n",
        "            axes[i, j].xaxis.set_visible(False)\n",
        "            axes[i, j].yaxis.set_visible(False)\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMGtJBm7ZiYW"
      },
      "outputs": [],
      "source": [
        "def plot_data_tensor_grid(data, index_data, nRow, nCol):\n",
        "    \n",
        "    size_col = 1.5\n",
        "    size_row = 1.5\n",
        "\n",
        "    fig, axes = plt.subplots(nRow, nCol, constrained_layout=True, figsize=(nCol * size_col, nRow * size_row))\n",
        "\n",
        "    data = data.detach().cpu().squeeze(axis=1)\n",
        "\n",
        "    for i in range(nRow):\n",
        "        for j in range(nCol):\n",
        "\n",
        "            k       = i * nCol + j\n",
        "            index   = index_data[k]\n",
        "\n",
        "            axes[i, j].imshow(data[index], cmap='gray', vmin=0, vmax=1)\n",
        "            axes[i, j].xaxis.set_visible(False)\n",
        "            axes[i, j].yaxis.set_visible(False)\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9Rvahj_ZiYX"
      },
      "outputs": [],
      "source": [
        "def plot_curve_error(data_mean, data_std, x_label, y_label, title, filename=None):\n",
        "\n",
        "    fig = plt.figure(figsize=(8, 6))\n",
        "    plt.title(title)\n",
        "\n",
        "    alpha = 0.3\n",
        "    \n",
        "    plt.plot(range(len(data_mean)), data_mean, '-', color = 'red')\n",
        "    plt.fill_between(range(len(data_mean)), data_mean - data_std, data_mean + data_std, facecolor = 'blue', alpha = alpha) \n",
        "    \n",
        "    plt.xlabel(x_label)\n",
        "    plt.ylabel(y_label)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    if filename is not None:\n",
        "\n",
        "        fig.savefig(filename)\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mrTtty56ZiYX"
      },
      "outputs": [],
      "source": [
        "def print_curve(data, index):\n",
        "    \n",
        "    for i in range(len(index)):\n",
        "\n",
        "        idx = index[i]\n",
        "        val = data[idx]\n",
        "\n",
        "        print('index = %2d, value = %12.10f' % (idx, val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIZj7BMNZiYX"
      },
      "outputs": [],
      "source": [
        "def get_data_last(data, index_start):\n",
        "\n",
        "    data_last = data[index_start:]\n",
        "\n",
        "    return data_last"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hSWP-LXEZiYX"
      },
      "outputs": [],
      "source": [
        "def get_max_last_range(data, index_start):\n",
        "\n",
        "    data_range = get_data_last(data, index_start)\n",
        "    value = data_range.max()\n",
        "\n",
        "    return value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJl9ZgUxZiYX"
      },
      "outputs": [],
      "source": [
        "def get_min_last_range(data, index_start):\n",
        "\n",
        "    data_range = get_data_last(data, index_start)\n",
        "    value = data_range.min()\n",
        "\n",
        "    return value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOdRbokrZiYX"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ukEa2-2ZiYY"
      },
      "source": [
        "# functions for presenting the results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyWYXuUpZiYY"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MEe41fgTZiYY"
      },
      "outputs": [],
      "source": [
        "def function_result_01():\n",
        "\n",
        "    print('[plot examples of the real images]')\n",
        "    print('') \n",
        "\n",
        "    nRow = 8\n",
        "    nCol = 6\n",
        "\n",
        "    number_data = len(dataset_real)\n",
        "    step        = int(np.floor(number_data / (nRow * nCol)))\n",
        "    index_data  = np.arange(0, number_data, step)\n",
        "    index_plot  = np.arange(0, nRow * nCol)\n",
        "\n",
        "    data = dataset_real[index_data]\n",
        "    data = data[0]\n",
        "    \n",
        "    plot_data_grid(data, index_plot, nRow, nCol)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UL9Dr3NaZiYY"
      },
      "outputs": [],
      "source": [
        "def function_result_02():\n",
        "\n",
        "    print('[plot examples of the fake images]')\n",
        "    print('') \n",
        "\n",
        "    nRow = 8\n",
        "    nCol = 6\n",
        "    number_latent = nRow * nCol\n",
        "\n",
        "    latent  = torch.randn(number_latent, dim_latent, device=device)\n",
        "    latent  = torch.reshape(latent, [number_latent, dim_latent, 1, 1])\n",
        "\n",
        "    generator.eval()\n",
        "\n",
        "    data_fake   = generator(latent)\n",
        "    filename    = 'fake_image.png'\n",
        "\n",
        "    plot_image_grid(data_fake, nRow, nCol, filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8sXOPrEkZiYY"
      },
      "outputs": [],
      "source": [
        "def function_result_03():\n",
        "\n",
        "    print('[plot the generator loss]')\n",
        "    print('') \n",
        "\n",
        "    plot_curve_error(loss_generator_mean, loss_generator_std, 'epoch', 'loss', 'generator loss', 'loss_generator.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZ7wWBKmZiYY"
      },
      "outputs": [],
      "source": [
        "def function_result_04():\n",
        "    \n",
        "    print('[plot the discriminator loss]')\n",
        "    print('') \n",
        "    \n",
        "    plot_curve_error(loss_discriminator_mean, loss_discriminator_std, 'epoch', 'loss', 'discriminator loss', 'loss_discriminator.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4GVlcfwZiYY"
      },
      "outputs": [],
      "source": [
        "def function_result_05():\n",
        "    \n",
        "    print('[plot the accuracy]')\n",
        "    print('') \n",
        "    \n",
        "    plot_curve_error(accuracy_mean, accuracy_std, 'epoch', 'accuracy', 'training accuracy', 'training_accuracy.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DfgfXKPuZiYY"
      },
      "outputs": [],
      "source": [
        "def function_result_06():\n",
        "    \n",
        "    print('[print the generator loss at the last 10 epochs]')\n",
        "    print('') \n",
        "\n",
        "    data_last = get_data_last(loss_generator_mean, -10)\n",
        "    index = np.arange(0, 10)\n",
        "    print_curve(data_last, index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HgAGW3flZiYZ"
      },
      "outputs": [],
      "source": [
        "def function_result_07():\n",
        "    \n",
        "    print('[print the discriminator loss at the last 10 epochs]')\n",
        "    print('') \n",
        "\n",
        "    data_last = get_data_last(loss_discriminator_mean, -10)\n",
        "    index = np.arange(0, 10)\n",
        "    print_curve(data_last, index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m25LJAC-ZiYZ"
      },
      "outputs": [],
      "source": [
        "def function_result_08():\n",
        "    \n",
        "    print('[print the accuracy at the last 10 epochs]')\n",
        "    print('') \n",
        "\n",
        "    data_last = get_data_last(accuracy_mean, -10)\n",
        "    index = np.arange(0, 10)\n",
        "    print_curve(data_last, index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kecDyh2GZiYZ"
      },
      "outputs": [],
      "source": [
        "def function_result_09():\n",
        "    \n",
        "    print('[print the best accuracy within the last 10 epochs]')\n",
        "    print('') \n",
        "    \n",
        "    value = get_max_last_range(accuracy_mean, -10)\n",
        "    print('best accuracy = %12.10f' % (value))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZbp5pPOZiYZ"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TU1P0Y7qZiYZ"
      },
      "source": [
        "# RESULTS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuWnhH62ZiYZ"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XP0f5JD9ZiYa"
      },
      "outputs": [],
      "source": [
        "number_result = 9\n",
        "\n",
        "for i in range(number_result):\n",
        "\n",
        "    title           = '# RESULT # {:02d}'.format(i+1) \n",
        "    name_function   = 'function_result_{:02d}()'.format(i+1)\n",
        "\n",
        "    print('') \n",
        "    print('################################################################################')\n",
        "    print('#') \n",
        "    print(title)\n",
        "    print('#') \n",
        "    print('################################################################################')\n",
        "    print('') \n",
        "\n",
        "    eval(name_function)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "assignment_02.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "17ed1555cfbb96ddcf655400d6c25a9cebe961c1b69daf25bae91d698acdd2a7"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 64-bit ('hsh': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}